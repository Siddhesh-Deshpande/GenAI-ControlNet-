{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T05:54:11.343962Z",
     "iopub.status.busy": "2025-12-15T05:54:11.343361Z",
     "iopub.status.idle": "2025-12-15T05:54:11.366772Z",
     "shell.execute_reply": "2025-12-15T05:54:11.365881Z",
     "shell.execute_reply.started": "2025-12-15T05:54:11.343938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_human_pose.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_human_pose.py\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Memory optimization utilities for Kaggle\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    def clear_memory():\n",
    "        \"\"\"Clear GPU and system memory\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        print(\"Memory cleared\")\n",
    "    \n",
    "    def print_memory_usage():\n",
    "        \"\"\"Print current GPU memory usage\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "        \n",
    "    print(\"Memory utilities loaded\")\n",
    "    \n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import torchvision.transforms as transforms\n",
    "    import os\n",
    "    from pycocotools.coco import COCO\n",
    "    import requests\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    import cv2\n",
    "    import zipfile\n",
    "    \n",
    "    class COCOPoseDataset(Dataset):\n",
    "        def __init__(self, root_dir='/kaggle/working/coco_data', split='train', transform=None, image_size=512, \n",
    "                     custom_captions_file=None, max_samples=None, download=True):\n",
    "            \"\"\"\n",
    "            Custom Dataset with COCO Pose Skeletons + Custom Captions\n",
    "            \"\"\"\n",
    "            self.root_dir = root_dir\n",
    "            self.split = split\n",
    "            self.transform = transform\n",
    "            self.image_size = image_size\n",
    "            self.custom_captions_file = custom_captions_file\n",
    "            \n",
    "            # COCO 2017 URLs\n",
    "            self.annotation_urls = {\n",
    "                'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "                'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
    "            }\n",
    "            \n",
    "            # Setup paths\n",
    "            self.ann_dir = os.path.join(root_dir, 'annotations')\n",
    "            self.img_dir = os.path.join(root_dir, f'{split}2017')\n",
    "            \n",
    "            split_name = 'train' if split == 'train' else 'val'\n",
    "            self.ann_file = os.path.join(self.ann_dir, f'person_keypoints_{split_name}2017.json')\n",
    "            \n",
    "            # Create directories\n",
    "            os.makedirs(self.ann_dir, exist_ok=True)\n",
    "            os.makedirs(self.img_dir, exist_ok=True)\n",
    "            \n",
    "            # Load custom captions\n",
    "            self.custom_captions = None\n",
    "            self.custom_caption_map = {}\n",
    "            self.img_ids = []\n",
    "            \n",
    "            if not custom_captions_file:\n",
    "                raise ValueError(f\"custom_captions_file is required.\")\n",
    "            \n",
    "            # Check if caption file exists\n",
    "            caption_path = custom_captions_file\n",
    "            if not os.path.exists(caption_path):\n",
    "                # Try absolute path if relative doesn't work\n",
    "                caption_path = os.path.abspath(custom_captions_file)\n",
    "            \n",
    "            if not os.path.exists(caption_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Caption file not found: {custom_captions_file}\\n\"\n",
    "                )\n",
    "            \n",
    "            print(f\"Loading custom captions from {caption_path}...\")\n",
    "            with open(caption_path, 'r') as f:\n",
    "                self.custom_captions = json.load(f)\n",
    "            print(f\"Loaded {len(self.custom_captions)} custom captions\")\n",
    "            \n",
    "            # Download COCO annotations if needed\n",
    "            if download and not os.path.exists(self.ann_file):\n",
    "                print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n",
    "                self._download_annotations()\n",
    "            \n",
    "            # Check if annotation file exists\n",
    "            if not os.path.exists(self.ann_file):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Annotation file not found: {self.ann_file}\\n\"\n",
    "                    f\"Download COCO 2017 annotations from:\\n\"\n",
    "                    f\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\\n\"\n",
    "                    f\"Extract to: {self.ann_dir}\"\n",
    "                )\n",
    "            \n",
    "            print(f\"Loading COCO {split} pose annotations...\")\n",
    "            self.coco = COCO(self.ann_file)\n",
    "            \n",
    "            # Map images from caption file\n",
    "            print(\"Setting up dataset with images, poses from COCO, and custom captions...\")\n",
    "            items_to_process = list(self.custom_captions.items())\n",
    "            if max_samples:\n",
    "                items_to_process = items_to_process[:max_samples]\n",
    "            \n",
    "            for img_filename, caption in items_to_process:\n",
    "                # Extract image ID from filename (e.g., '000000391895.jpg' -> 391895)\n",
    "                img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n",
    "                self.img_ids.append(img_id)\n",
    "                self.custom_caption_map[img_id] = caption\n",
    "            \n",
    "            limit_msg = f\" (limited to first {max_samples})\" if max_samples else \"\"\n",
    "            print(f\"Dataset ready with {len(self.img_ids)} images\")\n",
    "            print(f\"  - Images from: {self.img_dir}\")\n",
    "            print(f\"  - Captions from: {custom_captions_file}{limit_msg}\")\n",
    "            print(f\"  - Poses from: COCO person_keypoints annotations\")\n",
    "            print(f\"Using actual pose skeletons as conditioning.\\n\")\n",
    "            \n",
    "            if len(self.img_ids) == 0:\n",
    "                print(\"\\nERROR: No images found in caption file.\")\n",
    "        \n",
    "        def _download_annotations(self):\n",
    "            \"\"\"Download COCO annotations\"\"\"\n",
    "            url = self.annotation_urls[self.split]\n",
    "            zip_path = os.path.join(self.root_dir, 'annotations.zip')\n",
    "            \n",
    "            print(f\"Downloading from {url}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(zip_path, 'wb') as f:\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "            \n",
    "            print(\"Extracting annotations...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.root_dir)\n",
    "\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "            \n",
    "            print(\"Annotations downloaded successfully!\")\n",
    "        \n",
    "        def _download_image(self, img_id, img_filename):\n",
    "            \"\"\"Download a single image from COCO dataset on-the-fly\"\"\"\n",
    "            img_path = os.path.join(self.img_dir, img_filename)\n",
    "            \n",
    "            # If image already exists, skip download\n",
    "            if os.path.exists(img_path):\n",
    "                return img_path\n",
    "            \n",
    "            # Get image info from COCO API\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            img_url = img_info['coco_url']\n",
    "            \n",
    "            # Download image\n",
    "            try:\n",
    "                response = requests.get(img_url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Save image\n",
    "                with open(img_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                return img_path\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to download image {img_filename} from {img_url}: {str(e)}\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.img_ids)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img_id = self.img_ids[idx]\n",
    "            img_filename = list(self.custom_captions.keys())[idx]\n",
    "            img_path = os.path.join(self.img_dir, img_filename)\n",
    "            \n",
    "            # Download image if it doesn't exist\n",
    "            if not os.path.exists(img_path):\n",
    "                # print(f\"Downloading image: {img_filename}...\")\n",
    "                img_path = self._download_image(img_id, img_filename)\n",
    "            \n",
    "            # Try to load image, skip if corrupted\n",
    "            max_retries = 3\n",
    "            retry_count = 0\n",
    "            image = None\n",
    "            \n",
    "            while retry_count < max_retries and image is None:\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                    width, height = image.size\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    retry_count += 1\n",
    "                    print(f\"Failed to load image {img_filename} (attempt {retry_count}/{max_retries}): {str(e)[:50]}\")\n",
    "                    \n",
    "                    # Delete corrupted file and try re-downloading\n",
    "                    if os.path.exists(img_path):\n",
    "                        os.remove(img_path)\n",
    "                        print(f\"Deleted corrupted file: {img_filename}\")\n",
    "                    \n",
    "                    if retry_count < max_retries:\n",
    "                        try:\n",
    "                            print(f\"Re-downloading image...\")\n",
    "                            img_path = self._download_image(img_id, img_filename)\n",
    "                        except Exception as download_err:\n",
    "                            print(f\"Download failed: {str(download_err)[:50]}\")\n",
    "            \n",
    "            # If still can't load, return a placeholder sample\n",
    "            if image is None:\n",
    "                print(f\"Giving up on {img_filename}, returning next valid image instead...\")\n",
    "                # Try next image in dataset\n",
    "                next_idx = (idx + 1) % len(self.img_ids)\n",
    "                if next_idx != idx:  # Avoid infinite loop\n",
    "                    return self.__getitem__(next_idx)\n",
    "                else:\n",
    "                    # Return black image as fallback\n",
    "                    image = Image.new('RGB', (self.image_size, self.image_size), color='black')\n",
    "                    width, height = self.image_size, self.image_size\n",
    "            \n",
    "            # Get caption\n",
    "            caption = self.custom_caption_map[img_id]\n",
    "            \n",
    "            # Extract pose keypoints from COCO\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Get pose keypoints from first person with visible keypoints\n",
    "            keypoints = None\n",
    "            for ann in anns:\n",
    "                if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                    keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                    break\n",
    "            \n",
    "            if keypoints is None:\n",
    "                keypoints = np.zeros((17, 3))\n",
    "            \n",
    "            # Create actual pose skeleton\n",
    "            pose_skeleton = self.create_pose_skeleton(keypoints, width, height)\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = transforms.Compose([\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5], [0.5])\n",
    "                ])(image)\n",
    "            \n",
    "            pose_map = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "            ])(pose_skeleton)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'pose': pose_map,\n",
    "                'raw_keypoints': keypoints,\n",
    "                'image_id': img_id,\n",
    "                'captions': [caption]  # Your custom caption\n",
    "            }\n",
    "        \n",
    "        def create_pose_skeleton(self, keypoints, width, height):\n",
    "            \"\"\"\n",
    "            Create actual human pose skeleton from COCO keypoint annotations\n",
    "            Draws a stick figure with bones connecting the 17 joints\n",
    "            \"\"\"\n",
    "            # Create black canvas\n",
    "            pose_img = np.zeros((height, width), dtype=np.uint8)\n",
    "            \n",
    "            # COCO keypoint skeleton connections (bones linking joints)\n",
    "            skeleton = [\n",
    "                (0, 1), (0, 2),           # nose to eyes\n",
    "                (1, 3), (2, 4),           # eyes to ears\n",
    "                (0, 5), (0, 6),           # nose to shoulders\n",
    "                (5, 7), (7, 9),           # left arm (shoulder -> elbow -> wrist)\n",
    "                (6, 8), (8, 10),          # right arm (shoulder -> elbow -> wrist)\n",
    "                (5, 11), (6, 12),         # shoulders to hips\n",
    "                (11, 12),                 # hip to hip\n",
    "                (11, 13), (13, 15),       # left leg (hip -> knee -> ankle)\n",
    "                (12, 14), (14, 16)        # right leg (hip -> knee -> ankle)\n",
    "            ]\n",
    "            \n",
    "            # Line thickness and circle radius scale with image size\n",
    "            line_thickness = max(2, int(min(width, height) / 100))\n",
    "            circle_radius = max(3, int(min(width, height) / 80))\n",
    "            \n",
    "            # Draw bones (connections)\n",
    "            for start_idx, end_idx in skeleton:\n",
    "                if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
    "                    x1, y1, v1 = keypoints[start_idx]\n",
    "                    x2, y2, v2 = keypoints[end_idx]\n",
    "                    \n",
    "                    # Draw line only if both keypoints are visible (v > 0)\n",
    "                    if v1 > 0 and v2 > 0:\n",
    "                        cv2.line(pose_img, (int(x1), int(y1)), (int(x2), int(y2)), \n",
    "                                255, line_thickness, cv2.LINE_AA)\n",
    "            \n",
    "            # Draw keypoint circles on top of bones\n",
    "            for i, (x, y, v) in enumerate(keypoints):\n",
    "                if v > 0:  # Only draw visible keypoints\n",
    "                    cv2.circle(pose_img, (int(x), int(y)), circle_radius, 255, -1)\n",
    "            \n",
    "            return pose_img\n",
    "    \n",
    "    # Create datasets with COCO poses + custom captions\n",
    "    train_dataset = COCOPoseDataset(\n",
    "        root_dir='/kaggle/working/coco_data',\n",
    "        split='train',\n",
    "        image_size=512,\n",
    "        custom_captions_file='/kaggle/input/captions/train_captions.json',\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    # Validation dataset\n",
    "    val_dataset = COCOPoseDataset(\n",
    "        root_dir='/kaggle/working/coco_data',\n",
    "        split='val',\n",
    "        image_size=512,\n",
    "        custom_captions_file='/kaggle/input/captions/val_captions.json',\n",
    "        max_samples=None,\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully.\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Show a sample\n",
    "    if len(train_dataset) > 0:\n",
    "        sample = train_dataset[0]\n",
    "        if sample['captions']:\n",
    "            print(f\"\\nSample caption from training:\")\n",
    "            caption = sample['captions'][0]\n",
    "            print(f\"   \\\"{caption[:200]}...\\\"\" if len(caption) > 200 else f\"   \\\"{caption}\\\"\")\n",
    "        print(f\"Sample has {(sample['raw_keypoints'][:, 2] > 0).sum()} visible pose keypoints\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get one sample from the training dataset\n",
    "    sample = train_dataset[0]\n",
    "    img_id = sample['image_id']\n",
    "    \n",
    "    # Get annotations for this image from COCO\n",
    "    ann_ids = train_dataset.coco.getAnnIds(imgIds=img_id)\n",
    "    anns = train_dataset.coco.loadAnns(ann_ids)\n",
    "    img_info = train_dataset.coco.loadImgs(img_id)[0]\n",
    "    \n",
    "    # Get captions (text prompts)\n",
    "    captions = sample['captions']\n",
    "    \n",
    "    # Build metadata text\n",
    "    metadata = f\"Image ID: {img_id}\\nFilename: {img_info['file_name']}\\n\"\n",
    "    metadata += f\"Size: {img_info['width']}x{img_info['height']}\\n\"\n",
    "    metadata += f\"Person annotations: {len([a for a in anns if a.get('category_id') == 1])}\\n\"\n",
    "    \n",
    "    for i, ann in enumerate(anns):\n",
    "        if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "            metadata += f\"\\nPerson {i+1}: {ann['num_keypoints']} keypoints\"\n",
    "            if 'area' in ann:\n",
    "                metadata += f\", area: {int(ann['area'])}\"\n",
    "            break\n",
    "    \n",
    "    # Convert tensors back to displayable format\n",
    "    image = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    # Convert PIL Image to numpy array\n",
    "    pose_map = np.array(sample['pose'])\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Original image\n",
    "    ax1 = fig.add_subplot(gs[0:2, 0])\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Pose skeleton\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.imshow(pose_map, cmap='gray')\n",
    "    ax2.set_title('Pose Skeleton', fontsize=14, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.imshow(image)\n",
    "    ax3.imshow(pose_map, cmap='hot', alpha=0.5)\n",
    "    ax3.set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Text Captions/Prompts\n",
    "    ax_captions = fig.add_subplot(gs[1, 1:3])\n",
    "    ax_captions.axis('off')\n",
    "    ax_captions.text(0.05, 0.95, 'Text Prompts:', \n",
    "                    fontsize=13, fontweight='bold', va='top')\n",
    "    \n",
    "    if captions:\n",
    "        caption_text = \"\\n\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
    "    else:\n",
    "        caption_text = \"No captions available for this image.\"\n",
    "    \n",
    "    ax_captions.text(0.05, 0.80, caption_text, \n",
    "                    fontsize=10, va='top', wrap=True,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    # Metadata\n",
    "    ax_meta = fig.add_subplot(gs[2, 0])\n",
    "    ax_meta.axis('off')\n",
    "    ax_meta.text(0.05, 0.95, 'Image Metadata:', \n",
    "                fontsize=12, fontweight='bold', va='top')\n",
    "    ax_meta.text(0.05, 0.75, metadata, \n",
    "                fontsize=10, va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Pose keypoint details\n",
    "    ax_kp = fig.add_subplot(gs[2, 1:3])\n",
    "    ax_kp.axis('off')\n",
    "    \n",
    "    keypoint_names = [\n",
    "        'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "        'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "        'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "        'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "    ]\n",
    "    \n",
    "    keypoint_text = \"Keypoint Details:\\n\" + \"â”€\" * 40 + \"\\n\"\n",
    "    for i, (x, y, v) in enumerate(sample['raw_keypoints']):\n",
    "        if v > 0:  # visible keypoint\n",
    "            visibility = \"visible\" if v == 2 else \"occluded\"\n",
    "            keypoint_text += f\"{keypoint_names[i]:15s}: ({int(x):3d}, {int(y):3d}) - {visibility}\\n\"\n",
    "    \n",
    "    ax_kp.text(0.05, 0.95, keypoint_text, \n",
    "              fontsize=9, va='top', family='monospace')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image shape: {sample['image'].shape}\")\n",
    "    print(f\"Pose skeleton shape: {pose_map.shape}\")\n",
    "    print(f\"Number of visible keypoints: {(sample['raw_keypoints'][:, 2] > 0).sum()}\")\n",
    "    print(f\"Number of captions: {len(captions)}\")\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "    from diffusers.optimization import get_scheduler\n",
    "    from transformers import CLIPTextModel, CLIPTokenizer\n",
    "    from accelerate import Accelerator\n",
    "    from tqdm.auto import tqdm\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"Imports successful.\")\n",
    "\n",
    "    # Training Configuration\n",
    "    class TrainingConfig:\n",
    "        # Model settings\n",
    "        pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        controlnet_conditioning_channels = 1  # Grayscale pose skeleton\n",
    "        \n",
    "        # Training settings\n",
    "        num_training_samples = None  # None means use all images\n",
    "        num_epochs = 50\n",
    "        train_batch_size = 1\n",
    "        gradient_accumulation_steps = 8\n",
    "        learning_rate = 1e-4\n",
    "        lr_warmup_steps = 375\n",
    "        lr_scheduler_type = \"consine\"\n",
    "        caption_dropout_prob = 0.5\n",
    "        \n",
    "        # Data paths\n",
    "        train_captions_file = '/kaggle/input/captions/train_captions.json'\n",
    "        \n",
    "        # Image settings\n",
    "        resolution = 512\n",
    "        \n",
    "        # Checkpointing & Validation\n",
    "        output_dir = \"/kaggle/working/controlnet_pose_output\"\n",
    "        validate_every_n_epochs = 1  # Generate validation samples every N epochs\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir = \"./logs\"\n",
    "        report_to = \"tensorboard\"\n",
    "        \n",
    "        # Hardware & Optimization\n",
    "        mixed_precision = \"fp16\"  # Use \"bf16\" if available, \"no\" for CPU\n",
    "        gradient_checkpointing = True\n",
    "        use_8bit_optimizer = False\n",
    "        \n",
    "        # Validation\n",
    "        validation_steps = 500\n",
    "        num_validation_images = 4\n",
    "        validation_prompt = \"a person standing\"\n",
    "    \n",
    "    config = TrainingConfig()\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    os.makedirs(config.logging_dir, exist_ok=True)\n",
    "    \n",
    "    import random\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    # Define collate function with caption dropout\n",
    "    def collate_fn(batch):\n",
    "        images = []\n",
    "        poses = []\n",
    "        captions = []\n",
    "        \n",
    "        for sample in batch:\n",
    "            images.append(sample['image'])\n",
    "            \n",
    "            # Convert pose PIL Image to tensor and normalize to [-1, 1]\n",
    "            pose_tensor = transforms.ToTensor()(sample['pose'])\n",
    "            poses.append(pose_tensor * 2 - 1)  # Normalize from [0, 1] to [-1, 1]\n",
    "            \n",
    "            # Caption dropout: 50% chance to use empty caption\n",
    "            caption = sample['captions'][0] if sample['captions'] else \"\"\n",
    "            if random.random() < config.caption_dropout_prob:\n",
    "                caption = \"\"\n",
    "            captions.append(caption)\n",
    "        \n",
    "        return {\n",
    "            \"images\": torch.stack(images),\n",
    "            \"poses\": torch.stack(poses),\n",
    "            \"captions\": captions\n",
    "        }\n",
    "    \n",
    "    # Create training dataloader with caption dropout\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "        \n",
    "    # Initialize models\n",
    "    print(\"Loading pretrained models...\")\n",
    "    \n",
    "    # Load tokenizer and text encoder\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        config.pretrained_model_name, \n",
    "        subfolder=\"tokenizer\"\n",
    "    )\n",
    "    \n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        config.pretrained_model_name, \n",
    "        subfolder=\"text_encoder\"\n",
    "    )\n",
    "    \n",
    "    # Load VAE\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        config.pretrained_model_name, \n",
    "        subfolder=\"vae\"\n",
    "    )\n",
    "    \n",
    "    # Load UNet\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        config.pretrained_model_name, \n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "    \n",
    "    # Initialize ControlNet from UNet\n",
    "    print(\"Initializing ControlNet...\")\n",
    "    controlnet = ControlNetModel.from_unet(\n",
    "        unet,\n",
    "        conditioning_channels=config.controlnet_conditioning_channels\n",
    "    )\n",
    "    \n",
    "    # Load noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        config.pretrained_model_name,\n",
    "        subfolder=\"scheduler\"\n",
    "    )\n",
    "    \n",
    "    # Freeze VAE and text encoder - we only train ControlNet\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    \n",
    "    print(\"Models loaded successfully.\")\n",
    "\n",
    "    # Clear memory after model loading\n",
    "    clear_memory()\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Setup optimizer and learning rate scheduler\n",
    "        \n",
    "    if config.use_8bit_optimizer:\n",
    "        import bitsandbytes as bnb\n",
    "        \n",
    "        optimizer = bnb.optim.AdamW8bit(\n",
    "            controlnet.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-3,\n",
    "            eps=1e-8,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        # Standard AdamW optimizer\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            controlnet.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-3,\n",
    "            eps=1e-8,\n",
    "        )\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
    "        num_training_steps=len(train_dataloader) * config.num_epochs * config.gradient_accumulation_steps,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Initialize Accelerator for distributed training and mixed precision\n",
    "        \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        log_with=config.report_to,\n",
    "        project_dir=config.logging_dir,\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if config.gradient_checkpointing:\n",
    "        controlnet.enable_gradient_checkpointing()\n",
    "    \n",
    "    # Prepare models with accelerator\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        controlnet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    # Move models to device\n",
    "    unet.to(accelerator.device)\n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    \n",
    "    # Set models to eval mode (only ControlNet is in training mode)\n",
    "    unet.eval()\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    # For training on multi-gpu\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    import math\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    def train_controlnet():\n",
    "        \"\"\"\n",
    "        Training function with multi-GPU support via DataParallel\n",
    "        Prints epoch-level training loss summary.\n",
    "        \"\"\"\n",
    "        \n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        \n",
    "        device = accelerator.device\n",
    "        weight_dtype = torch.float32\n",
    "        if accelerator.mixed_precision == \"fp16\":\n",
    "            weight_dtype = torch.float16\n",
    "        elif accelerator.mixed_precision == \"bf16\":\n",
    "            weight_dtype = torch.bfloat16\n",
    "            \n",
    "        # Cast frozen models to weight_dtype for memory efficiency\n",
    "        vae_model = vae.module if isinstance(vae, torch.nn.DataParallel) else vae\n",
    "        text_encoder_model = text_encoder.module if isinstance(text_encoder, torch.nn.DataParallel) else text_encoder\n",
    "        unet_model = unet.module if isinstance(unet, torch.nn.DataParallel) else unet\n",
    "        \n",
    "        vae_model.to(device, dtype=weight_dtype)\n",
    "        text_encoder_model.to(device, dtype=weight_dtype)\n",
    "        unet_model.to(device, dtype=weight_dtype)\n",
    "    \n",
    "        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / config.gradient_accumulation_steps)\n",
    "        max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "        progress_bar = tqdm(\n",
    "            range(max_train_steps), \n",
    "            desc=\"Steps\", \n",
    "            disable=not accelerator.is_local_main_process\n",
    "        )\n",
    "        \n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(config.num_epochs):\n",
    "            controlnet.train()\n",
    "            train_loss = 0.0\n",
    "            epoch_loss_sum = 0.0\n",
    "            epoch_loss_count = 0\n",
    "            \n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                with accelerator.accumulate(controlnet):\n",
    "                    pixel_values = batch[\"images\"].to(device, dtype=torch.float32)\n",
    "                    controlnet_image = batch[\"poses\"].to(device, dtype=torch.float32)\n",
    "                    captions = batch[\"captions\"]\n",
    "                    \n",
    "                    with accelerator.autocast():\n",
    "                        # VAE encode\n",
    "                        with torch.no_grad():\n",
    "                            latents = vae_model.encode(pixel_values.to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                            latents = latents * vae_model.config.scaling_factor\n",
    "    \n",
    "                        # Add noise\n",
    "                        noise = torch.randn_like(latents)\n",
    "                        bsz = latents.shape[0]\n",
    "                        timesteps = torch.randint(\n",
    "                            0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device\n",
    "                        ).long()\n",
    "    \n",
    "                        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    \n",
    "                        # Text embeddings\n",
    "                        with torch.no_grad():\n",
    "                            inputs = tokenizer(\n",
    "                                captions, \n",
    "                                max_length=tokenizer.model_max_length, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\"\n",
    "                            )\n",
    "                            encoder_hidden_states = text_encoder_model(inputs.input_ids.to(device))[0]\n",
    "    \n",
    "                        # ControlNet forward\n",
    "                        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states=encoder_hidden_states,\n",
    "                            controlnet_cond=controlnet_image,\n",
    "                            return_dict=False,\n",
    "                        )\n",
    "    \n",
    "                        # UNet forward\n",
    "                        model_pred = unet_model(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states=encoder_hidden_states,\n",
    "                            down_block_additional_residuals=down_block_res_samples,\n",
    "                            mid_block_additional_residual=mid_block_res_sample,\n",
    "                        ).sample\n",
    "    \n",
    "                        # Loss\n",
    "                        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "    \n",
    "                    # Backprop\n",
    "                    avg_loss = accelerator.gather(loss.repeat(config.train_batch_size)).mean()\n",
    "                    train_loss += avg_loss.item() / config.gradient_accumulation_steps\n",
    "                    \n",
    "                    accelerator.backward(loss)\n",
    "                    \n",
    "                    if accelerator.sync_gradients:\n",
    "                        accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        progress_bar.update(1)\n",
    "                        global_step += 1\n",
    "                        \n",
    "                        # Accumulate epoch loss summary\n",
    "                        epoch_loss_sum += avg_loss.item()\n",
    "                        epoch_loss_count += 1\n",
    "                        \n",
    "                        logs = {\"loss\": train_loss, \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                        progress_bar.set_postfix(**logs)\n",
    "                        accelerator.log(logs, step=global_step)\n",
    "                        train_loss = 0.0\n",
    "    \n",
    "                        if hasattr(config, 'checkpointing_steps') and global_step % config.checkpointing_steps == 0:\n",
    "                             save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                             accelerator.save_state(save_path)\n",
    "    \n",
    "            # End of Epoch\n",
    "            accelerator.wait_for_everyone()\n",
    "            if accelerator.is_main_process:\n",
    "                # Print epoch-level training loss summary\n",
    "                if epoch_loss_count > 0:\n",
    "                    epoch_loss_avg = epoch_loss_sum / epoch_loss_count\n",
    "                    accelerator.print(f\"Epoch {epoch+1}/{config.num_epochs} - Training Loss: {epoch_loss_avg:.6f}\")\n",
    "                    accelerator.log({\"train_loss_epoch\": epoch_loss_avg}, step=global_step)\n",
    "    \n",
    "                controlnet_unwrapped = accelerator.unwrap_model(controlnet)\n",
    "            \n",
    "                save_path = os.path.join(config.output_dir, f\"epoch-{epoch+1}\")\n",
    "                \n",
    "                controlnet_unwrapped.save_pretrained(\n",
    "                    save_path,\n",
    "                    save_function=accelerator.save\n",
    "                )\n",
    "\n",
    "                accelerator.print(f\"Epoch {epoch+1} Saved: {save_path}\")\n",
    "                \n",
    "                # Auto-cleanup: Delete previous epoch checkpoint\n",
    "                if epoch > 0:\n",
    "                    prev_epoch_path = os.path.join(config.output_dir, f\"epoch-{epoch}\")\n",
    "                    if os.path.exists(prev_epoch_path):\n",
    "                        import shutil\n",
    "                        shutil.rmtree(prev_epoch_path)\n",
    "                        accelerator.print(f\"Cleaned up previous checkpoint: epoch-{epoch}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "        # Final Save\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            controlnet_unwrapped = controlnet.module if isinstance(controlnet, torch.nn.DataParallel) else controlnet\n",
    "            final_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "            controlnet_unwrapped.save_pretrained(final_path)\n",
    "            accelerator.print(f\"Training Complete. Final model saved to {final_path}\")\n",
    "        \n",
    "        return os.path.join(config.output_dir, \"controlnet_final\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "    trained_controlnet = train_controlnet()\n",
    "        \n",
    "    print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T05:54:11.368238Z",
     "iopub.status.busy": "2025-12-15T05:54:11.368048Z",
     "iopub.status.idle": "2025-12-15T05:54:11.376981Z",
     "shell.execute_reply": "2025-12-15T05:54:11.376323Z",
     "shell.execute_reply.started": "2025-12-15T05:54:11.368212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch --multi_gpu --num_processes=2 --mixed-precision=fp16 train_human_pose.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8887620,
     "sourceId": 14095779,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8999286,
     "sourceId": 14124750,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
