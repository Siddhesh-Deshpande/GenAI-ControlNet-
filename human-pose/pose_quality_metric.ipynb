{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2f2e8c4",
      "metadata": {
        "id": "e2f2e8c4"
      },
      "source": [
        "# COCO Pose Estimation Quality Metric\n",
        "\n",
        "**Evaluating ControlNet-Generated Poses using YOLOv8-Pose & Procrustes Alignment**\n",
        "\n",
        "This notebook compares poses from ControlNet-generated images against ground truth COCO keypoints using:\n",
        "1. **COCO API** to extract ground truth keypoints from dataset\n",
        "2. **YOLOv8-Pose** unified model for keypoint detection from generated images\n",
        "3. **Procrustes Alignment** (translation + rotation + scale) to isolate pose shape\n",
        "4. **MPJPE** (primary metric) and **OKS** (secondary metric) for evaluation\n",
        "\n",
        "**Key Innovation:**\n",
        "- Procrustes alignment removes location, size, and orientation differences â†’ measures pure **pose shape similarity**\n",
        "- MPJPE: Mean Per Joint Position Error (lower = better)\n",
        "- OKS: Object Keypoint Similarity (higher = better)\n",
        "\n",
        "**Workflow:**\n",
        "- Load 300 images from val_captions.json\n",
        "- Extract COCO ground truth keypoints\n",
        "- Run YOLOv8-Pose on generated images\n",
        "- Align predictions using Procrustes analysis\n",
        "- Compare using MPJPE and OKS metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0iJpNE7qIPM-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iJpNE7qIPM-",
        "outputId": "591b6bf8-f976-47a8-ea75-d55bdb6c9b74"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5c5d09",
      "metadata": {
        "id": "9d5c5d09"
      },
      "source": [
        "## 1. Install & Verify Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec510f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ec510f1",
        "outputId": "231ba188-f644-42f2-ccf2-a543fccd49bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install -q numpy pandas matplotlib seaborn scipy\n",
        "%pip install -q opencv-python\n",
        "%pip install -q pycocotools\n",
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "\n",
        "print(\"Installing YOLOv8-Pose (ultralytics)...\")\n",
        "%pip install -q -U ultralytics torch torchvision\n",
        "\n",
        "print(\"Installation complete (YOLOv8-Pose)\")\n",
        "print(\"No MMPose/MMDetection/OpenMMLab compilation issues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ec3701",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ec3701",
        "outputId": "6faad361-344c-4c3a-f232-fd6318308dda"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import torch\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Torch:   {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(f\"Ultralytics (YOLOv8): imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Ultralytics import failed: {type(e).__name__}: {e}\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    print(f\"Core libs (cv2, numpy, pandas): imported successfully\")\n",
        "    print(\"\\nAll packages loaded successfully (YOLOv8-Pose ready!)\")\n",
        "except Exception as e:\n",
        "    print(f\"Core libs import failed: {type(e).__name__}: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2b4759",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2b4759",
        "outputId": "ef30782b-bdd9-4eda-ceb9-4f80f38e63a0"
      },
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "VAL_CAPTIONS_PATH = \"/content/val_captions.json\"\n",
        "GENERATED_IMAGES_PATH = \"/content/drive/MyDrive/generatedimages\"  # e.g., \"./generated_images\"\n",
        "NUM_IMAGES = 300\n",
        "\n",
        "\n",
        "YOLO_MODEL = 'yolov8m-pose.pt'  \n",
        "CONF_THRESHOLD = 0.4  # confidence threshold for detections\n",
        "\n",
        "\n",
        "COCO_KEYPOINT_NAMES = [\n",
        "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
        "]\n",
        "\n",
        "\n",
        "SKELETON = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4), (3, 5), (4, 6),\n",
        "    (5, 7), (7, 9), (6, 8), (8, 10), (5, 11), (6, 12),\n",
        "    (11, 13), (13, 15), (12, 14), (14, 16)\n",
        "]\n",
        "\n",
        "print(\" Imports & config loaded\")\n",
        "print(f\"  Val Captions: {VAL_CAPTIONS_PATH}\")\n",
        "print(f\"  Generated Images: {GENERATED_IMAGES_PATH}\")\n",
        "print(f\"  Model: {YOLO_MODEL} (YOLOv8-Pose)\")\n",
        "print(f\"  Confidence threshold: {CONF_THRESHOLD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa77bfe8",
      "metadata": {
        "id": "aa77bfe8"
      },
      "source": [
        "## 2. Configuration & Dataset Setup\n",
        "\n",
        "Load COCO annotations and extract ground truth keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce9cfc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ce9cfc3",
        "outputId": "0e04d6aa-0e73-45f6-bd4e-a5444148c791"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "print(\"Loading validation captions...\")\n",
        "with open(VAL_CAPTIONS_PATH, 'r') as f:\n",
        "    val_captions_raw = json.load(f)\n",
        "\n",
        "\n",
        "image_ids = []\n",
        "\n",
        "if isinstance(val_captions_raw, dict):\n",
        "    sample_key = next(iter(val_captions_raw.keys()), None)\n",
        "\n",
        "    # Case 1: filename -> caption mapping\n",
        "    if sample_key and isinstance(sample_key, str) and sample_key.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        print(\"Detected filename-based caption format\")\n",
        "\n",
        "        for filename in list(val_captions_raw.keys())[:NUM_IMAGES]:\n",
        "            basename = filename.rsplit('.', 1)[0]\n",
        "            try:\n",
        "                image_ids.append(int(basename))\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "    # Case 2: COCO-style annotation dictionaries\n",
        "    elif 'annotations' in val_captions_raw:\n",
        "        for cap in val_captions_raw['annotations'][:NUM_IMAGES]:\n",
        "            if isinstance(cap, dict):\n",
        "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
        "                if img_id is not None:\n",
        "                    image_ids.append(img_id)\n",
        "\n",
        "    elif 'images' in val_captions_raw:\n",
        "        for cap in val_captions_raw['images'][:NUM_IMAGES]:\n",
        "            if isinstance(cap, dict):\n",
        "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
        "                if img_id is not None:\n",
        "                    image_ids.append(img_id)\n",
        "\n",
        "    # Fallback: dictionary values containing image metadata\n",
        "    else:\n",
        "        for cap in list(val_captions_raw.values())[:NUM_IMAGES]:\n",
        "            if isinstance(cap, dict):\n",
        "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
        "                if img_id is not None:\n",
        "                    image_ids.append(img_id)\n",
        "\n",
        "elif isinstance(val_captions_raw, list):\n",
        "    for cap in val_captions_raw[:NUM_IMAGES]:\n",
        "        if isinstance(cap, dict):\n",
        "            img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
        "            if img_id is not None:\n",
        "                image_ids.append(img_id)\n",
        "\n",
        "image_ids = list(dict.fromkeys(image_ids))  # preserve order, remove duplicates\n",
        "\n",
        "if not image_ids:\n",
        "    raise ValueError(\"Failed to extract image IDs from validation captions\")\n",
        "\n",
        "print(f\"Extracted {len(image_ids)} image IDs\")\n",
        "\n",
        "\n",
        "print(\"Loading COCO keypoint annotations...\")\n",
        "\n",
        "cache_dir = Path.home() / \".cache\" / \"coco_annotations\"\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_file = cache_dir / \"person_keypoints_val2017.json\"\n",
        "\n",
        "if cache_file.exists():\n",
        "    with open(cache_file, 'r') as f:\n",
        "        coco_annotations = json.load(f)\n",
        "else:\n",
        "    zip_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "    zip_path = cache_dir / \"annotations_trainval2017.zip\"\n",
        "\n",
        "    urllib.request.urlretrieve(zip_url, zip_path)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        target = \"annotations/person_keypoints_val2017.json\"\n",
        "        zip_ref.extract(target, cache_dir)\n",
        "\n",
        "    extracted_path = cache_dir / target\n",
        "    extracted_path.rename(cache_file)\n",
        "    (cache_dir / \"annotations\").rmdir()\n",
        "    zip_path.unlink()\n",
        "\n",
        "    with open(cache_file, 'r') as f:\n",
        "        coco_annotations = json.load(f)\n",
        "\n",
        "print(\"COCO annotations loaded\")\n",
        "\n",
        "\n",
        "coco_images = {img['id']: img for img in coco_annotations.get('images', [])}\n",
        "coco_annotations_by_img = {}\n",
        "\n",
        "for ann in coco_annotations.get('annotations', []):\n",
        "    coco_annotations_by_img.setdefault(ann['image_id'], []).append(ann)\n",
        "\n",
        "\n",
        "gt_keypoints_dict = {}\n",
        "\n",
        "print(f\"Extracting ground-truth keypoints for {len(image_ids)} images...\")\n",
        "for idx, img_id in enumerate(image_ids):\n",
        "    anns = coco_annotations_by_img.get(img_id, [])\n",
        "    if not anns:\n",
        "        continue\n",
        "\n",
        "    gt_keypoints_dict[img_id] = {}\n",
        "    for ann in anns:\n",
        "        if 'keypoints' in ann:\n",
        "            kpts = np.array(ann['keypoints']).reshape(17, 3)\n",
        "            gt_keypoints_dict[img_id][ann['id']] = {\n",
        "                'keypoints': kpts,\n",
        "                'bbox': ann.get('bbox'),\n",
        "                'area': ann.get('area'),\n",
        "                'iscrowd': ann.get('iscrowd', 0),\n",
        "                'category_id': ann.get('category_id')\n",
        "            }\n",
        "\n",
        "print(f\"Images with GT annotations: {len(gt_keypoints_dict)}\")\n",
        "print(f\"Total GT instances: {sum(len(v) for v in gt_keypoints_dict.values())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044ce29c",
      "metadata": {
        "id": "044ce29c"
      },
      "source": [
        "## 3. Load YOLOv8-Pose Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18ac3e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e18ac3e0",
        "outputId": "7a38f199-ada8-42ba-a969-3e7d9f9b3394"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(\"Loading YOLOv8-Pose model...\")\n",
        "print(\"(Model will auto-download on first run)\\n\")\n",
        "\n",
        "try:\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    \n",
        "    yolo_model = YOLO(YOLO_MODEL)\n",
        "    yolo_model.to(device)\n",
        "\n",
        "    print(f\"YOLOv8-Pose model loaded successfully\")\n",
        "    print(f\"  Model: {YOLO_MODEL}\")\n",
        "    print(f\"  Device: {device}\")\n",
        "    print(f\"  Unified detection + pose (no separate pipeline)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLOv8-Pose: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f2df5a4",
      "metadata": {
        "id": "2f2df5a4"
      },
      "source": [
        "## 4. Helper Functions & Metrics\n",
        "\n",
        "Procrustes Alignment + MPJPE + OKS computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b708c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "14b708c6",
        "outputId": "59caa8ec-0828-4237-ffd2-8ae9cd91bd37"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def extract_keypoints_from_generated_image(image_path: str) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Runs YOLOv8-Pose to detect persons and extract 17-keypoint poses.\n",
        "    Returns a list of (17, 3) arrays [x, y, confidence] per detected person,\n",
        "    or None if no valid poses are found.\n",
        "    \"\"\"\n",
        "    global yolo_model\n",
        "\n",
        "    results = yolo_model(image_path, conf=CONF_THRESHOLD, verbose=False)\n",
        "    if not results:\n",
        "        return None\n",
        "\n",
        "    result = results[0]\n",
        "    if result.keypoints is None or len(result.keypoints) == 0:\n",
        "        return None\n",
        "\n",
        "    keypoints_xy = result.keypoints.xy\n",
        "    confidences = result.keypoints.conf\n",
        "    if keypoints_xy is None or len(keypoints_xy) == 0:\n",
        "        return None\n",
        "\n",
        "    detections = []\n",
        "    for i in range(len(keypoints_xy)):\n",
        "        kpts_xy = keypoints_xy[i].cpu().numpy()\n",
        "        kpts_conf = confidences[i].cpu().numpy().reshape(17, 1)\n",
        "        detections.append(np.concatenate([kpts_xy, kpts_conf], axis=1))\n",
        "\n",
        "    return detections if detections else None\n",
        "\n",
        "\n",
        "def procrustes_align(gt_keypoints: np.ndarray, pred_keypoints: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Aligns predicted keypoints to ground truth using Procrustes analysis.\n",
        "    Removes effects of translation, rotation, and uniform scaling so that\n",
        "    evaluation focuses on pose shape rather than absolute position.\n",
        "    \"\"\"\n",
        "    gt_visible = gt_keypoints[:, 2] > 0\n",
        "    pred_visible = pred_keypoints[:, 2] > 0\n",
        "    common_visible = gt_visible & pred_visible\n",
        "\n",
        "    if common_visible.sum() < 3:\n",
        "        return pred_keypoints\n",
        "\n",
        "    gt_pts = gt_keypoints[common_visible, :2]\n",
        "    pred_pts = pred_keypoints[common_visible, :2]\n",
        "\n",
        "    gt_centroid = gt_pts.mean(axis=0)\n",
        "    pred_centroid = pred_pts.mean(axis=0)\n",
        "\n",
        "    gt_centered = gt_pts - gt_centroid\n",
        "    pred_centered = pred_pts - pred_centroid\n",
        "\n",
        "    H = pred_centered.T @ gt_centered\n",
        "    U, S, Vt = np.linalg.svd(H)\n",
        "    R = Vt.T @ U.T\n",
        "\n",
        "    if np.linalg.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt.T @ U.T\n",
        "\n",
        "    pred_norm = np.sum(pred_centered ** 2)\n",
        "    scale = np.trace(np.diag(S)) / pred_norm if pred_norm > 0 else 1.0\n",
        "\n",
        "    aligned = pred_keypoints.copy()\n",
        "    aligned[:, :2] -= pred_centroid\n",
        "    aligned[:, :2] *= scale\n",
        "    aligned[:, :2] = (R @ aligned[:, :2].T).T\n",
        "    aligned[:, :2] += gt_centroid\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def compute_mpjpe(gt_keypoints: np.ndarray, pred_keypoints: np.ndarray) -> Dict:\n",
        "    \"\"\"\n",
        "    Computes Mean Per Joint Position Error (MPJPE) over visible ground-truth joints.\n",
        "    \"\"\"\n",
        "    visible = gt_keypoints[:, 2] > 0\n",
        "    if visible.sum() == 0:\n",
        "        return {\n",
        "            'mpjpe': 0.0,\n",
        "            'per_joint_errors': np.zeros(17),\n",
        "            'num_visible': 0\n",
        "        }\n",
        "\n",
        "    diff = pred_keypoints[:, :2] - gt_keypoints[:, :2]\n",
        "    distances = np.linalg.norm(diff, axis=1)\n",
        "\n",
        "    return {\n",
        "        'mpjpe': float(distances[visible].mean()),\n",
        "        'per_joint_errors': distances,\n",
        "        'num_visible': int(visible.sum())\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_oks(gt_keypoints: np.ndarray, pred_keypoints: np.ndarray, bbox: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes Object Keypoint Similarity (OKS) following the COCO formulation.\n",
        "    \"\"\"\n",
        "    sigmas = np.array([\n",
        "        .26, .25, .25, .35, .35, .79, .79, .72, .72,\n",
        "        .62, .62, 1.07, 1.07, .87, .87, .89, .89\n",
        "    ]) / 10.0\n",
        "\n",
        "    x, y, w, h = bbox\n",
        "    scale = w * h\n",
        "    if scale <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    visible = gt_keypoints[:, 2] > 0\n",
        "    if visible.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    dx = pred_keypoints[:, 0] - gt_keypoints[:, 0]\n",
        "    dy = pred_keypoints[:, 1] - gt_keypoints[:, 1]\n",
        "    d2 = dx**2 + dy**2\n",
        "\n",
        "    oks_per_kpt = np.exp(-d2 / (2 * scale * sigmas**2))\n",
        "    return float((oks_per_kpt * visible).sum() / visible.sum())\n",
        "\n",
        "\n",
        "print(\"Pose evaluation utilities initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b6e4fe",
      "metadata": {
        "id": "f3b6e4fe"
      },
      "source": [
        "## 5. Extract Keypoints from Generated Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6996240e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6996240e",
        "outputId": "519c72b3-0eec-4b08-b7d5-8bdf6f060bda"
      },
      "outputs": [],
      "source": [
        "\n",
        "gen_images_dir = Path(GENERATED_IMAGES_PATH)\n",
        "if not gen_images_dir.exists():\n",
        "    raise FileNotFoundError(f\"Generated images directory not found: {GENERATED_IMAGES_PATH}\")\n",
        "\n",
        "image_extensions = ['.png', '.jpg', '.jpeg']\n",
        "generated_image_files = []\n",
        "for ext in image_extensions:\n",
        "    generated_image_files.extend(gen_images_dir.glob(f'*{ext}'))\n",
        "    generated_image_files.extend(gen_images_dir.glob(f'*{ext.upper()}'))\n",
        "\n",
        "# Deduplicate\n",
        "generated_image_files = list(dict.fromkeys(generated_image_files))\n",
        "print(f\"Found {len(generated_image_files)} generated images in {GENERATED_IMAGES_PATH}\")\n",
        "print(f\"Need matches for {len(image_ids)} validation captions\")\n",
        "\n",
        "# Helper: find file whose name contains the image id\n",
        "def find_generated_image_for_id(img_id: int) -> Path:\n",
        "    id_plain = str(img_id)\n",
        "    id_padded = f\"{img_id:012d}\"\n",
        "    candidates = []\n",
        "    for f in generated_image_files:\n",
        "        name = f.name\n",
        "        if not name.lower().startswith(\"generated_\"):\n",
        "            continue\n",
        "        if id_plain in name or id_padded in name:\n",
        "            candidates.append(f)\n",
        "    if not candidates:\n",
        "        return None\n",
        "    # Prefer exact plain match, then padded, else first candidate\n",
        "    for f in candidates:\n",
        "        if f\"generated_{id_plain}\" in f.name:\n",
        "            return f\n",
        "    for f in candidates:\n",
        "        if f\"generated_{id_padded}\" in f.name:\n",
        "            return f\n",
        "    return sorted(candidates)[0]\n",
        "\n",
        "print(\"\\n Testing YOLOv8-Pose extraction on first image...\")\n",
        "test_img_id = image_ids[0]\n",
        "test_img_path = find_generated_image_for_id(test_img_id)\n",
        "if test_img_path:\n",
        "    print(f\"Test image: {test_img_path}\")\n",
        "    try:\n",
        "        test_kpts_list = extract_keypoints_from_generated_image(str(test_img_path))\n",
        "        if test_kpts_list:\n",
        "            print(f\" Extracted {len(test_kpts_list)} person(s) with keypoints; first shape: {test_kpts_list[0].shape}\")\n",
        "        else:\n",
        "            print(\" Extraction returned None or empty list\")\n",
        "    except Exception as e:\n",
        "        print(f\" Extraction error: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\" Could not find test image for ID {test_img_id}\")\n",
        "\n",
        "\n",
        "\n",
        "generated_keypoints_dict = {}  # {image_id: [keypoints_array_per_person]}\n",
        "missing_images = []\n",
        "extraction_failures = []\n",
        "\n",
        "print(\"\\nMatching generated images by filename substring (image id) ...\\n\")\n",
        "\n",
        "for idx, img_id in enumerate(image_ids):\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"  [{idx+1}/{len(image_ids)}] Processed\", end='\\r')\n",
        "\n",
        "    gen_img_path = find_generated_image_for_id(img_id)\n",
        "\n",
        "    if gen_img_path is None:\n",
        "        missing_images.append(img_id)\n",
        "        continue\n",
        "\n",
        "    # Extract keypoints (all detected persons)\n",
        "    try:\n",
        "        kpts_list = extract_keypoints_from_generated_image(str(gen_img_path))\n",
        "\n",
        "        if kpts_list:\n",
        "            generated_keypoints_dict[img_id] = kpts_list\n",
        "        else:\n",
        "            extraction_failures.append((img_id, \"Extraction returned None or empty\"))\n",
        "    except Exception as e:\n",
        "        error_msg = f\"{type(e).__name__}: {str(e)}\" if str(e) else type(e).__name__\n",
        "        extraction_failures.append((img_id, error_msg))\n",
        "\n",
        "print(f\"\\n\\n Processed {len(image_ids)} target images\")\n",
        "print(f\"  Successfully extracted (>=1 person): {len(generated_keypoints_dict)}\")\n",
        "print(f\"  Missing images: {len(missing_images)}\")\n",
        "print(f\"  Extraction failures: {len(extraction_failures)}\")\n",
        "\n",
        "if missing_images[:5]:\n",
        "    print(f\"\\n  First 5 missing image IDs: {missing_images[:5]}\")\n",
        "    print(f\"   Ensure filenames include the image id, e.g., generated_<id>.png\")\n",
        "\n",
        "if extraction_failures[:5]:\n",
        "    print(f\"\\n  First 5 extraction failures:\")\n",
        "    for img_id, reason in extraction_failures[:5]:\n",
        "        print(f\"    - Image {img_id}: {reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f11502c",
      "metadata": {
        "id": "2f11502c"
      },
      "source": [
        "## 6. Compute Pose Shape Similarity Metrics\n",
        "\n",
        "MPJPE (primary) and OKS (secondary) using Procrustes-aligned predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efead197",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efead197",
        "outputId": "a13b39ed-13d9-4f86-fdad-6abda5a0f2fa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Validate that we have both GT and predictions\n",
        "print(f\"\\nValidation:\")\n",
        "print(f\"  Images with GT keypoints: {len(gt_keypoints_dict)}\")\n",
        "print(f\"  Images with predicted keypoints (any person): {len(generated_keypoints_dict)}\")\n",
        "\n",
        "# Find intersection\n",
        "common_images = set(gt_keypoints_dict.keys()) & set(generated_keypoints_dict.keys())\n",
        "print(f\"  Images with both GT and predictions: {len(common_images)}\")\n",
        "\n",
        "if len(common_images) == 0:\n",
        "    print(\"\\n ERROR: No images have both GT and predicted keypoints!\")\n",
        "    print(\"Please check:\")\n",
        "    print(\"  1. Generated images are in the correct folder\")\n",
        "    print(\"  2. Image naming matches one of the patterns\")\n",
        "    print(\"  3. YOLOv8-Pose is correctly detecting poses\")\n",
        "    raise ValueError(\"No valid image pairs for comparison\")\n",
        "\n",
        "print(f\"\\n Ready to compare {len(common_images)} image-annotation pairs\")\n",
        "\n",
        "results = []\n",
        "best_generated_keypoints_dict_aligned = {}  \n",
        "best_generated_keypoints_dict_raw = {}      \n",
        "\n",
        "total_skipped_low_vis = 0\n",
        "comparison_count = 0\n",
        "\n",
        "print(\"\\n Strategy: Procrustes Alignment + MPJPE for Pose Shape Similarity\")\n",
        "print(\"    Translation: Centering to same location\")\n",
        "print(\"    Rotation: Optimal angle alignment\")\n",
        "print(\"    Scale: Uniform scaling to same size\")\n",
        "print(\"Measures PURE POSE SHAPE, ignoring location/size/orientation\\n\")\n",
        "\n",
        "for img_id in common_images:\n",
        "    gt_data = gt_keypoints_dict[img_id]\n",
        "\n",
        "    if len(gt_data) == 0:\n",
        "        continue\n",
        "\n",
        "    ann_id = list(gt_data.keys())[0]\n",
        "    gt_info = gt_data[ann_id]\n",
        "    gt_kpts = gt_info['keypoints']  \n",
        "    gt_bbox = gt_info['bbox'] \n",
        "\n",
        "    # Skip if labeled keypoints < 10\n",
        "    visible_count = int((gt_kpts[:, 2] > 0).sum())\n",
        "    if visible_count < 10:\n",
        "        total_skipped_low_vis += 1\n",
        "        continue\n",
        "\n",
        "    # Get predictions from generated image\n",
        "    if img_id not in generated_keypoints_dict:\n",
        "        continue\n",
        "\n",
        "    pred_kpts_list = generated_keypoints_dict[img_id]\n",
        "\n",
        "    if not pred_kpts_list or len(pred_kpts_list) == 0:\n",
        "        continue\n",
        "\n",
        "   \n",
        "    best_mpjpe = float('inf')\n",
        "    best_pred_aligned = None\n",
        "    best_pred_raw = None\n",
        "    best_oks = 0.0\n",
        "\n",
        "    for pred_kpts in pred_kpts_list:\n",
        "        # PROCRUSTES ALIGN: translation + rotation + scale\n",
        "        aligned_pred = procrustes_align(gt_kpts, pred_kpts)\n",
        "\n",
        "        \n",
        "        mpjpe_result = compute_mpjpe(gt_kpts, aligned_pred)\n",
        "        mpjpe = mpjpe_result['mpjpe']\n",
        "\n",
        "        \n",
        "        oks = compute_oks(gt_kpts, aligned_pred, gt_bbox)\n",
        "\n",
        "        # Keep track of best match (lowest MPJPE = best shape match)\n",
        "        if mpjpe < best_mpjpe:\n",
        "            best_mpjpe = mpjpe\n",
        "            best_pred_aligned = aligned_pred\n",
        "            best_pred_raw = pred_kpts\n",
        "            best_oks = oks\n",
        "\n",
        "    if best_pred_aligned is None:\n",
        "        continue\n",
        "\n",
        "    results.append({\n",
        "        'image_id': img_id,\n",
        "        'annotation_id': ann_id,\n",
        "        'mpjpe': best_mpjpe,  # Primary metric: lower = better pose shape match\n",
        "        'oks': best_oks,      # Secondary: higher = better\n",
        "        'num_detected_poses': len(pred_kpts_list)\n",
        "    })\n",
        "\n",
        "    best_generated_keypoints_dict_aligned[img_id] = best_pred_aligned\n",
        "    best_generated_keypoints_dict_raw[img_id] = best_pred_raw\n",
        "    comparison_count += 1\n",
        "\n",
        "print(f\"\\nCompared {comparison_count} image-annotation pairs\")\n",
        "print(f\"Skipped (labeled keypoints < 10): {total_skipped_low_vis}\")\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(f\"\\n MPJPE (Mean Per Joint Position Error) - PRIMARY METRIC:\")\n",
        "print(f\"   Lower is better (measures pose shape similarity)\")\n",
        "print(f\"  Mean:   {results_df['mpjpe'].mean():.2f} pixels\")\n",
        "print(f\"  Median: {results_df['mpjpe'].median():.2f} pixels\")\n",
        "print(f\"  Std:    {results_df['mpjpe'].std():.2f} pixels\")\n",
        "print(f\"  Min:    {results_df['mpjpe'].min():.2f} pixels (best)\")\n",
        "print(f\"  Max:    {results_df['mpjpe'].max():.2f} pixels (worst)\")\n",
        "\n",
        "print(f\"\\nOKS (Object Keypoint Similarity) - SECONDARY METRIC:\")\n",
        "print(f\"  Mean:   {results_df['oks'].mean():.4f}\")\n",
        "print(f\"  Median: {results_df['oks'].median():.4f}\")\n",
        "print(f\"  Std:    {results_df['oks'].std():.4f}\")\n",
        "print(f\"  Min:    {results_df['oks'].min():.4f}\")\n",
        "print(f\"  Max:    {results_df['oks'].max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a438b2f0",
      "metadata": {
        "id": "a438b2f0"
      },
      "source": [
        "## 7. Visualization: Best & Worst Pose Examples\n",
        "\n",
        "Ranked by MPJPE (lower = better pose shape match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa6459d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "dfa6459d",
        "outputId": "95e12529-e93b-4e4d-c563-984c70f89c6b"
      },
      "outputs": [],
      "source": [
        "def draw_skeleton(image, keypoints, skeleton, title=\"\"):\n",
        "    \"\"\"Draw skeleton on image.\"\"\"\n",
        "    image = image.copy()\n",
        "    # Draw keypoints\n",
        "    for kpt_idx, (x, y, conf) in enumerate(keypoints):\n",
        "        if conf > 0.3:  # Only draw if confidence > 0.3\n",
        "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
        "            cv2.putText(image, str(kpt_idx), (int(x) + 5, int(y) - 5),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
        "\n",
        "    # Draw skeleton connections\n",
        "    for start, end in skeleton:\n",
        "        if keypoints[start, 2] > 0.3 and keypoints[end, 2] > 0.3:\n",
        "            x1, y1, _ = keypoints[start]\n",
        "            x2, y2, _ = keypoints[end]\n",
        "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def find_generated_image_for_id(img_id: int) -> Path:\n",
        "    \"\"\"Locate generated image by id in GENERATED_IMAGES_PATH.\"\"\"\n",
        "    gen_dir = Path(GENERATED_IMAGES_PATH)\n",
        "    if not gen_dir.exists():\n",
        "        return None\n",
        "    id_plain = str(img_id)\n",
        "    id_padded = f\"{img_id:012d}\"\n",
        "    image_extensions = ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']\n",
        "    candidates = []\n",
        "    for ext in image_extensions:\n",
        "        for f in gen_dir.glob(f\"generated_*{ext}\"):\n",
        "            name = f.name\n",
        "            if id_plain in name or id_padded in name:\n",
        "                candidates.append(f)\n",
        "    if not candidates:\n",
        "        return None\n",
        "    # Prefer exact plain match, then padded\n",
        "    for f in candidates:\n",
        "        if f\"generated_{id_plain}\" in f.name:\n",
        "            return f\n",
        "    for f in candidates:\n",
        "        if f\"generated_{id_padded}\" in f.name:\n",
        "            return f\n",
        "    return sorted(candidates)[0]\n",
        "\n",
        "\n",
        "def find_coco_original_image(img_id: int) -> Path:\n",
        "    \"\"\"Download COCO image on-demand if not in cache.\"\"\"\n",
        "    import urllib.request\n",
        "    cache_dir = Path.home() / \".cache\" / \"coco_val2017\"\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    img_filename = f\"{img_id:012d}.jpg\"\n",
        "    cached_path = cache_dir / img_filename\n",
        "\n",
        "    if cached_path.exists():\n",
        "        return cached_path\n",
        "\n",
        "    \n",
        "    coco_url = f\"http://images.cocodataset.org/val2017/{img_filename}\"\n",
        "    try:\n",
        "        print(f\"  Downloading COCO image {img_id}...\", end='\\r')\n",
        "        urllib.request.urlretrieve(coco_url, cached_path)\n",
        "        return cached_path\n",
        "    except Exception as e:\n",
        "        print(f\"  Failed to download {img_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "print(\"Visualizing samples with best and worst pose shape similarity...\")\n",
        "print(\"(Ranked by MPJPE: lower = better pose shape match)\")\n",
        "print(\"(Left: Generated images | Right: Original COCO images with GT poses)\\n\")\n",
        "\n",
        "if results_df.empty:\n",
        "    print(\"No results to visualize. Run metric computation first.\")\n",
        "else:\n",
        "    # Sort by MPJPE: lowest = best shape match\n",
        "    best_indices = results_df.nsmallest(3, 'mpjpe').index\n",
        "    worst_indices = results_df.nlargest(3, 'mpjpe').index\n",
        "\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(20, 14))\n",
        "    fig.suptitle('Pose Shape Similarity: Best vs Worst Examples\\n(MPJPE metric: lower = better | Left: Generated | Right: Original COCO)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    for row, (best_idx, worst_idx) in enumerate(zip(best_indices, worst_indices)):\n",
        "        best_result = results_df.iloc[best_idx]\n",
        "        worst_result = results_df.iloc[worst_idx]\n",
        "\n",
        "        best_img_id = int(best_result['image_id'])\n",
        "        worst_img_id = int(worst_result['image_id'])\n",
        "\n",
        "        # BEST EXAMPLE\n",
        "        # Generated image\n",
        "        best_img_path = find_generated_image_for_id(best_img_id)\n",
        "        if best_img_path and best_img_id in best_generated_keypoints_dict_raw:\n",
        "            img = cv2.imread(str(best_img_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred_kpts = best_generated_keypoints_dict_raw[best_img_id]\n",
        "            img_with_pose = draw_skeleton(img_rgb, pred_kpts, SKELETON)\n",
        "            axes[row, 0].imshow(img_with_pose)\n",
        "            axes[row, 0].set_title(f'Best Generated (MPJPE={best_result[\"mpjpe\"]:.1f}px)',\n",
        "                                  fontsize=11, fontweight='bold', color='green')\n",
        "        else:\n",
        "            axes[row, 0].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "            axes[row, 0].set_title('Best (generated) missing', fontsize=11, color='red')\n",
        "        axes[row, 0].axis('off')\n",
        "\n",
        "        # Original COCO image\n",
        "        best_coco_path = find_coco_original_image(best_img_id)\n",
        "        if best_coco_path and best_img_id in gt_keypoints_dict:\n",
        "            img = cv2.imread(str(best_coco_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            ann_id = list(gt_keypoints_dict[best_img_id].keys())[0]\n",
        "            gt_kpts = gt_keypoints_dict[best_img_id][ann_id]['keypoints']\n",
        "            img_with_pose = draw_skeleton(img_rgb, gt_kpts, SKELETON)\n",
        "            axes[row, 1].imshow(img_with_pose)\n",
        "            axes[row, 1].set_title(f'Best Original COCO (OKS={best_result[\"oks\"]:.3f})',\n",
        "                                  fontsize=11, fontweight='bold', color='darkgreen')\n",
        "        else:\n",
        "            axes[row, 1].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "            axes[row, 1].set_title('Best (COCO) missing', fontsize=11, color='red')\n",
        "        axes[row, 1].axis('off')\n",
        "\n",
        "        # WORST EXAMPLE \n",
        "        # Generated image\n",
        "        worst_img_path = find_generated_image_for_id(worst_img_id)\n",
        "        if worst_img_path and worst_img_id in best_generated_keypoints_dict_raw:\n",
        "            img = cv2.imread(str(worst_img_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred_kpts = best_generated_keypoints_dict_raw[worst_img_id]\n",
        "            img_with_pose = draw_skeleton(img_rgb, pred_kpts, SKELETON)\n",
        "            axes[row, 2].imshow(img_with_pose)\n",
        "            axes[row, 2].set_title(f'Worst Generated (MPJPE={worst_result[\"mpjpe\"]:.1f}px)',\n",
        "                                  fontsize=11, fontweight='bold', color='red')\n",
        "        else:\n",
        "            axes[row, 2].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "            axes[row, 2].set_title('Worst (generated) missing', fontsize=11, color='red')\n",
        "        axes[row, 2].axis('off')\n",
        "\n",
        "        # Original COCO image\n",
        "        worst_coco_path = find_coco_original_image(worst_img_id)\n",
        "        if worst_coco_path and worst_img_id in gt_keypoints_dict:\n",
        "            img = cv2.imread(str(worst_coco_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            ann_id = list(gt_keypoints_dict[worst_img_id].keys())[0]\n",
        "            gt_kpts = gt_keypoints_dict[worst_img_id][ann_id]['keypoints']\n",
        "            img_with_pose = draw_skeleton(img_rgb, gt_kpts, SKELETON)\n",
        "            axes[row, 3].imshow(img_with_pose)\n",
        "            axes[row, 3].set_title(f'Worst Original COCO (OKS={worst_result[\"oks\"]:.3f})',\n",
        "                                  fontsize=11, fontweight='bold', color='darkred')\n",
        "        else:\n",
        "            axes[row, 3].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "            axes[row, 3].set_title('Worst (COCO) missing', fontsize=11, color='red')\n",
        "        axes[row, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MeVrmn27ZfeF",
      "metadata": {
        "id": "MeVrmn27ZfeF"
      },
      "source": [
        "## 8. Summary & Interpretation\n",
        "\n",
        "**MPJPE:** Mean Per Joint Position Error after Procrustes alignment (pixels)\n",
        "- Measures pure pose **shape** similarity independent of location/size/orientation\n",
        "- Lower is better\n",
        "\n",
        "**OKS:** Object Keypoint Similarity (reference metric)\n",
        "- Standard COCO evaluation metric\n",
        "- Higher is better\n",
        "\n",
        "**Note:** Both metrics computed on Procrustes-aligned keypoints to isolate pose shape quality from spatial differences."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
