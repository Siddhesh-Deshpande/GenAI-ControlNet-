{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefb3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:33.757141Z",
     "iopub.status.busy": "2025-12-10T09:46:33.756941Z",
     "iopub.status.idle": "2025-12-10T09:46:47.538489Z",
     "shell.execute_reply": "2025-12-10T09:46:47.537635Z"
    },
    "papermill": {
     "duration": 13.788067,
     "end_time": "2025-12-10T09:46:47.539725",
     "exception": false,
     "start_time": "2025-12-10T09:46:33.751658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful.\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0608466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.548072Z",
     "iopub.status.busy": "2025-12-10T09:46:47.547568Z",
     "iopub.status.idle": "2025-12-10T09:46:47.553035Z",
     "shell.execute_reply": "2025-12-10T09:46:47.552300Z"
    },
    "papermill": {
     "duration": 0.010547,
     "end_time": "2025-12-10T09:46:47.554060",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.543513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"âœ“ Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40859b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.568511Z",
     "iopub.status.busy": "2025-12-10T09:46:47.568286Z",
     "iopub.status.idle": "2025-12-10T09:46:47.577558Z",
     "shell.execute_reply": "2025-12-10T09:46:47.576924Z"
    },
    "papermill": {
     "duration": 0.014289,
     "end_time": "2025-12-10T09:46:47.578542",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.564253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Generation Configuration\n",
    "class Config:\n",
    "    # Get notebook directory for absolute paths\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "    \n",
    "    # Paths\n",
    "    COCO_ROOT = os.path.join(NOTEBOOK_DIR, 'coco_data')\n",
    "    OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, 'dataset_output')\n",
    "    \n",
    "    # Dataset sizes\n",
    "    TARGET_TRAIN = 5000  # Training images\n",
    "    TARGET_VAL = 500     # Validation images\n",
    "    \n",
    "    # Quality filtering thresholds\n",
    "    MIN_KEYPOINTS = 10   # Minimum visible keypoints (out of 17)\n",
    "    MIN_PERSON_AREA = 5000  # Minimum person area\n",
    "    \n",
    "    # Caption generation\n",
    "    MAX_CAPTION_TOKENS = 70  # CLIP limit is 77, we use 70 for safety\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Model - Using Qwen2-VL-2B\n",
    "    MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    MODEL_TYPE = \"qwen2-vl\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.COCO_ROOT, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'train2017'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'val2017'), exist_ok=True)\n",
    "\n",
    "print(f\"\\nDirectories created/verified:\")\n",
    "print(f\"- {config.COCO_ROOT}\")\n",
    "print(f\"- {config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ecae6",
   "metadata": {
    "papermill": {
     "duration": 0.003272,
     "end_time": "2025-12-10T09:46:47.585180",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.581908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Load COCO Annotations\n",
    "\n",
    "Download and load COCO 2017 annotations for person keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d1885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.592608Z",
     "iopub.status.busy": "2025-12-10T09:46:47.592426Z",
     "iopub.status.idle": "2025-12-10T09:47:03.710511Z",
     "shell.execute_reply": "2025-12-10T09:47:03.709677Z"
    },
    "papermill": {
     "duration": 16.123209,
     "end_time": "2025-12-10T09:47:03.711732",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.588523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "ann_dir = os.path.join(config.COCO_ROOT, 'annotations')\n",
    "train_ann_file = os.path.join(ann_dir, 'person_keypoints_train2017.json')\n",
    "val_ann_file = os.path.join(ann_dir, 'person_keypoints_val2017.json')\n",
    "\n",
    "# Download annotations if they don't exist\n",
    "if not os.path.exists(train_ann_file) or not os.path.exists(val_ann_file):\n",
    "    print(\"COCO annotations not found. Downloading...\")\n",
    "    \n",
    "    import zipfile\n",
    "    import urllib.request\n",
    "    \n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    zip_path = os.path.join(config.COCO_ROOT, 'annotations_trainval2017.zip')\n",
    "    \n",
    "    print(f\"Downloading from: {ann_url}\")\n",
    "    \n",
    "    # Download with progress\n",
    "    urllib.request.urlretrieve(ann_url, zip_path)\n",
    "    print(\"Download complet. Extracting...\")\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(config.COCO_ROOT)\n",
    "    \n",
    "    # Cleanup zip file\n",
    "    os.remove(zip_path)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "# Now load COCO annotations\n",
    "print(\"\\nLoading COCO annotations...\")\n",
    "coco_train = COCO(train_ann_file)\n",
    "coco_val = COCO(val_ann_file)\n",
    "print(\"COCO annotations loaded successfully.\")\n",
    "print(f\"- Train images: {len(coco_train.getImgIds())}\")\n",
    "print(f\"- Val images: {len(coco_val.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217e515",
   "metadata": {
    "papermill": {
     "duration": 0.003653,
     "end_time": "2025-12-10T09:47:03.719410",
     "exception": false,
     "start_time": "2025-12-10T09:47:03.715757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Filter High-Quality Pose Images\n",
    "\n",
    "Filter COCO dataset for images with high-quality pose annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f83c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:47:03.727674Z",
     "iopub.status.busy": "2025-12-10T09:47:03.727289Z",
     "iopub.status.idle": "2025-12-10T09:47:04.330887Z",
     "shell.execute_reply": "2025-12-10T09:47:04.329852Z"
    },
    "papermill": {
     "duration": 0.609069,
     "end_time": "2025-12-10T09:47:04.332024",
     "exception": false,
     "start_time": "2025-12-10T09:47:03.722955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_high_quality_images(coco, split='train', min_keypoints=10, min_area=5000, max_images=None):\n",
    "    \"\"\"\n",
    "    Filter COCO dataset for high-quality pose images\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_ids = coco.getCatIds(catNms=['person'])\n",
    "    all_img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "    \n",
    "    quality_images = []\n",
    "    \n",
    "    for img_id in tqdm(all_img_ids, desc=f\"Filtering {split} images\"):\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids, iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        best_quality_score = 0\n",
    "        best_keypoint_count = 0\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'keypoints' not in ann:\n",
    "                continue\n",
    "            \n",
    "            num_keypoints = ann.get('num_keypoints', 0)\n",
    "            person_area = ann.get('area', 0)\n",
    "            \n",
    "            # Quality filtering\n",
    "            if (num_keypoints >= min_keypoints and \n",
    "                person_area >= min_area and \n",
    "                ann.get('iscrowd', 0) == 0):\n",
    "                \n",
    "                # Quality score: combine keypoints and area\n",
    "                quality_score = num_keypoints * 1.0 + (person_area / 10000) * 0.5\n",
    "                \n",
    "                if quality_score > best_quality_score:\n",
    "                    best_quality_score = quality_score\n",
    "                    best_keypoint_count = num_keypoints\n",
    "        \n",
    "        if best_quality_score > 0:\n",
    "            quality_images.append({\n",
    "                'image_id': img_id,\n",
    "                'quality_score': best_quality_score,\n",
    "                'keypoints': best_keypoint_count\n",
    "            })\n",
    "    \n",
    "    # Sort by quality score (best first)\n",
    "    quality_images.sort(key=lambda x: x['quality_score'], reverse=True)\n",
    "    \n",
    "    # Limit to max_images if specified\n",
    "    if max_images and len(quality_images) > max_images:\n",
    "        quality_images = quality_images[:max_images]\n",
    "    \n",
    "    print(f\"\\nFiltered {len(quality_images)} high-quality images from {len(all_img_ids)} total\")\n",
    "    print(f\"- Average keypoints: {np.mean([img['keypoints'] for img in quality_images]):.1f}/17\")\n",
    "    print(f\"- Quality range: {quality_images[-1]['quality_score']:.2f} to {quality_images[0]['quality_score']:.2f}\")\n",
    "    \n",
    "    return quality_images\n",
    "\n",
    "# Check if COCO annotations are loaded\n",
    "if 'coco_train' not in globals() or 'coco_val' not in globals():\n",
    "    print(\"ERROR: COCO annotations not loaded.\")\n",
    "    raise NameError(\"coco_train and coco_val are not defined.\")\n",
    "\n",
    "# Filter train and val images\n",
    "train_images = filter_high_quality_images(\n",
    "    coco_train, \n",
    "    split='train',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_TRAIN\n",
    ")\n",
    "\n",
    "val_images = filter_high_quality_images(\n",
    "    coco_val, \n",
    "    split='val',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_VAL\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_images)} images selected\")\n",
    "print(f\"Val: {len(val_images)} images selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e86f9a",
   "metadata": {
    "papermill": {
     "duration": 0.004112,
     "end_time": "2025-12-10T09:47:04.340637",
     "exception": false,
     "start_time": "2025-12-10T09:47:04.336525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Initialize Caption Generator\n",
    "\n",
    "Load Qwen-VL-Chat model for generating CLIP-compatible captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d9146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:47:04.349730Z",
     "iopub.status.busy": "2025-12-10T09:47:04.349516Z",
     "iopub.status.idle": "2025-12-10T09:49:01.736907Z",
     "shell.execute_reply": "2025-12-10T09:49:01.736090Z"
    },
    "papermill": {
     "duration": 117.393323,
     "end_time": "2025-12-10T09:49:01.738014",
     "exception": false,
     "start_time": "2025-12-10T09:47:04.344691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required dependencies for Qwen2-VL-2B\n",
    "import subprocess\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "print(\"Installing dependencies for Qwen2-VL-2B...\")\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "# Install compatible versions\n",
    "packages = [\n",
    "    'transformers>=4.45.0',\n",
    "    'accelerate',\n",
    "    'qwen-vl-utils',\n",
    "    'pillow',\n",
    "    'torchvision'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"\\nAll dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b588e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:01.749496Z",
     "iopub.status.busy": "2025-12-10T09:49:01.748875Z",
     "iopub.status.idle": "2025-12-10T09:49:02.447915Z",
     "shell.execute_reply": "2025-12-10T09:49:02.447188Z"
    },
    "papermill": {
     "duration": 0.70583,
     "end_time": "2025-12-10T09:49:02.449052",
     "exception": false,
     "start_time": "2025-12-10T09:49:01.743222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU Memory Status Check\n",
    "import gc\n",
    "\n",
    "# Force garbage collection and clear cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"Total GPU Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Get current memory usage\n",
    "    current_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = props.total_memory / 1e9\n",
    "    free = total - (reserved)\n",
    "    \n",
    "    print(f\"\\nCurrent Memory Usage:\")\n",
    "    print(f\"- Allocated: {current_allocated:.2f} GB\")\n",
    "    print(f\"- Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"- Free: {free:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82393a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:02.460419Z",
     "iopub.status.busy": "2025-12-10T09:49:02.460156Z",
     "iopub.status.idle": "2025-12-10T09:49:02.471549Z",
     "shell.execute_reply": "2025-12-10T09:49:02.470948Z"
    },
    "papermill": {
     "duration": 0.018472,
     "end_time": "2025-12-10T09:49:02.472638",
     "exception": false,
     "start_time": "2025-12-10T09:49:02.454166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionGenerator:\n",
    "    \"\"\"\n",
    "    Caption generator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2-VL-2B-Instruct\"):\n",
    "        import gc\n",
    "        from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        # Pre-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Load processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Load model in fp16 for memory efficiency\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        ).eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully\")\n",
    "        \n",
    "        # Post-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        import gc\n",
    "        from PIL import Image as PILImage\n",
    "        \n",
    "        try:\n",
    "            # Pre-process cleanup\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Load and resize image\n",
    "            image = PILImage.open(image_path).convert('RGB')\n",
    "            \n",
    "            if max(image.size) > 768:\n",
    "                ratio = 768 / max(image.size)\n",
    "                new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "                image = image.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            \n",
    "            # Create prompt\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image for image generation. Include: main subjects, their positions, colors, lighting, mood, background, and style. Be specific, vivid, and under 70 words.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process with minimal overhead\n",
    "            text_prompt = self.processor.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=text_prompt,\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=70,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            # isolate generated tokens (remove prompt)\n",
    "            gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            \n",
    "            # decode only the assistant's answer\n",
    "            caption = self.processor.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Remove common prefixes\n",
    "            for prefix in ['assistant:', '<|assistant|>', 'sure ', 'here ', 'the image']:\n",
    "                if caption.lower().startswith(prefix):\n",
    "                    caption = caption[len(prefix):].strip()\n",
    "            \n",
    "            # Enforce 70-token limit\n",
    "            words = caption.split()\n",
    "            if len(words) > 70:\n",
    "                caption = ' '.join(words[:70])\n",
    "            \n",
    "            # Ensure proper ending\n",
    "            if caption and not caption.endswith(('.', '!', '?')):\n",
    "                caption += '.'\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del image, inputs, output_ids, text_prompt, conversation\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            return caption if len(caption) > 5 else \"A person in a scene.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)[:40]}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return \"A person in a scene.\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        import gc\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Model cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3601ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:02.483321Z",
     "iopub.status.busy": "2025-12-10T09:49:02.483089Z",
     "iopub.status.idle": "2025-12-10T09:49:56.004732Z",
     "shell.execute_reply": "2025-12-10T09:49:56.003888Z"
    },
    "papermill": {
     "duration": 53.52831,
     "end_time": "2025-12-10T09:49:56.005940",
     "exception": false,
     "start_time": "2025-12-10T09:49:02.477630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Caption Generator\n",
    "caption_gen = CaptionGenerator(model_name=config.MODEL_NAME)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9204a",
   "metadata": {
    "papermill": {
     "duration": 0.006507,
     "end_time": "2025-12-10T09:49:56.019902",
     "exception": false,
     "start_time": "2025-12-10T09:49:56.013395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Generate Training Captions\n",
    "\n",
    "Download training images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acd11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:56.034316Z",
     "iopub.status.busy": "2025-12-10T09:49:56.033580Z"
    },
    "papermill": {
     "duration": 3.011359,
     "end_time": "2025-12-10T09:49:59.037863",
     "exception": false,
     "start_time": "2025-12-10T09:49:56.026504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test caption generation on 5 random images\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pick 5 random images from COCO train set\n",
    "sample_images = random.sample(train_images, 5)\n",
    "\n",
    "test_captions = {}\n",
    "img_dir_train = os.path.join(config.COCO_ROOT, 'train2017')\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(sample_images, desc=\"Test\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_train.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_train, filename)\n",
    "\n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Download error ({filename}): {e}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            test_captions[filename] = caption\n",
    "            print(f\"\\n[{filename}]\")\n",
    "            print(\"Caption:\", caption)\n",
    "            print(\"-\" * 80)\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "\n",
    "    # cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nSuccess: {success}\")\n",
    "print(f\"Failed : {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b52eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:38:40.915232Z",
     "iopub.status.busy": "2025-12-10T09:38:40.914638Z",
     "iopub.status.idle": "2025-12-10T09:38:40.993454Z",
     "shell.execute_reply": "2025-12-10T09:38:40.992551Z",
     "shell.execute_reply.started": "2025-12-10T09:38:40.915207Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "CHECKPOINT_PATH = \"train_captions_checkpoint.json\"\n",
    "SAVE_INTERVAL = 200   # save every N images\n",
    "\n",
    "# Load previous progress if exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, \"r\") as f:\n",
    "        train_captions = json.load(f)\n",
    "    print(f\"Loaded {len(train_captions)} previously saved captions.\")\n",
    "else:\n",
    "    train_captions = {}\n",
    "\n",
    "img_dir_train = os.path.join(config.COCO_ROOT, 'train2017')\n",
    "\n",
    "success = len(train_captions)\n",
    "failed = 0\n",
    "\n",
    "for idx, img_data in enumerate(tqdm(train_images, desc=\"Train\")):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_train.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "\n",
    "    # Skip if already processed\n",
    "    if filename in train_captions:\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(img_dir_train, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            train_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "\n",
    "    # Periodic cleanup + save\n",
    "    if (success + failed) % SAVE_INTERVAL == 0:\n",
    "        with open(CHECKPOINT_PATH, \"w\") as f:\n",
    "            json.dump(train_captions, f)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nSaved checkpoint at {success} captions.\\n\")\n",
    "\n",
    "# Final save\n",
    "with open(CHECKPOINT_PATH, \"w\") as f:\n",
    "    json.dump(train_captions, f)\n",
    "\n",
    "print(f\"\\nGenerated: {success}\")\n",
    "print(f\"Failed: {failed}\")\n",
    "print(f\"Final save completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6687d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Generate Validation Captions\n",
    "\n",
    "Download validation images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43706edf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Validation Captions - Ultra-optimized\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "val_captions = {}\n",
    "img_dir_val = os.path.join(config.COCO_ROOT, 'val2017')\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(val_images, desc=\"Val\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_val, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            val_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "    \n",
    "    # Periodic cleanup\n",
    "    if (success + failed) % 50 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nGenerated: {success}\")\n",
    "print(f\"Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7be8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Save Caption Files\n",
    "\n",
    "Save the generated captions to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff7fde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Dataset & Cleanup\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Cleanup model\n",
    "if 'caption_gen' in globals():\n",
    "    caption_gen.cleanup()\n",
    "\n",
    "# Save captions\n",
    "train_out = os.path.join(config.OUTPUT_DIR, 'train_captions.json')\n",
    "val_out = os.path.join(config.OUTPUT_DIR, 'val_captions.json')\n",
    "\n",
    "with open(train_out, 'w') as f:\n",
    "    json.dump(train_captions, f, indent=2)\n",
    "\n",
    "with open(val_out, 'w') as f:\n",
    "    json.dump(val_captions, f, indent=2)\n",
    "\n",
    "if train_captions:\n",
    "    avg_train = sum(len(c.split()) for c in train_captions.values()) / len(train_captions)\n",
    "    print(f\"  Avg caption (train): {avg_train:.1f} tokens\")\n",
    "if val_captions:\n",
    "    avg_val = sum(len(c.split()) for c in val_captions.values()) / len(val_captions)\n",
    "    print(f\"  Avg caption (val): {avg_val:.1f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf47dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Verify Dataset Quality\n",
    "\n",
    "Check a few samples to verify caption quality and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee1715",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify Dataset\n",
    "if train_captions:\n",
    "    for i, (fname, cap) in enumerate(list(train_captions.items())[:2]):\n",
    "        print(f\"\\nTrain[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")\n",
    "\n",
    "if val_captions:\n",
    "    for i, (fname, cap) in enumerate(list(val_captions.items())[:2]):\n",
    "        print(f\"\\nVal[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 210.335121,
   "end_time": "2025-12-10T09:49:59.046802",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T09:46:28.711681",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
