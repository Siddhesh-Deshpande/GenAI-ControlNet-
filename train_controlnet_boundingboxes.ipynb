{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet Training with COCO Bounding Boxes\n",
    "\n",
    "Train a ControlNet model using bounding boxes as spatial conditioning:\n",
    "- **Spatial Conditioning**: Bounding box masks from COCO annotations\n",
    "- **Text Conditioning**: COCO captions\n",
    "- **Base Model**: Stable Diffusion v1.5\n",
    "- **Multi-GPU Support**: Optimized for Kaggle 2√óT4 GPUs\n",
    "\n",
    "## Installation\n",
    "\n",
    "Run this first to install required packages (compatible with Kaggle):\n",
    "\n",
    "```bash\n",
    "pip install -q diffusers==0.21.4 transformers accelerate safetensors tensorboard pycocotools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:12:47.487467Z",
     "iopub.status.busy": "2025-11-29T17:12:47.487187Z",
     "iopub.status.idle": "2025-11-29T17:12:51.290210Z",
     "shell.execute_reply": "2025-11-29T17:12:51.289485Z",
     "shell.execute_reply.started": "2025-11-29T17:12:47.487443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úì Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"‚úì Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Dataset Loader with Bounding Boxes\n",
    "\n",
    "This dataset:\n",
    "- Downloads COCO 2017 annotations automatically\n",
    "- Extracts bounding boxes for all objects in each image\n",
    "- Creates binary masks from bounding boxes as spatial conditioning\n",
    "- Loads corresponding captions for text conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:12:51.292053Z",
     "iopub.status.busy": "2025-11-29T17:12:51.291637Z",
     "iopub.status.idle": "2025-11-29T17:12:57.911607Z",
     "shell.execute_reply": "2025-11-29T17:12:57.910766Z",
     "shell.execute_reply.started": "2025-11-29T17:12:51.292032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.45.0\" accelerate qwen-vl-utils pillow torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:12:57.913069Z",
     "iopub.status.busy": "2025-11-29T17:12:57.912810Z",
     "iopub.status.idle": "2025-11-29T17:16:03.905993Z",
     "shell.execute_reply": "2025-11-29T17:16:03.905215Z",
     "shell.execute_reply.started": "2025-11-29T17:12:57.913044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import zipfile\n",
    "import io\n",
    "import gc\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# --- Qwen2-VL captioning (high quality prompts) ---\n",
    "try:\n",
    "    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "    from qwen_vl_utils import process_vision_info\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    MODEL_DTYPE = torch.bfloat16 if DEVICE == 'cuda' else torch.float32\n",
    "    print(f\"Using Qwen2-VL-2B captioner. Device: {DEVICE}, Precision: {MODEL_DTYPE}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"WARNING: 'transformers' or 'qwen-vl-utils' not found. Captioning will be disabled.\")\n",
    "    Qwen2VLForConditionalGeneration = None\n",
    "    AutoProcessor = None\n",
    "    process_vision_info = None\n",
    "    MODEL_DTYPE = torch.float32\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Hugging Face Login (optional) ---\n",
    "print(\"--- Checking Hugging Face Authentication ---\")\n",
    "try:\n",
    "    login(token=os.environ.get('HF_TOKEN'), add_to_git_credential=False)\n",
    "    print(\"Logged into Hugging Face successfully (via HF_TOKEN secret).\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è WARNING: HF_TOKEN not found (optional for public models).\")\n",
    "print(\"------------------------------------------\")\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class COCOBoundingBoxDataset(Dataset):\n",
    "    def __init__(self, root_dir='./coco_data', split='train', transform=None, image_size=512, \n",
    "                 max_samples=None, download=True, enable_captioning=True):\n",
    "        \"\"\"\n",
    "        Dataset for COCO bounding boxes with optional Qwen2-VL captioning.\n",
    "        \n",
    "        Args:\n",
    "            enable_captioning: If False, skips model loading (use when captions cached)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # COCO URL and Path setup\n",
    "        self.annotation_urls = {'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip', 'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'}\n",
    "        self.ann_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.img_dir = os.path.join(root_dir, f'{split}2017')\n",
    "        split_name = 'train' if split == 'train' else 'val'\n",
    "        self.ann_file = os.path.join(self.ann_dir, f'instances_{split_name}2017.json')\n",
    "        self.caption_file = os.path.join(self.ann_dir, f'captions_{split_name}2017.json')\n",
    "        \n",
    "        os.makedirs(self.ann_dir, exist_ok=True)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        \n",
    "        if download and not os.path.exists(self.ann_file):\n",
    "            print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n",
    "            self._download_annotations()\n",
    "        \n",
    "        if not os.path.exists(self.ann_file):\n",
    "            raise FileNotFoundError(f\"Annotation file not found: {self.ann_file}\")\n",
    "            \n",
    "        print(f\"Loading COCO {split} annotations...\")\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        \n",
    "        # --- Caption disk cache ---\n",
    "        self.cache_path = os.path.join(root_dir, f'captions_cache_{split}.json')\n",
    "        if os.path.exists(self.cache_path):\n",
    "            print(f\"Loading caption cache from {self.cache_path}\")\n",
    "            with open(self.cache_path, 'r') as f:\n",
    "                self.captions_cache = json.load(f)\n",
    "            print(f\"‚úì Loaded {len(self.captions_cache)} cached captions\")\n",
    "        else:\n",
    "            self.captions_cache = {}\n",
    "        \n",
    "        # --- Initialize Qwen2-VL captioner (only if enabled) ---\n",
    "        self.captioning_model = None\n",
    "        self.captioning_processor = None\n",
    "        \n",
    "        if enable_captioning and Qwen2VLForConditionalGeneration and AutoProcessor:\n",
    "            try:\n",
    "                print(f\"\\nInitializing Qwen2-VL-2B-Instruct captioner on {DEVICE}...\")\n",
    "                model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "                \n",
    "                self.captioning_processor = AutoProcessor.from_pretrained(\n",
    "                    model_name,\n",
    "                    min_pixels=256*28*28,\n",
    "                    max_pixels=512*28*28\n",
    "                )\n",
    "                \n",
    "                self.captioning_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=MODEL_DTYPE,\n",
    "                    device_map=\"auto\" if DEVICE == 'cuda' else None\n",
    "                ).eval()\n",
    "                \n",
    "                print(\"‚úì Qwen2-VL model loaded successfully\")\n",
    "                if DEVICE == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Qwen2-VL load failed: {e}\")\n",
    "                self.captioning_model = None\n",
    "                self.captioning_processor = None\n",
    "        elif not enable_captioning:\n",
    "            print(\"Captioning disabled (using cached captions only)\")\n",
    "        \n",
    "        # Filter images with bounding box annotations\n",
    "        print(\"Filtering images with bounding box annotations...\")\n",
    "        all_img_ids = list(self.coco.imgs.keys())\n",
    "        self.img_ids = []\n",
    "        limit = max_samples if max_samples else len(all_img_ids)\n",
    "        for img_id in tqdm(all_img_ids[:limit], desc=\"Filtering images\"):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
    "            if len(ann_ids) > 0:\n",
    "                self.img_ids.append(img_id)\n",
    "        print(f\"Total images with bounding boxes: {len(self.img_ids)}\")\n",
    "\n",
    "    def _download_annotations(self):\n",
    "        \"\"\"Download COCO annotations\"\"\"\n",
    "        url = self.annotation_urls[self.split]\n",
    "        zip_path = os.path.join(self.root_dir, 'annotations.zip')\n",
    "        print(f\"Downloading from {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        print(\"Extracting annotations...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root_dir)\n",
    "        os.remove(zip_path)\n",
    "        print(\"Annotations downloaded successfully!\")\n",
    "    \n",
    "    def _download_image(self, img_info):\n",
    "        \"\"\"Download a single image from COCO\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            try:\n",
    "                response = requests.get(img_info['coco_url'])\n",
    "                with open(img_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {img_info['file_name']}: {e}\")\n",
    "                return None\n",
    "        return img_path\n",
    "\n",
    "    def save_caption_cache(self):\n",
    "        \"\"\"Persist captions to disk\"\"\"\n",
    "        with open(self.cache_path, 'w') as f:\n",
    "            json.dump(self.captions_cache, f)\n",
    "        print(f\"‚úì Saved {len(self.captions_cache)} captions to {self.cache_path}\")\n",
    "\n",
    "    def generate_caption(self, image: Image.Image, filename: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generates a detailed prompt for the given PIL image using Qwen2-VL.\n",
    "        Checks cache first; if miss, generates and caches.\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        if filename in self.captions_cache:\n",
    "            return [self.captions_cache[filename]]\n",
    "        \n",
    "        if self.captioning_model is None or self.captioning_processor is None:\n",
    "            return [\"A scene with multiple objects\"]  # Fallback\n",
    "            \n",
    "        try:\n",
    "            # Qwen2-VL message format\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Describe this image in rich detail for image generation. Include: main subjects, their positions, colors, lighting, mood, background elements, and artistic style. Be specific and vivid.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            text = self.captioning_processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            \n",
    "            inputs = self.captioning_processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = inputs.to(self.captioning_model.device)\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.captioning_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            \n",
    "            caption = self.captioning_processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0].strip()\n",
    "            \n",
    "            # Cache result\n",
    "            self.captions_cache[filename] = caption\n",
    "            return [caption]\n",
    "            \n",
    "        except Exception as e:\n",
    "            if DEVICE == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"Error during caption generation for {filename}: {e}\")\n",
    "            return [\"A scene with multiple objects\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = self._download_image(img_info)\n",
    "            if img_path is None:\n",
    "                new_idx = (idx + 1) % len(self)\n",
    "                return self.__getitem__(new_idx)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "        \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        bboxes = [ann['bbox'] for ann in anns if 'bbox' in ann]\n",
    "        \n",
    "        # Generate caption (uses cache if available)\n",
    "        captions = self.generate_caption(image, img_info['file_name'])\n",
    "        \n",
    "        bbox_mask = self.create_bbox_mask(bboxes, width, height)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        else:\n",
    "            image_transform = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            image_tensor = image_transform(image)\n",
    "        \n",
    "        bbox_mask = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.image_size, self.image_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])(bbox_mask)\n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'bbox_mask': bbox_mask,\n",
    "            'raw_bboxes': bboxes,\n",
    "            'image_id': img_id,\n",
    "            'captions': captions\n",
    "        }\n",
    "    \n",
    "    def create_bbox_mask(self, bboxes, width, height):\n",
    "        \"\"\"\n",
    "        Create a clear bounding box visualization with distinct rectangles\n",
    "        Draws thick rectangular borders for each bounding box\n",
    "        \"\"\"\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        line_thickness = max(2, min(8, int(min(width, height) / 100)))\n",
    "        for bbox in bboxes:\n",
    "            x, y, w, h = bbox\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            x1, y1 = max(0, x), max(0, y)\n",
    "            x2, y2 = min(width, x + w), min(height, y + h)\n",
    "            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, line_thickness)\n",
    "        return mask\n",
    "\n",
    "    def unload_captioner(self):\n",
    "        \"\"\"Free captioning model from memory\"\"\"\n",
    "        if self.captioning_model is not None:\n",
    "            del self.captioning_model\n",
    "            del self.captioning_processor\n",
    "            self.captioning_model = None\n",
    "            self.captioning_processor = None\n",
    "            if DEVICE == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(\"‚úì Captioning model unloaded and memory cleared\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Execution Block (Demonstration)\n",
    "# ==============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"COCO BOUNDING BOX DATASET LOADER WITH QWEN2-VL CAPTIONING & DISK CACHE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {DEVICE}, Precision: {MODEL_DTYPE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create small test datasets\n",
    "train_dataset = COCOBoundingBoxDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=512,\n",
    "    max_samples=5,\n",
    "    download=True,\n",
    "    enable_captioning=True  # Load Qwen2-VL for demo\n",
    ")\n",
    "\n",
    "val_dataset = COCOBoundingBoxDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='val',\n",
    "    image_size=512,\n",
    "    max_samples=5,\n",
    "    download=True,\n",
    "    enable_captioning=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    print(\"\\n--- Training Sample (Qwen2-VL Captioner ACTIVE) ---\")\n",
    "    sample_train = train_dataset[0]\n",
    "    print(f\"Sample data shapes:\")\n",
    "    print(f\"  - Image: {sample_train['image'].shape}\")\n",
    "    print(f\"  - Bbox mask: {sample_train['bbox_mask'].shape}\")\n",
    "    print(f\"  - **Generated Prompt:** \\\"{sample_train['captions'][0]}\\\"\")\n",
    "    \n",
    "    # Save cache after demo\n",
    "    train_dataset.save_caption_cache()\n",
    "    val_dataset.save_caption_cache()\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  RECOMMENDATION: Before training, unload captioner to free VRAM:\")\n",
    "    print(\"    train_dataset.unload_captioner()\")\n",
    "    print(\"    val_dataset.unload_captioner()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Captions (Optional but Recommended)\n",
    "\n",
    "Run this cell **before training** to generate captions for all training images and cache them to disk. This allows you to unload the BLIP model and free VRAM for ControlNet training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_captions(dataset, batch_size=16, save_every=100):\n",
    "    \"\"\"\n",
    "    Generate captions for all images in dataset and save to cache.\n",
    "    Processes in mini-batches to manage memory.\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing captions for {len(dataset)} images...\")\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Generating captions\"):\n",
    "        # Access item (triggers caption generation if not cached)\n",
    "        _ = dataset[i]\n",
    "        \n",
    "        # Periodically save cache\n",
    "        if (i + 1) % save_every == 0:\n",
    "            dataset.save_caption_cache()\n",
    "            if DEVICE == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final save\n",
    "    dataset.save_caption_cache()\n",
    "    print(f\"‚úì All captions preprocessed and cached!\")\n",
    "\n",
    "\n",
    "# Preprocess training captions BEFORE starting ControlNet training\n",
    "# This is REQUIRED to avoid kernel crashes - Qwen2-VL needs to be unloaded before loading SD models\n",
    "print(\"Creating full training dataset for caption preprocessing...\")\n",
    "train_full = COCOBoundingBoxDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=512,\n",
    "    max_samples=5000,  # Matches training config (5K samples)\n",
    "    download=True,\n",
    "    enable_captioning=True  # Load Qwen2-VL\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING CAPTIONS FOR ALL TRAINING IMAGES\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will take ~15-20 minutes but prevents kernel crashes during training.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "preprocess_all_captions(train_full)\n",
    "\n",
    "# Unload captioner to free VRAM BEFORE loading ControlNet models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNLOADING QWEN2-VL MODEL\")\n",
    "print(\"=\"*80)\n",
    "train_full.unload_captioner()\n",
    "print(\"‚úì Ready for training! Captions are cached; captioner unloaded.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:16:03.908554Z",
     "iopub.status.busy": "2025-11-29T17:16:03.907954Z",
     "iopub.status.idle": "2025-11-29T17:16:05.101254Z",
     "shell.execute_reply": "2025-11-29T17:16:05.100243Z",
     "shell.execute_reply.started": "2025-11-29T17:16:03.908529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Get one sample\n",
    "sample = train_dataset[3]\n",
    "img_id = sample['image_id']\n",
    "\n",
    "# Get image info and annotations\n",
    "img_info = train_dataset.coco.loadImgs(img_id)[0]\n",
    "ann_ids = train_dataset.coco.getAnnIds(imgIds=img_id)\n",
    "anns = train_dataset.coco.loadAnns(ann_ids)\n",
    "\n",
    "# Get captions\n",
    "captions = sample['captions']\n",
    "\n",
    "# Convert tensors back to displayable format\n",
    "image = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "image = (image * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "bbox_mask = sample['bbox_mask'].squeeze().cpu().numpy()\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Bounding box mask\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.imshow(bbox_mask, cmap='gray')\n",
    "ax2.set_title('Bounding Box Mask (Spatial Conditioning)', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "ax3.imshow(image)\n",
    "ax3.imshow(bbox_mask, cmap='Reds', alpha=0.4)\n",
    "ax3.set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display metadata\n",
    "print(\"=\" * 80)\n",
    "print(f\"Image ID: {img_id}\")\n",
    "print(f\"Filename: {img_info['file_name']}\")\n",
    "print(f\"Original size: {img_info['width']}x{img_info['height']}\")\n",
    "print(f\"Number of objects: {len(sample['raw_bboxes'])}\")\n",
    "print(f\"\\nüìù CAPTIONS:\")\n",
    "if captions:\n",
    "    for i, cap in enumerate(captions, 1):\n",
    "        print(f\"  {i}. {cap}\")\n",
    "else:\n",
    "    print(\"  No captions available\")\n",
    "print(\"\\nüì¶ BOUNDING BOXES (x, y, width, height):\")\n",
    "for i, bbox in enumerate(sample['raw_bboxes'][:5], 1):  # Show first 5\n",
    "    print(f\"  {i}. {[int(x) for x in bbox]}\")\n",
    "if len(sample['raw_bboxes']) > 5:\n",
    "    print(f\"  ... and {len(sample['raw_bboxes']) - 5} more\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:16:05.102852Z",
     "iopub.status.busy": "2025-11-29T17:16:05.102459Z",
     "iopub.status.idle": "2025-11-29T17:16:09.652273Z",
     "shell.execute_reply": "2025-11-29T17:16:09.651639Z",
     "shell.execute_reply.started": "2025-11-29T17:16:05.102814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install diffusers transformers accelerate safetensors tensorboard\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:16:09.653249Z",
     "iopub.status.busy": "2025-11-29T17:16:09.653021Z",
     "iopub.status.idle": "2025-11-29T17:16:09.660055Z",
     "shell.execute_reply": "2025-11-29T17:16:09.659210Z",
     "shell.execute_reply.started": "2025-11-29T17:16:09.653230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    # Model settings\n",
    "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "    controlnet_conditioning_channels = 1  # Grayscale bbox mask\n",
    "    \n",
    "    # Training settings\n",
    "    num_training_samples = 5000  # More data = better generalization\n",
    "    num_epochs = 3               # Fewer epochs = less overfitting\n",
    "    train_batch_size = 1  # Optimized for Kaggle\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 1e-5\n",
    "    lr_warmup_steps = 500\n",
    "    \n",
    "    # Image settings\n",
    "    resolution = 512\n",
    "    \n",
    "    # Checkpointing\n",
    "    output_dir = \"./controlnet_bbox_output\"\n",
    "    save_steps = 500\n",
    "    checkpointing_steps = 1000\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir = \"./logs_bbox\"\n",
    "    report_to = \"tensorboard\"\n",
    "    \n",
    "    # Hardware\n",
    "    mixed_precision = \"fp16\"\n",
    "    gradient_checkpointing = True\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Training {config.num_training_samples} samples for {config.num_epochs} epochs\")\n",
    "print(f\"Batch size: {config.train_batch_size}, Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:16:09.661269Z",
     "iopub.status.busy": "2025-11-29T17:16:09.660883Z",
     "iopub.status.idle": "2025-11-29T17:17:29.649253Z",
     "shell.execute_reply": "2025-11-29T17:17:29.648462Z",
     "shell.execute_reply.started": "2025-11-29T17:16:09.661246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for training (using cached captions)\n",
    "print(\"Creating training dataset with cached captions...\")\n",
    "print(\"IMPORTANT: Run the preprocessing cell above FIRST to generate and cache captions.\\n\")\n",
    "\n",
    "# Use cached captions (Qwen2-VL already unloaded)\n",
    "train_dataset_full = COCOBoundingBoxDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=config.resolution,\n",
    "    max_samples=config.num_training_samples,\n",
    "    download=True,\n",
    "    enable_captioning=False  # Use cached captions only - NO MODEL LOADING\n",
    ")\n",
    "\n",
    "# Create dataloader with collate function\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function to handle captions\"\"\"\n",
    "    images = torch.stack([example['image'] for example in examples])\n",
    "    bbox_masks = torch.stack([example['bbox_mask'] for example in examples])\n",
    "    \n",
    "    # Get first caption for each image\n",
    "    captions = []\n",
    "    for example in examples:\n",
    "        if example['captions'] and len(example['captions']) > 0:\n",
    "            captions.append(example['captions'][0])\n",
    "        else:\n",
    "            captions.append(\"objects in a scene\")  # Fallback caption\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'bbox_masks': bbox_masks,\n",
    "        'captions': captions\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_full,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training dataset ready: {len(train_dataset_full)} samples\")\n",
    "print(f\"‚úì Total batches per epoch: {len(train_dataloader)}\")\n",
    "print(\"‚úì Using cached captions - no captioning model loaded\")\n",
    "\n",
    "# Clear memory after dataset creation\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:17:29.650416Z",
     "iopub.status.busy": "2025-11-29T17:17:29.650098Z",
     "iopub.status.idle": "2025-11-29T17:17:50.761283Z",
     "shell.execute_reply": "2025-11-29T17:17:50.760474Z",
     "shell.execute_reply.started": "2025-11-29T17:17:29.650395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# Initialize ControlNet from UNet\n",
    "print(\"Initializing ControlNet...\")\n",
    "controlnet = ControlNetModel.from_unet(\n",
    "    unet,\n",
    "    conditioning_channels=config.controlnet_conditioning_channels\n",
    ")\n",
    "\n",
    "# Load noise scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# Freeze VAE and text encoder - we only train ControlNet\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"‚úì Models loaded successfully!\")\n",
    "print(f\"  - ControlNet parameters: {sum(p.numel() for p in controlnet.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  - Text encoder (frozen): {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  - UNet (frozen): {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "\n",
    "# Clear memory after model loading\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:17:50.762763Z",
     "iopub.status.busy": "2025-11-29T17:17:50.762200Z",
     "iopub.status.idle": "2025-11-29T17:17:50.769635Z",
     "shell.execute_reply": "2025-11-29T17:17:50.769083Z",
     "shell.execute_reply.started": "2025-11-29T17:17:50.762736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
    "    num_training_steps=len(train_dataloader) * config.num_epochs * config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Optimizer configured\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Warmup steps: {config.lr_warmup_steps}\")\n",
    "print(f\"  - Total training steps: {len(train_dataloader) * config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:17:50.771640Z",
     "iopub.status.busy": "2025-11-29T17:17:50.771369Z",
     "iopub.status.idle": "2025-11-29T17:17:54.724359Z",
     "shell.execute_reply": "2025-11-29T17:17:54.723536Z",
     "shell.execute_reply.started": "2025-11-29T17:17:50.771607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure Accelerate for Kaggle multi-GPU\n",
    "# This ensures both T4 GPUs are properly utilized\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"üîç Detected {num_gpus} GPU(s)\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Set environment variables for multi-GPU training on Kaggle\n",
    "    if num_gpus > 1:\n",
    "        print(f\"\\n‚úì Multi-GPU training will be enabled ({num_gpus} GPUs)\")\n",
    "        # Ensure CUDA devices are visible\n",
    "        if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in range(num_gpus))\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Only 1 GPU detected - training will use single GPU\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - training will use CPU (VERY SLOW!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:17:54.725397Z",
     "iopub.status.busy": "2025-11-29T17:17:54.725089Z",
     "iopub.status.idle": "2025-11-29T17:17:56.498637Z",
     "shell.execute_reply": "2025-11-29T17:17:56.497855Z",
     "shell.execute_reply.started": "2025-11-29T17:17:54.725348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Accelerator for multi-GPU training\n",
    "print(\"Initializing Accelerator...\")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    log_with=config.report_to,\n",
    "    project_dir=config.logging_dir,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if config.gradient_checkpointing:\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "\n",
    "# Prepare models with accelerator\n",
    "controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Move frozen models to device\n",
    "unet.to(accelerator.device)\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "# Set models to eval mode (only ControlNet is in training mode)\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Accelerator initialized successfully!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  - Device: {accelerator.device}\")\n",
    "print(f\"  - Number of processes: {accelerator.num_processes}\")\n",
    "print(f\"  - Process index: {accelerator.process_index}\")\n",
    "print(f\"  - Local process index: {accelerator.local_process_index}\")\n",
    "print(f\"  - Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"  - Gradient accumulation: {config.gradient_accumulation_steps} steps\")\n",
    "\n",
    "if accelerator.num_processes > 1:\n",
    "    print(f\"\\nüöÄ MULTI-GPU TRAINING ENABLED!\")\n",
    "    print(f\"   Training will be distributed across {accelerator.num_processes} GPUs\")\n",
    "    print(f\"   Effective speed: ~{accelerator.num_processes}x faster\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Single GPU mode\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T17:17:56.499816Z",
     "iopub.status.busy": "2025-11-29T17:17:56.499538Z",
     "iopub.status.idle": "2025-11-29T17:17:56.514547Z",
     "shell.execute_reply": "2025-11-29T17:17:56.513844Z",
     "shell.execute_reply.started": "2025-11-29T17:17:56.499797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_controlnet():\n",
    "    \"\"\"Main training loop for ControlNet\"\"\"\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    # Calculate total training steps (accounting for gradient accumulation)\n",
    "    total_steps = (len(train_dataloader) * config.num_epochs) // config.gradient_accumulation_steps\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(\n",
    "        range(total_steps),\n",
    "        desc=\"Training\",\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting ControlNet Training (Bounding Box Conditioning)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total epochs: {config.num_epochs}\")\n",
    "    print(f\"Samples per epoch: {len(train_dataset_full)}\")\n",
    "    print(f\"Batches per epoch: {len(train_dataloader)}\")\n",
    "    print(f\"Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"Total optimizer steps: {total_steps}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        controlnet.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                # Get images and conditioning\n",
    "                images = batch['images'].to(accelerator.device, dtype=torch.float32)\n",
    "                bbox_conditioning = batch['bbox_masks'].to(accelerator.device, dtype=torch.float32)\n",
    "                captions = batch['captions']\n",
    "                \n",
    "                # Encode images to latent space with VAE\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(images).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Sample noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                \n",
    "                # Sample random timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps, (bsz,),\n",
    "                    device=latents.device\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "                \n",
    "                # Add noise to latents\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Encode text prompts\n",
    "                with torch.no_grad():\n",
    "                    text_inputs = tokenizer(\n",
    "                        captions,\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "                    text_embeddings = text_encoder(text_inputs.input_ids.to(accelerator.device))[0]\n",
    "                \n",
    "                # Get ControlNet output\n",
    "                down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    controlnet_cond=bbox_conditioning,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "                \n",
    "                # Predict noise with UNet + ControlNet\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    down_block_additional_residuals=down_block_res_samples,\n",
    "                    mid_block_additional_residual=mid_block_res_sample,\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                \n",
    "                # Backpropagation\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                epoch_loss += loss.detach().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % 10 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    logs = {\n",
    "                        \"loss\": loss.detach().item(),\n",
    "                        \"avg_loss\": avg_loss,\n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch,\n",
    "                    }\n",
    "                    progress_bar.set_postfix(**logs)\n",
    "                    accelerator.log(logs, step=global_step)\n",
    "                \n",
    "                # Memory monitoring\n",
    "                if global_step % 100 == 0:\n",
    "                    print_memory_usage()\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % config.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        print(f\"\\n‚úì Checkpoint saved: {save_path}\")\n",
    "        \n",
    "        # End of epoch summary\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs} completed\")\n",
    "        print(f\"Average loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "        \n",
    "        # Clear memory after each epoch\n",
    "        if accelerator.is_main_process:\n",
    "            clear_memory()\n",
    "    \n",
    "    # Save final model\n",
    "    if accelerator.is_main_process:\n",
    "        controlnet_save_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "        unwrapped_controlnet = accelerator.unwrap_model(controlnet)\n",
    "        unwrapped_controlnet.save_pretrained(controlnet_save_path)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì Training completed!\")\n",
    "        print(f\"‚úì Final ControlNet saved to: {controlnet_save_path}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    return controlnet\n",
    "\n",
    "print(\"‚úì Training function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-29T18:29:38.199Z",
     "iopub.execute_input": "2025-11-29T17:17:56.515496Z",
     "iopub.status.busy": "2025-11-29T17:17:56.515299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "trained_controlnet = train_controlnet()\n",
    "\n",
    "print(f\"\\n‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Trained ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-29T18:29:38.200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the trained ControlNet and create pipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Clear memory before inference\n",
    "clear_memory()\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Loading trained ControlNet pipeline...\")\n",
    "\n",
    "# Load the saved ControlNet\n",
    "controlnet_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "controlnet_trained = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "# Create inference pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    controlnet=controlnet_trained,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ")\n",
    "pipe = pipe.to(accelerator.device)\n",
    "\n",
    "# Enable memory efficient attention (optional)\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"‚úì XFormers memory efficient attention enabled\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  XFormers not available: {e}\")\n",
    "    print(\"   (This is fine, just uses a bit more memory)\")\n",
    "\n",
    "print(\"‚úì Pipeline ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-29T18:29:38.200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate images using trained ControlNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a test sample from validation dataset\n",
    "test_sample = val_dataset[0]\n",
    "test_bbox_mask = test_sample['bbox_mask']\n",
    "test_caption = test_sample['captions'][0] if test_sample['captions'] else \"objects in a scene\"\n",
    "\n",
    "print(f\"Test prompt: \\\"{test_caption}\\\"\")\n",
    "print(f\"Bbox mask shape: {test_bbox_mask.shape}\")\n",
    "\n",
    "# Prepare bbox mask as tensor (keep single channel!)\n",
    "test_bbox_input = test_bbox_mask.unsqueeze(0).to(accelerator.device, dtype=torch.float16)\n",
    "print(f\"Bbox input shape: {test_bbox_input.shape}\")\n",
    "\n",
    "# Generate image\n",
    "print(\"\\nGenerating image with ControlNet...\")\n",
    "generator = torch.Generator(device=accelerator.device).manual_seed(42)\n",
    "\n",
    "output = pipe(\n",
    "    prompt=test_caption,\n",
    "    image=test_bbox_input,\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    guidance_scale=7.5,\n",
    ").images[0]\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Original image\n",
    "orig_img = test_sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "orig_img = (orig_img * 0.5 + 0.5)\n",
    "orig_img = np.clip(orig_img, 0, 1)\n",
    "axes[0].imshow(orig_img)\n",
    "axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Bounding box conditioning\n",
    "axes[1].imshow(test_bbox_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "axes[1].set_title('Bounding Box Mask', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[2].imshow(output)\n",
    "axes[2].set_title('Generated Image', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Side by side comparison\n",
    "axes[3].imshow(orig_img)\n",
    "axes[3].imshow(test_bbox_mask.squeeze().cpu().numpy(), cmap='Reds', alpha=0.3)\n",
    "axes[3].set_title('Original + Bbox Mask', fontsize=14, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{test_caption}\"', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Image generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
