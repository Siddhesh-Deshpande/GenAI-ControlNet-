{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T19:22:43.090803Z",
     "iopub.status.busy": "2025-12-14T19:22:43.090587Z",
     "iopub.status.idle": "2025-12-14T19:24:02.223518Z",
     "shell.execute_reply": "2025-12-14T19:24:02.222811Z",
     "shell.execute_reply.started": "2025-12-14T19:22:43.090784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T20:12:41.722611Z",
     "iopub.status.busy": "2025-12-14T20:12:41.721794Z",
     "iopub.status.idle": "2025-12-14T20:12:41.733073Z",
     "shell.execute_reply": "2025-12-14T20:12:41.732484Z",
     "shell.execute_reply.started": "2025-12-14T20:12:41.722572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train_bbox.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.auto import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Accelerate & Diffusers\n",
    "from accelerate import Accelerator\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    ControlNetModel, \n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class Config:\n",
    "    COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\n",
    "    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n",
    "    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n",
    "    \n",
    "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/controlnet-coco-bbox-filled\"\n",
    "    \n",
    "    RESUME_FROM_CHECKPOINT = \"latest\"\n",
    "    \n",
    "    # ORIGINAL HYPERPARAMETERS\n",
    "    RESOLUTION = 512\n",
    "    BATCH_SIZE = 8          \n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    LEARNING_RATE = 7e-5    # High LR for single controlnet\n",
    "    NUM_EPOCHS = 10         \n",
    "    \n",
    "    LOG_INTERVAL = 100       \n",
    "    LOG_BATCH_SIZE = 8\n",
    "    SAVE_INTERVAL = 1000    \n",
    "    MAX_SAMPLES = 20000 \n",
    "    \n",
    "    NUM_WORKERS = 1         \n",
    "    \n",
    "    PROMPT_DROPOUT_PROB = 0.4 \n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "class COCOBBoxFilledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        \n",
    "        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "        \n",
    "        if max_samples:\n",
    "            self.img_ids = self.img_ids[:max_samples]\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]), \n",
    "        ])\n",
    "        \n",
    "        # Nearest Neighbor for sharp edges on filled boxes\n",
    "        self.cond_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "        \n",
    "        self.color_map = self._generate_color_map()\n",
    "\n",
    "    def _generate_color_map(self):\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        palette = {}\n",
    "        for cat in cats:\n",
    "            import hashlib\n",
    "            hash_object = hashlib.md5(str(cat['id']).encode())\n",
    "            hex_hash = hash_object.hexdigest()\n",
    "            r = int(hex_hash[0:2], 16)\n",
    "            g = int(hex_hash[2:4], 16)\n",
    "            b = int(hex_hash[4:6], 16)\n",
    "            palette[cat['id']] = (r, g, b)\n",
    "        return palette\n",
    "\n",
    "    def draw_bbox_map(self, img_shape, anns):\n",
    "        # Create black canvas\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        canvas = Image.fromarray(mask)\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        \n",
    "        # IMPORTANT: Sort by area (largest to smallest)\n",
    "        # However, we want to draw Largest First? No, we want Largest at back.\n",
    "        # So we draw Largest first, then Smallest on top.\n",
    "        # If we sort by area descending (Big -> Small), Big is drawn first, Small covers it. Correct.\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']\n",
    "            cat_id = ann['category_id']\n",
    "            color = self.color_map.get(cat_id, (255, 255, 255))\n",
    "            \n",
    "            x, y, w, h = bbox\n",
    "            # FILL the rectangle\n",
    "            draw.rectangle([x, y, x+w, y+h], fill=color, outline=None)\n",
    "            \n",
    "        return canvas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        control_image = self.draw_bbox_map(image.size, anns)\n",
    "        \n",
    "        cat_ids = [ann['category_id'] for ann in anns]\n",
    "        cats = self.coco.loadCats(cat_ids)\n",
    "        cat_names = list(set([cat['name'] for cat in cats]))\n",
    "        \n",
    "        if random.random() < Config.PROMPT_DROPOUT_PROB:\n",
    "            text_prompt = \"\"\n",
    "        else:\n",
    "            text_prompt = f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": self.image_transforms(image),\n",
    "            \"conditioning_pixel_values\": self.cond_transforms(control_image),\n",
    "            \"input_ids\": self.tokenizer(\n",
    "                text_prompt, max_length=self.tokenizer.model_max_length, \n",
    "                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "            ).input_ids[0],\n",
    "            \"raw_prompt\": text_prompt\n",
    "        }\n",
    "\n",
    "# --- VALIDATION HELPER ---\n",
    "def log_validation(accelerator, controlnet, unet, vae, text_encoder, tokenizer, val_batch, step):\n",
    "    if not accelerator.is_main_process: return\n",
    "\n",
    "    try:\n",
    "        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            Config.MODEL_ID,\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "            unet=unet, controlnet=controlnet,\n",
    "            safety_checker=None, torch_dtype=torch.float16\n",
    "        ).to(accelerator.device)\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        log_images = []\n",
    "        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            prompt = val_batch[\"raw_prompt\"][i]\n",
    "            if prompt == \"\": prompt = \"A photorealistic image of the scene\"\n",
    "\n",
    "            gt_tensor = val_batch[\"pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n",
    "            gt_image = (gt_tensor / 2 + 0.5).clamp(0, 1)\n",
    "            gt_image = transforms.ToPILImage()(gt_image)\n",
    "\n",
    "            cond_tensor = val_batch[\"conditioning_pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n",
    "            cond_image = transforms.ToPILImage()(cond_tensor)\n",
    "\n",
    "            # 1. With Control\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                pred_image = pipeline(\n",
    "                    prompt, \n",
    "                    image=cond_image, \n",
    "                    num_inference_steps=20, \n",
    "                    generator=generator,\n",
    "                    controlnet_conditioning_scale=1.0 \n",
    "                ).images[0]\n",
    "            \n",
    "            # 2. No Control (Ablation)\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                base_image = pipeline(\n",
    "                    prompt, \n",
    "                    image=cond_image, \n",
    "                    num_inference_steps=20, \n",
    "                    generator=generator,\n",
    "                    controlnet_conditioning_scale=0.0 \n",
    "                ).images[0]\n",
    "            \n",
    "            log_images.append(wandb.Image(cond_image, caption=f\"#{i} Filled BBox\"))\n",
    "            log_images.append(wandb.Image(gt_image, caption=f\"#{i} Truth\"))\n",
    "            log_images.append(wandb.Image(pred_image, caption=f\"#{i} With Control\"))\n",
    "            log_images.append(wandb.Image(base_image, caption=f\"#{i} No Control\"))\n",
    "        \n",
    "        wandb.log({\"validation\": log_images})\n",
    "        del pipeline\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping validation log due to error: {e}\")\n",
    "\n",
    "# --- MAIN FUNCTION ---\n",
    "def main():\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n",
    "        mixed_precision=\"fp16\",\n",
    "        log_with=\"wandb\",\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        try:\n",
    "            user_secrets = UserSecretsClient()\n",
    "            wandb.login(key=user_secrets.get_secret(\"wandb\"))\n",
    "            accelerator.init_trackers(\"controlnet-coco-bbox\", config=Config.__dict__)\n",
    "        except Exception as e:\n",
    "            print(f\"WandB init warning: {e}\")\n",
    "\n",
    "    if accelerator.is_main_process: print(\"Loading models...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n",
    "    \n",
    "    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
    "    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "    \n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    \n",
    "    controlnet.train()\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "    if accelerator.is_main_process: print(\"Loading dataset...\")\n",
    "    # Using the FILLED dataset class\n",
    "    dataset = COCOBBoxFilledDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_batch = next(iter(train_dataloader))\n",
    "    optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    controlnet, optimizer, train_dataloader = accelerator.prepare(\n",
    "        controlnet, optimizer, train_dataloader\n",
    "    )\n",
    "    \n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    unet.to(accelerator.device)\n",
    "\n",
    "    # --- RESUME LOGIC ---\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    \n",
    "    if Config.RESUME_FROM_CHECKPOINT == \"latest\":\n",
    "        if os.path.exists(Config.OUTPUT_DIR):\n",
    "            dirs = os.listdir(Config.OUTPUT_DIR)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            if dirs:\n",
    "                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                path = dirs[-1]\n",
    "                accelerator.print(f\"Resuming from latest checkpoint: {path}\")\n",
    "                accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n",
    "                global_step = int(path.split(\"-\")[1])\n",
    "                first_epoch = global_step // len(train_dataloader)\n",
    "            else:\n",
    "                accelerator.print(\"No checkpoint found in output dir. Starting from scratch.\")\n",
    "        else:\n",
    "             accelerator.print(\"Output dir does not exist yet. Starting from scratch.\")\n",
    "\n",
    "    if accelerator.is_main_process: print(f\"Starting training from Step {global_step}, Epoch {first_epoch}...\")\n",
    "    \n",
    "    for epoch in range(first_epoch, Config.NUM_EPOCHS):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                control_cond = batch[\"conditioning_pixel_values\"].to(dtype=torch.float16)\n",
    "                \n",
    "                down_res, mid_res = controlnet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    controlnet_cond=control_cond,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "                \n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    down_block_additional_residuals=[r.to(dtype=torch.float16) for r in down_res],\n",
    "                    mid_block_additional_residual=mid_res.to(dtype=torch.float16),\n",
    "                ).sample\n",
    "                \n",
    "                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                global_step += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if accelerator.is_main_process:\n",
    "                    wandb.log({\"train_loss\": loss.item(), \"global_step\": global_step})\n",
    "                    \n",
    "                    if global_step % Config.LOG_INTERVAL == 0:\n",
    "                        log_validation(accelerator, accelerator.unwrap_model(controlnet), unet, vae, text_encoder, tokenizer, val_batch, global_step)\n",
    "                        \n",
    "                    if global_step % Config.SAVE_INTERVAL == 0:\n",
    "                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.unwrap_model(controlnet).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_controlnet_bbox_filled\"))\n",
    "        print(\"Training Finished.\")\n",
    "        accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T20:12:41.930750Z",
     "iopub.status.busy": "2025-12-14T20:12:41.930576Z",
     "iopub.status.idle": "2025-12-15T02:45:56.402360Z",
     "shell.execute_reply": "2025-12-15T02:45:56.401606Z",
     "shell.execute_reply.started": "2025-12-14T20:12:41.930736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch --multi_gpu --num_processes=2 --mixed_precision=fp16 train_bbox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:40:32.476309Z",
     "iopub.status.busy": "2025-12-16T11:40:32.475499Z",
     "iopub.status.idle": "2025-12-16T11:42:03.927266Z",
     "shell.execute_reply": "2025-12-16T11:42:03.926416Z",
     "shell.execute_reply.started": "2025-12-16T11:40:32.476263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config\n",
    "# -----------------------------\n",
    "VAL_IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "VAL_ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "OUTPUT_DIR = \"/kaggle/working/controlnet-coco-bbox-filled\"\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "NUM_SAMPLES = 16\n",
    "BATCH_SIZE = 16\n",
    "RESOLUTION = 512\n",
    "CONTROLNET_COND_SCALE = 1.0\n",
    "\n",
    "RNG = np.random.default_rng()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load latest checkpoint\n",
    "# -----------------------------\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    final_path = os.path.join(output_dir, \"final_controlnet_bbox_filled\")\n",
    "    if os.path.exists(os.path.join(final_path, \"config.json\")):\n",
    "        return final_path, \"diffusers\"\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    return latest_checkpoint, \"accelerator\"\n",
    "\n",
    "\n",
    "def load_controlnet(checkpoint_path, type_hint):\n",
    "    if type_hint == \"diffusers\":\n",
    "        print(f\"Loading Diffusers ControlNet from {checkpoint_path}\")\n",
    "        return ControlNetModel.from_pretrained(\n",
    "            checkpoint_path,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    print(f\"Loading Accelerate SafeTensors checkpoint from {checkpoint_path}\")\n",
    "\n",
    "    # Build ControlNet from base UNet\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    weight_file = os.path.join(checkpoint_path, \"model.safetensors\")\n",
    "    if not os.path.exists(weight_file):\n",
    "        raise FileNotFoundError(f\"Missing model.safetensors in {checkpoint_path}\")\n",
    "\n",
    "    # âœ… Correct SafeTensors loading\n",
    "    state_dict = load_file(weight_file, device=\"cpu\")\n",
    "\n",
    "    cleaned = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            k = k[len(\"module.\"):]\n",
    "        cleaned[k] = v\n",
    "\n",
    "    missing, unexpected = controlnet.load_state_dict(cleaned, strict=False)\n",
    "    print(f\"Loaded ControlNet | Missing: {len(missing)} | Unexpected: {len(unexpected)}\")\n",
    "\n",
    "    return controlnet.to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "ckpt_path, ckpt_type = get_latest_checkpoint(OUTPUT_DIR)\n",
    "controlnet = load_controlnet(ckpt_path, ckpt_type)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Pipeline\n",
    "# -----------------------------\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Filled BBox helpers\n",
    "# -----------------------------\n",
    "def generate_color_map(coco):\n",
    "    palette = {}\n",
    "    for cat in coco.loadCats(coco.getCatIds()):\n",
    "        h = hashlib.md5(str(cat[\"id\"]).encode()).hexdigest()\n",
    "        palette[cat[\"id\"]] = (\n",
    "            int(h[0:2], 16),\n",
    "            int(h[2:4], 16),\n",
    "            int(h[4:6], 16),\n",
    "        )\n",
    "    return palette\n",
    "\n",
    "\n",
    "def draw_bbox_map(orig_size, anns, color_map):\n",
    "    w, h = orig_size\n",
    "    canvas = Image.new(\"RGB\", (w, h), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    anns = sorted(anns, key=lambda x: x[\"area\"], reverse=True)\n",
    "\n",
    "    for ann in anns:\n",
    "        x, y, bw, bh = ann[\"bbox\"]\n",
    "        color = color_map.get(ann[\"category_id\"], (255, 255, 255))\n",
    "        draw.rectangle([x, y, x + bw, y + bh], fill=color)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def build_prompt(coco, anns):\n",
    "    cat_ids = [ann[\"category_id\"] for ann in anns]\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    names = sorted(set(cat[\"name\"] for cat in cats))\n",
    "    if not names:\n",
    "        return \"A photorealistic image\"\n",
    "    return f\"A photorealistic image containing {', '.join(names)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Batched Inference\n",
    "# -----------------------------\n",
    "print(\"Loading COCO...\")\n",
    "coco = COCO(VAL_ANN_FILE)\n",
    "color_map = generate_color_map(coco)\n",
    "\n",
    "img_ids = coco.getImgIds()\n",
    "img_ids = [i for i in img_ids if len(coco.getAnnIds(imgIds=i)) > 0]\n",
    "selected_ids = RNG.choice(img_ids, size=NUM_SAMPLES, replace=False)\n",
    "seeds = RNG.integers(low=0, high=2**31 - 1, size=NUM_SAMPLES, dtype=np.int64)\n",
    "\n",
    "results = []\n",
    "print(\"Generating in batches...\")\n",
    "\n",
    "for start in range(0, NUM_SAMPLES, BATCH_SIZE):\n",
    "    end = min(start + BATCH_SIZE, NUM_SAMPLES)\n",
    "\n",
    "    batch_prompts = []\n",
    "    batch_control_images = []\n",
    "    batch_gt_images = []\n",
    "    batch_generators = []\n",
    "\n",
    "    for j, img_id in enumerate(selected_ids[start:end]):\n",
    "        img_info = coco.loadImgs(int(img_id))[0]\n",
    "        img_path = os.path.join(VAL_IMG_DIR, img_info[\"file_name\"])\n",
    "\n",
    "        orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        gt_image = orig_image.resize((RESOLUTION, RESOLUTION))\n",
    "\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=int(img_id)))\n",
    "\n",
    "        control_image = draw_bbox_map(orig_image.size, anns, color_map)\n",
    "        control_image = control_image.resize(\n",
    "            (RESOLUTION, RESOLUTION),\n",
    "            resample=Image.NEAREST\n",
    "        )\n",
    "\n",
    "        prompt = build_prompt(coco, anns)\n",
    "\n",
    "        batch_gt_images.append(gt_image)\n",
    "        batch_control_images.append(control_image)\n",
    "        batch_prompts.append(prompt)\n",
    "\n",
    "        g = torch.Generator(device=\"cuda\").manual_seed(int(seeds[start + j]))\n",
    "        batch_generators.append(g)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = pipe(\n",
    "            prompt=batch_prompts,\n",
    "            image=batch_control_images,\n",
    "            num_inference_steps=20,\n",
    "            generator=batch_generators,\n",
    "            controlnet_conditioning_scale=CONTROLNET_COND_SCALE,\n",
    "        )\n",
    "\n",
    "    for k in range(len(out.images)):\n",
    "        results.append((\n",
    "            batch_control_images[k],\n",
    "            batch_gt_images[k],\n",
    "            out.images[k],\n",
    "            batch_prompts[k]\n",
    "        ))\n",
    "\n",
    "    print(f\"Processed {end}/{NUM_SAMPLES}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Plot\n",
    "# -----------------------------\n",
    "print(\"Plotting...\")\n",
    "fig, axes = plt.subplots(NUM_SAMPLES, 3, figsize=(15, 5 * NUM_SAMPLES))\n",
    "if NUM_SAMPLES == 1:\n",
    "    axes = np.expand_dims(axes, 0)\n",
    "\n",
    "for idx, (cond, gt, pred, prompt) in enumerate(results):\n",
    "    axes[idx, 0].imshow(cond)\n",
    "    axes[idx, 0].set_title(\"Filled BBox Map\")\n",
    "    axes[idx, 0].axis(\"off\")\n",
    "\n",
    "    axes[idx, 1].imshow(gt)\n",
    "    axes[idx, 1].set_title(\"Ground Truth\")\n",
    "    axes[idx, 1].axis(\"off\")\n",
    "\n",
    "    axes[idx, 2].imshow(pred)\n",
    "    axes[idx, 2].set_title(f\"Pred: {prompt[:60]}...\")\n",
    "    axes[idx, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:42:46.948159Z",
     "iopub.status.busy": "2025-12-16T11:42:46.947365Z",
     "iopub.status.idle": "2025-12-16T11:43:09.150941Z",
     "shell.execute_reply": "2025-12-16T11:43:09.150246Z",
     "shell.execute_reply.started": "2025-12-16T11:42:46.948132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Upload latest ControlNet to Hugging Face\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "from diffusers import ControlNetModel, UNet2DConditionModel\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"/kaggle/working/controlnet-coco-bbox-filled\"\n",
    "\n",
    "HF_REPO_ID = \"ritishshrirao/controlnet-coco-bbox-filled\"\n",
    "PRIVATE_REPO = False\n",
    "\n",
    "# -----------------------------\n",
    "# HF LOGIN (Kaggle-safe)\n",
    "# -----------------------------\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# -----------------------------\n",
    "# Find latest checkpoint\n",
    "# -----------------------------\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    final_path = os.path.join(output_dir, \"final_controlnet_bbox_filled\")\n",
    "    if os.path.exists(os.path.join(final_path, \"config.json\")):\n",
    "        return final_path, \"diffusers\"\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(\"No checkpoints found\")\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    return latest_checkpoint, \"accelerator\"\n",
    "\n",
    "\n",
    "ckpt_path, ckpt_type = get_latest_checkpoint(OUTPUT_DIR)\n",
    "print(f\"Using checkpoint: {ckpt_path} ({ckpt_type})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load ControlNet\n",
    "# -----------------------------\n",
    "if ckpt_type == \"diffusers\":\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        ckpt_path,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        subfolder=\"unet\"\n",
    "    )\n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    weights = load_file(\n",
    "        os.path.join(ckpt_path, \"model.safetensors\"),\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    cleaned = {}\n",
    "    for k, v in weights.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            k = k[len(\"module.\"):]\n",
    "        cleaned[k] = v\n",
    "\n",
    "    controlnet.load_state_dict(cleaned, strict=False)\n",
    "\n",
    "controlnet = controlnet.to(dtype=torch.float16)\n",
    "\n",
    "# -----------------------------\n",
    "# Save in Diffusers format\n",
    "# -----------------------------\n",
    "SAVE_DIR = \"/kaggle/working/hf_controlnet_export\"\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    shutil.rmtree(SAVE_DIR)\n",
    "\n",
    "controlnet.save_pretrained(\n",
    "    SAVE_DIR,\n",
    "    safe_serialization=True\n",
    ")\n",
    "\n",
    "print(\"Saved Diffusers-format ControlNet\")\n",
    "\n",
    "# -----------------------------\n",
    "# Upload to Hugging Face\n",
    "# -----------------------------\n",
    "api = HfApi()\n",
    "\n",
    "api.create_repo(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    token=HF_TOKEN,\n",
    "    private=PRIVATE_REPO,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    folder_path=SAVE_DIR,\n",
    "    path_in_repo=\"\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"Uploaded to https://huggingface.co/{HF_REPO_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
