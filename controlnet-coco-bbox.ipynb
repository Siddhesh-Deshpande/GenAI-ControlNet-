{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_bbox.py\nimport os\nimport torch\nimport numpy as np\nimport random\nimport glob\nfrom PIL import Image, ImageDraw\nfrom tqdm.auto import tqdm\nfrom pycocotools.coco import COCO\nfrom torchvision import transforms\nimport bitsandbytes as bnb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# Accelerate & Diffusers\nfrom accelerate import Accelerator\nfrom diffusers import (\n    StableDiffusionControlNetPipeline, \n    ControlNetModel, \n    DDPMScheduler,\n    AutoencoderKL,\n    UNet2DConditionModel\n)\nfrom transformers import AutoTokenizer, CLIPTextModel\n\n# --- CONFIGURATION ---\nclass Config:\n    COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\n    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n    \n    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n    OUTPUT_DIR = \"/kaggle/working/controlnet-coco-bbox\"\n    \n    # Auto-resume logic (\"latest\" or path or None)\n    RESUME_FROM_CHECKPOINT = \"latest\"\n    \n    # Hyperparameters\n    RESOLUTION = 512\n    BATCH_SIZE = 8          \n    GRAD_ACCUM_STEPS = 1\n    LEARNING_RATE = 1e-4    # High LR for ControlNet training\n    NUM_EPOCHS = 10         \n    \n    # Logging\n    LOG_INTERVAL = 100       \n    LOG_BATCH_SIZE = 8\n    SAVE_INTERVAL = 1000    \n    MAX_SAMPLES = 20000 \n    \n    # System\n    NUM_WORKERS = 1         \n    \n    # Robustness\n    PROMPT_DROPOUT_PROB = 0.4 \n\n# --- DATASET CLASS ---\nclass COCOBBoxDataset(torch.utils.data.Dataset):\n    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n        self.img_dir = img_dir\n        self.coco = COCO(ann_file)\n        self.img_ids = self.coco.getImgIds()\n        self.tokenizer = tokenizer\n        self.size = size\n        \n        # Filter images with annotations\n        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n        \n        if max_samples:\n            self.img_ids = self.img_ids[:max_samples]\n\n        self.image_transforms = transforms.Compose([\n            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]), \n        ])\n        \n        self.cond_transforms = transforms.Compose([\n            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n            transforms.CenterCrop(size),\n            transforms.ToTensor(), \n        ])\n\n    def draw_bbox_map(self, img_shape, anns):\n        canvas = Image.new(\"RGB\", img_shape, (0, 0, 0))\n        draw = ImageDraw.Draw(canvas)\n        for ann in anns:\n            bbox = ann['bbox']\n            x, y, w, h = bbox\n            # Draw white outlines on black background\n            draw.rectangle([x, y, x+w, y+h], outline=(255, 255, 255), width=2)\n        return canvas\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_info = self.coco.loadImgs(img_id)[0]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        control_image = self.draw_bbox_map(image.size, anns)\n        \n        cat_ids = [ann['category_id'] for ann in anns]\n        cats = self.coco.loadCats(cat_ids)\n        cat_names = list(set([cat['name'] for cat in cats]))\n        \n        # --- PROMPT DROPOUT LOGIC ---\n        if random.random() < Config.PROMPT_DROPOUT_PROB:\n            text_prompt = \"\"\n        else:\n            text_prompt = f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n        \n        return {\n            \"pixel_values\": self.image_transforms(image),\n            \"conditioning_pixel_values\": self.cond_transforms(control_image),\n            \"input_ids\": self.tokenizer(\n                text_prompt, max_length=self.tokenizer.model_max_length, \n                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n            ).input_ids[0],\n            \"raw_prompt\": text_prompt\n        }\n\n# --- VALIDATION HELPER ---\ndef log_validation(accelerator, controlnet, unet, vae, text_encoder, tokenizer, val_batch, step):\n    if not accelerator.is_main_process: return\n\n    try:\n        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n            Config.MODEL_ID,\n            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n            unet=unet, controlnet=controlnet,\n            safety_checker=None, torch_dtype=torch.float16\n        ).to(accelerator.device)\n        pipeline.set_progress_bar_config(disable=True)\n        \n        log_images = []\n        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n        \n        for i in range(num_samples):\n            # Ensure prompt exists for visualization\n            prompt = val_batch[\"raw_prompt\"][i]\n            if prompt == \"\": prompt = \"A photorealistic image of the scene\"\n\n            # 1. Ground Truth\n            gt_tensor = val_batch[\"pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n            gt_image = (gt_tensor / 2 + 0.5).clamp(0, 1)\n            gt_image = transforms.ToPILImage()(gt_image)\n\n            # 2. Control Image (BBox Map)\n            cond_tensor = val_batch[\"conditioning_pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n            cond_image = transforms.ToPILImage()(cond_tensor)\n\n            # 3. Generation WITH ControlNet (Scale=1.0)\n            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n            with torch.autocast(\"cuda\"):\n                pred_image = pipeline(\n                    prompt, \n                    image=cond_image, \n                    num_inference_steps=20, \n                    generator=generator,\n                    controlnet_conditioning_scale=1.0 \n                ).images[0]\n            \n            # 4. Generation WITHOUT ControlNet (Scale=0.0) - Ablation\n            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n            with torch.autocast(\"cuda\"):\n                base_image = pipeline(\n                    prompt, \n                    image=cond_image, \n                    num_inference_steps=20, \n                    generator=generator,\n                    controlnet_conditioning_scale=0.0 \n                ).images[0]\n            \n            log_images.append(wandb.Image(cond_image, caption=f\"#{i} BBox Map\"))\n            log_images.append(wandb.Image(gt_image, caption=f\"#{i} Truth\"))\n            log_images.append(wandb.Image(pred_image, caption=f\"#{i} With Control\"))\n            log_images.append(wandb.Image(base_image, caption=f\"#{i} No Control\"))\n        \n        wandb.log({\"validation\": log_images})\n        del pipeline\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Skipping validation log due to error: {e}\")\n\n# --- MAIN FUNCTION ---\ndef main():\n    accelerator = Accelerator(\n        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n        mixed_precision=\"fp16\",\n        log_with=\"wandb\",\n    )\n    \n    if accelerator.is_main_process:\n        try:\n            user_secrets = UserSecretsClient()\n            wandb.login(key=user_secrets.get_secret(\"wandb\"))\n            accelerator.init_trackers(\"controlnet-coco-bbox\", config=Config.__dict__)\n        except Exception as e:\n            print(f\"WandB init warning: {e}\")\n\n    # Load Models \n    if accelerator.is_main_process: print(\"Loading models...\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n    \n    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.float16)\n    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.float16)\n    \n    controlnet = ControlNetModel.from_unet(unet)\n    \n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    unet.requires_grad_(False)\n    \n    controlnet.train()\n    controlnet.enable_gradient_checkpointing()\n    unet.enable_gradient_checkpointing()\n\n    # Dataset\n    if accelerator.is_main_process: print(\"Loading dataset...\")\n    dataset = COCOBBoxDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=Config.NUM_WORKERS,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    val_batch = next(iter(train_dataloader))\n    optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=Config.LEARNING_RATE)\n\n    # Prepare\n    controlnet, optimizer, train_dataloader = accelerator.prepare(\n        controlnet, optimizer, train_dataloader\n    )\n    \n    vae.to(accelerator.device)\n    text_encoder.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # --- RESUME LOGIC ---\n    global_step = 0\n    first_epoch = 0\n    \n    if Config.RESUME_FROM_CHECKPOINT:\n        if Config.RESUME_FROM_CHECKPOINT == \"latest\":\n            # Check for existing checkpoints\n            if os.path.exists(Config.OUTPUT_DIR):\n                dirs = os.listdir(Config.OUTPUT_DIR)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n                path = dirs[-1] if len(dirs) > 0 else None\n            else:\n                path = None\n            \n            if path:\n                accelerator.print(f\"Resuming from latest checkpoint: {path}\")\n                accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n                global_step = int(path.split(\"-\")[1])\n                first_epoch = global_step // len(train_dataloader)\n            else:\n                accelerator.print(\"No checkpoint found. Starting from scratch.\")\n        else:\n            accelerator.print(f\"Resuming from checkpoint: {Config.RESUME_FROM_CHECKPOINT}\")\n            accelerator.load_state(Config.RESUME_FROM_CHECKPOINT)\n            global_step = int(Config.RESUME_FROM_CHECKPOINT.split(\"-\")[-1])\n            first_epoch = global_step // len(train_dataloader)\n\n    # --- TRAINING LOOP ---\n    if accelerator.is_main_process: print(f\"Starting training from Step {global_step}, Epoch {first_epoch}...\")\n    \n    for epoch in range(first_epoch, Config.NUM_EPOCHS):\n        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n        \n        for batch in train_dataloader:\n            with accelerator.accumulate(controlnet):\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n                latents = latents * vae.config.scaling_factor\n                \n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n                \n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n                control_cond = batch[\"conditioning_pixel_values\"].to(dtype=torch.float16)\n                \n                down_res, mid_res = controlnet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    controlnet_cond=control_cond,\n                    return_dict=False,\n                )\n                \n                model_pred = unet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    down_block_additional_residuals=[r.to(dtype=torch.float16) for r in down_res],\n                    mid_block_additional_residual=mid_res.to(dtype=torch.float16),\n                ).sample\n                \n                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n                \n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n\n            if accelerator.sync_gradients:\n                global_step += 1\n                progress_bar.update(1)\n                \n                if accelerator.is_main_process:\n                    wandb.log({\"train_loss\": loss.item(), \"global_step\": global_step})\n                    \n                    if global_step % Config.LOG_INTERVAL == 0:\n                        log_validation(accelerator, accelerator.unwrap_model(controlnet), unet, vae, text_encoder, tokenizer, val_batch, global_step)\n                        \n                    if global_step % Config.SAVE_INTERVAL == 0:\n                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n                        accelerator.save_state(save_path)\n    \n    if accelerator.is_main_process:\n        accelerator.unwrap_model(controlnet).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_controlnet_bbox\"))\n        print(\"Training Finished.\")\n        accelerator.end_training()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:19:55.158285Z","iopub.execute_input":"2025-12-13T14:19:55.158997Z","iopub.status.idle":"2025-12-13T14:19:55.168380Z","shell.execute_reply.started":"2025-12-13T14:19:55.158964Z","shell.execute_reply":"2025-12-13T14:19:55.167801Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_accelerate.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes=2 --mixed_precision=fp16 train_accelerate.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:19:55.513100Z","iopub.execute_input":"2025-12-13T14:19:55.513712Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\nThe following values were not passed to `accelerate launch` and had defaults used instead:\n\t`--num_machines` was set to a value of `1`\n\t`--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-13 14:20:14.834274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 14:20:14.834279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765635614.858817    1323 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765635614.858944    1324 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765635614.865910    1323 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765635614.865912    1324 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeErrorAttributeError: : 'MessageFactory' object has no attribute 'GetPrototype''MessageFactory' object has no attribute 'GetPrototype'\n\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mritishshrirao\u001b[0m (\u001b[33mritishtest1\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251213_142021-xqojkavk\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfiery-donkey-5\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ritishtest1/controlnet-coco-accelerate\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ritishtest1/controlnet-coco-accelerate/runs/xqojkavk\u001b[0m\nWandB init warning: config must be a dict or have a __dict__ attribute.\nLoading models...\nloading annotations into memory...\nLoading dataset...\nloading annotations into memory...\nDone (t=17.74s)\ncreating index...\nindex created!\nDone (t=17.68s)\ncreating index...\nindex created!\nStarting training...\nEpoch 0:   8%|‚ñà‚ñà‚ñé                          | 100/1250 [06:28<1:14:33,  3.89s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 1298.35it/s]\u001b[A\nYou have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nEpoch 0:  16%|‚ñà‚ñà‚ñà‚ñà‚ñã                        | 200/1250 [13:19<1:08:18,  3.90s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 2320.07it/s]\u001b[A\nYou have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nEpoch 0:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 300/1250 [20:14<1:01:52,  3.91s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 2177.73it/s]\u001b[A\nYou have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nEpoch 0:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 400/1250 [27:06<55:10,  3.89s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 2307.94it/s]\u001b[A\nYou have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nEpoch 0:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 500/1250 [33:58<48:28,  3.88s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 2329.09it/s]\u001b[A\nYou have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nEpoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 591/1250 [40:15<42:44,  3.89s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# EVALUATION & VISUALIZATION CELL\n# ==========================================\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nfrom pycocotools.coco import COCO\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n\n# 1. Configuration\nVAL_IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\nVAL_ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\nMODEL_PATH = \"/kaggle/working/controlnet-coco-bbox/final_controlnet_accelerate\" # Path to your saved model\nBASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\nNUM_SAMPLES = 20\n\n# 2. Helper to Draw Bounding Boxes\ndef draw_bbox_map(img_shape, anns):\n    canvas = Image.new(\"RGB\", img_shape, (0, 0, 0))\n    draw = ImageDraw.Draw(canvas)\n    for ann in anns:\n        bbox = ann['bbox'] # [x, y, w, h]\n        x, y, w, h = bbox\n        draw.rectangle([x, y, x+w, y+h], outline=(255, 255, 255), width=2)\n    return canvas\n\n# 3. Load COCO Validation Data\nprint(\"Loading COCO Annotations...\")\ncoco = COCO(VAL_ANN_FILE)\nimg_ids = coco.getImgIds()\n# Filter for images that actually have annotations\nimg_ids = [img_id for img_id in img_ids if len(coco.getAnnIds(imgIds=img_id)) > 0]\n\n# Select random samples\nnp.random.seed(42)\nselected_indices = np.random.choice(img_ids, NUM_SAMPLES, replace=False)\n\n# 4. Load Model Pipeline\nprint(f\"Loading Model from {MODEL_PATH}...\")\ncontrolnet = ControlNetModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    BASE_MODEL,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None\n).to(\"cuda\")\n\n# Use a fast scheduler for inference\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# Optimizations\npipe.enable_model_cpu_offload() \n\n# 5. Run Inference & Collect Results\nresults = []\nprint(\"Generating Images...\")\n\nfor i, img_id in enumerate(selected_indices):\n    # Load Info\n    img_info = coco.loadImgs(int(img_id))[0]\n    img_path = os.path.join(VAL_IMG_DIR, img_info['file_name'])\n    \n    # Load Original Image (Ground Truth)\n    gt_image = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n    \n    # Create Layout (Condition)\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    anns = coco.loadAnns(ann_ids)\n    control_image = draw_bbox_map((512, 512), anns)\n    \n    # Create Prompt\n    cat_ids = [ann['category_id'] for ann in anns]\n    cats = coco.loadCats(cat_ids)\n    cat_names = list(set([cat['name'] for cat in cats]))\n    prompt = f\"A photorealistic image containing {', '.join(cat_names)}\"\n    \n    # Generate\n    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n    with torch.inference_mode():\n        pred_image = pipe(\n            prompt, \n            image=control_image, \n            num_inference_steps=25, \n            generator=generator\n        ).images[0]\n    \n    results.append((control_image, gt_image, pred_image, prompt))\n    print(f\"Processed {i+1}/{NUM_SAMPLES}\")\n\n# 6. Plotting\nprint(\"Plotting results...\")\n# Create a figure with NUM_SAMPLES rows and 3 columns\nfig, axes = plt.subplots(NUM_SAMPLES, 3, figsize=(15, 5 * NUM_SAMPLES))\n\nfor idx, (cond, gt, pred, prompt) in enumerate(results):\n    # Column 1: Input Layout\n    axes[idx, 0].imshow(cond)\n    axes[idx, 0].set_title(f\"Input Layout\\n(BBox Map)\", fontsize=10)\n    axes[idx, 0].axis(\"off\")\n    \n    # Column 2: Ground Truth\n    axes[idx, 1].imshow(gt)\n    axes[idx, 1].set_title(f\"Ground Truth\", fontsize=10)\n    axes[idx, 1].axis(\"off\")\n    \n    # Column 3: Prediction\n    axes[idx, 2].imshow(pred)\n    axes[idx, 2].set_title(f\"Prediction\\nPrompt: {prompt[:50]}...\", fontsize=10)\n    axes[idx, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}