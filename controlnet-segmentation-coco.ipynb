{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T15:02:23.466070Z",
     "iopub.status.busy": "2025-12-14T15:02:23.465537Z",
     "iopub.status.idle": "2025-12-14T15:02:23.476510Z",
     "shell.execute_reply": "2025-12-14T15:02:23.475788Z",
     "shell.execute_reply.started": "2025-12-14T15:02:23.466038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train_seg.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Imports\n",
    "from accelerate import Accelerator\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\n",
    "    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n",
    "    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n",
    "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/controlnet-coco-seg\"\n",
    "    RESUME_FROM_CHECKPOINT = \"latest\"\n",
    "    RESOLUTION = 512\n",
    "    BATCH_SIZE = 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    LEARNING_RATE = 3e-5\n",
    "    NUM_EPOCHS = 10\n",
    "    LOG_INTERVAL = 100\n",
    "    LOG_BATCH_SIZE = 8\n",
    "    SAVE_INTERVAL = 1000\n",
    "    MAX_SAMPLES = 20000\n",
    "    NUM_WORKERS = 1\n",
    "    PROMPT_DROPOUT_PROB = 0.4\n",
    "\n",
    "# Dataset\n",
    "class COCOSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "        if max_samples:\n",
    "            self.img_ids = self.img_ids[:max_samples]\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        self.cond_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.color_map = self._generate_color_map()\n",
    "\n",
    "    def _generate_color_map(self):\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        palette = {}\n",
    "        for cat in cats:\n",
    "            import hashlib\n",
    "            hash_object = hashlib.md5(str(cat['id']).encode())\n",
    "            hex_hash = hash_object.hexdigest()\n",
    "            r = int(hex_hash[0:2], 16)\n",
    "            g = int(hex_hash[2:4], 16)\n",
    "            b = int(hex_hash[4:6], 16)\n",
    "            palette[cat['id']] = (r, g, b)\n",
    "        return palette\n",
    "\n",
    "    def draw_segmentation_map(self, img_shape, anns):\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        for ann in anns:\n",
    "            cat_id = ann['category_id']\n",
    "            color = self.color_map.get(cat_id, (255, 255, 255))\n",
    "            binary_mask = self.coco.annToMask(ann)\n",
    "            mask[binary_mask == 1] = color\n",
    "        return Image.fromarray(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        control_image = self.draw_segmentation_map(image.size, anns)\n",
    "        cat_ids = [ann['category_id'] for ann in anns]\n",
    "        cats = self.coco.loadCats(cat_ids)\n",
    "        cat_names = list(set([cat['name'] for cat in cats]))\n",
    "        if random.random() < Config.PROMPT_DROPOUT_PROB:\n",
    "            text_prompt = \"\"\n",
    "        else:\n",
    "            text_prompt = f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n",
    "        return {\n",
    "            \"pixel_values\": self.image_transforms(image),\n",
    "            \"conditioning_pixel_values\": self.cond_transforms(control_image),\n",
    "            \"input_ids\": self.tokenizer(\n",
    "                text_prompt, max_length=self.tokenizer.model_max_length,\n",
    "                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "            ).input_ids[0],\n",
    "            \"raw_prompt\": text_prompt\n",
    "        }\n",
    "\n",
    "# Validation helper\n",
    "def log_validation(accelerator, controlnet, unet, vae, text_encoder, tokenizer, val_batch, step):\n",
    "    if not accelerator.is_main_process: return\n",
    "    try:\n",
    "        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            Config.MODEL_ID,\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "            unet=unet, controlnet=controlnet,\n",
    "            safety_checker=None, torch_dtype=torch.float16\n",
    "        ).to(accelerator.device)\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        log_images = []\n",
    "        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n",
    "        for i in range(num_samples):\n",
    "            prompt = val_batch[\"raw_prompt\"][i]\n",
    "            if prompt == \"\": prompt = \"A photorealistic image of the scene\"\n",
    "            gt_tensor = val_batch[\"pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n",
    "            gt_image = (gt_tensor / 2 + 0.5).clamp(0, 1)\n",
    "            gt_image = transforms.ToPILImage()(gt_image)\n",
    "            cond_tensor = val_batch[\"conditioning_pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n",
    "            cond_image = transforms.ToPILImage()(cond_tensor)\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                pred_image = pipeline(\n",
    "                    prompt,\n",
    "                    image=cond_image,\n",
    "                    num_inference_steps=20,\n",
    "                    generator=generator,\n",
    "                    controlnet_conditioning_scale=1.0\n",
    "                ).images[0]\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                base_image = pipeline(\n",
    "                    prompt,\n",
    "                    image=cond_image,\n",
    "                    num_inference_steps=20,\n",
    "                    generator=generator,\n",
    "                    controlnet_conditioning_scale=0.0\n",
    "                ).images[0]\n",
    "            log_images.append(wandb.Image(cond_image, caption=f\"#{i} Seg Map\"))\n",
    "            log_images.append(wandb.Image(gt_image, caption=f\"#{i} Truth\"))\n",
    "            log_images.append(wandb.Image(pred_image, caption=f\"#{i} With Control\"))\n",
    "            log_images.append(wandb.Image(base_image, caption=f\"#{i} No Control\"))\n",
    "        wandb.log({\"validation\": log_images})\n",
    "        del pipeline\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping validation log due to error: {e}\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n",
    "        mixed_precision=\"fp16\",\n",
    "        log_with=\"wandb\",\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        try:\n",
    "            user_secrets = UserSecretsClient()\n",
    "            wandb.login(key=user_secrets.get_secret(\"wandb\"))\n",
    "            accelerator.init_trackers(\"controlnet-coco-seg\", config=Config.__dict__)\n",
    "        except Exception as e:\n",
    "            print(f\"WandB init warning: {e}\")\n",
    "    if accelerator.is_main_process: print(\"Loading models...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
    "    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    controlnet.train()\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    if accelerator.is_main_process: print(\"Loading dataset...\")\n",
    "    dataset = COCOSegmentationDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    ")\n",
    "    val_batch = next(iter(train_dataloader))\n",
    "    optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=Config.LEARNING_RATE)\n",
    "    controlnet, optimizer, train_dataloader = accelerator.prepare(\n",
    "        controlnet, optimizer, train_dataloader\n",
    ")\n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    unet.to(accelerator.device)\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    if Config.RESUME_FROM_CHECKPOINT:\n",
    "        if Config.RESUME_FROM_CHECKPOINT == \"latest\":\n",
    "            dirs = os.listdir(Config.OUTPUT_DIR)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "            if path:\n",
    "                accelerator.print(f\"Resuming from latest checkpoint: {path}\")\n",
    "                accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n",
    "                global_step = int(path.split(\"-\")[1])\n",
    "                first_epoch = global_step // len(train_dataloader)\n",
    "            else:\n",
    "                accelerator.print(\"No checkpoint found. Starting from scratch.\")\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint: {Config.RESUME_FROM_CHECKPOINT}\")\n",
    "            accelerator.load_state(Config.RESUME_FROM_CHECKPOINT)\n",
    "            global_step = int(Config.RESUME_FROM_CHECKPOINT.split(\"-\")[-1])\n",
    "            first_epoch = global_step // len(train_dataloader)\n",
    "    if accelerator.is_main_process: print(f\"Starting training from Step {global_step}, Epoch {first_epoch}...\")\n",
    "    for epoch in range(first_epoch, Config.NUM_EPOCHS):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
    "        for batch in train_dataloader:\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                control_cond = batch[\"conditioning_pixel_values\"].to(dtype=torch.float16)\n",
    "                down_res, mid_res = controlnet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    controlnet_cond=control_cond,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    down_block_additional_residuals=[r.to(dtype=torch.float16) for r in down_res],\n",
    "                    mid_block_additional_residual=mid_res.to(dtype=torch.float16),\n",
    "                ).sample\n",
    "                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            if accelerator.sync_gradients:\n",
    "                global_step += 1\n",
    "                progress_bar.update(1)\n",
    "                if accelerator.is_main_process:\n",
    "                    wandb.log({\"train_loss\": loss.item(), \"global_step\": global_step})\n",
    "                    if global_step % Config.LOG_INTERVAL == 0:\n",
    "                        log_validation(accelerator, accelerator.unwrap_model(controlnet), unet, vae, text_encoder, tokenizer, val_batch, global_step)\n",
    "                    if global_step % Config.SAVE_INTERVAL == 0:\n",
    "                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.unwrap_model(controlnet).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_controlnet_seg\"))\n",
    "        print(\"Training Finished.\")\n",
    "        accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch --multi_gpu --num_processes=2 --mixed-precision=fp16 train_seg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:15:52.270957Z",
     "iopub.status.busy": "2025-12-14T21:15:52.270599Z",
     "iopub.status.idle": "2025-12-14T21:16:15.597988Z",
     "shell.execute_reply": "2025-12-14T21:16:15.596936Z",
     "shell.execute_reply.started": "2025-12-14T21:15:52.270934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "\n",
    "api = HfApi(token=token)\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"/kaggle/working/controlnet_hf\",\n",
    "    repo_id=\"ritishshrirao/Controlnet_SD1.5_coco_segmentation\",\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload ControlNet model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "\n",
    "VAL_IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "VAL_ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "OUTPUT_DIR = \"/kaggle/working/controlnet-coco-seg\"\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "NUM_SAMPLES = 16\n",
    "BATCH_SIZE = 16\n",
    "RESOLUTION = 512\n",
    "CONTROLNET_COND_SCALE = 5.0\n",
    "\n",
    "RNG = np.random.default_rng()\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        raise FileNotFoundError(f\"Output directory {output_dir} does not exist.\")\n",
    "    final_path = os.path.join(output_dir, \"final_controlnet_seg\")\n",
    "    if os.path.exists(os.path.join(final_path, \"config.json\")):\n",
    "        return final_path, \"diffusers\"\n",
    "    checkpoints = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    return latest_checkpoint, \"accelerator\"\n",
    "\n",
    "def load_controlnet(checkpoint_path, type_hint):\n",
    "    if type_hint == \"diffusers\":\n",
    "        print(f\"Loading standard Diffusers model from {checkpoint_path}\")\n",
    "        return ControlNetModel.from_pretrained(checkpoint_path, torch_dtype=torch.float16)\n",
    "    print(f\"Loading Accelerator state from {checkpoint_path}\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL, subfolder=\"unet\")\n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "    possible_weights = os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
    "    if os.path.exists(possible_weights):\n",
    "        state_dict = torch.load(possible_weights, map_location=\"cpu\")\n",
    "    else:\n",
    "        bins = glob.glob(os.path.join(checkpoint_path, \"*.bin\"))\n",
    "        if len(bins) > 0:\n",
    "            state_dict = torch.load(bins[0], map_location=\"cpu\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find model weights in {checkpoint_path}\")\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k[7:]] = v if k.startswith(\"module.\") else v\n",
    "    missing, unexpected = controlnet.load_state_dict(new_state_dict, strict=False)\n",
    "    print(f\"Loaded weights. Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}\")\n",
    "    return controlnet.to(dtype=torch.float16)\n",
    "\n",
    "ckpt_path, ckpt_type = get_latest_checkpoint(OUTPUT_DIR)\n",
    "controlnet = load_controlnet(ckpt_path, ckpt_type)\n",
    "\n",
    "print(\"Setting up Pipeline...\")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "def generate_color_map(coco):\n",
    "    cats = coco.loadCats(coco.getCatIds())\n",
    "    palette = {}\n",
    "    import hashlib\n",
    "    for cat in cats:\n",
    "        h = hashlib.md5(str(cat[\"id\"]).encode()).hexdigest()\n",
    "        palette[cat[\"id\"]] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n",
    "    return palette\n",
    "\n",
    "def draw_segmentation_map(img_wh, anns, coco, color_map):\n",
    "    target_w, target_h = img_wh\n",
    "    mask_canvas = np.zeros((target_h, target_w, 3), dtype=np.uint8)\n",
    "    anns = sorted(anns, key=lambda x: x.get(\"area\", 0.0), reverse=True)\n",
    "    for ann in anns:\n",
    "        cat_id = ann[\"category_id\"]\n",
    "        color = color_map.get(cat_id, (255, 255, 255))\n",
    "        binary_mask_orig = coco.annToMask(ann)\n",
    "        binary_mask_resized = cv2.resize(\n",
    "            binary_mask_orig,\n",
    "            (target_w, target_h),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        mask_canvas[binary_mask_resized == 1] = color\n",
    "    return Image.fromarray(mask_canvas)\n",
    "\n",
    "def build_prompt(coco, anns):\n",
    "    cat_ids = [ann[\"category_id\"] for ann in anns]\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    cat_names = sorted(list(set([cat[\"name\"] for cat in cats])))\n",
    "    if not cat_names:\n",
    "        return \"A photorealistic image\"\n",
    "    return f\"A photorealistic image containing {', '.join(cat_names)}\"\n",
    "\n",
    "print(\"Loading COCO...\")\n",
    "coco = COCO(VAL_ANN_FILE)\n",
    "color_map = generate_color_map(coco)\n",
    "\n",
    "img_ids = coco.getImgIds()\n",
    "img_ids = [img_id for img_id in img_ids if len(coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "selected_ids = RNG.choice(img_ids, size=NUM_SAMPLES, replace=False)\n",
    "\n",
    "results = []\n",
    "print(\"Generating in batches...\")\n",
    "seeds = RNG.integers(low=0, high=2**31 - 1, size=NUM_SAMPLES, dtype=np.int64)\n",
    "\n",
    "for start in range(0, NUM_SAMPLES, BATCH_SIZE):\n",
    "    end = min(start + BATCH_SIZE, NUM_SAMPLES)\n",
    "    batch_ids = selected_ids[start:end]\n",
    "    batch_prompts = []\n",
    "    batch_control_images = []\n",
    "    batch_gt_images = []\n",
    "    batch_generators = []\n",
    "    for j, img_id in enumerate(batch_ids):\n",
    "        img_info = coco.loadImgs(int(img_id))[0]\n",
    "        img_path = os.path.join(VAL_IMG_DIR, img_info[\"file_name\"])\n",
    "        gt_image = Image.open(img_path).convert(\"RGB\").resize((RESOLUTION, RESOLUTION))\n",
    "        ann_ids = coco.getAnnIds(imgIds=int(img_id))\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        control_image = draw_segmentation_map((RESOLUTION, RESOLUTION), anns, coco, color_map)\n",
    "        prompt = build_prompt(coco, anns)\n",
    "        batch_gt_images.append(gt_image)\n",
    "        batch_control_images.append(control_image)\n",
    "        batch_prompts.append(prompt)\n",
    "        g = torch.Generator(device=\"cuda\").manual_seed(int(seeds[start + j]))\n",
    "        batch_generators.append(g)\n",
    "    with torch.inference_mode():\n",
    "        out = pipe(\n",
    "            prompt=batch_prompts,\n",
    "            image=batch_control_images,\n",
    "            num_inference_steps=20,\n",
    "            generator=batch_generators,\n",
    "            controlnet_conditioning_scale=CONTROLNET_COND_SCALE,\n",
    "        )\n",
    "    batch_preds = out.images\n",
    "    for k in range(len(batch_ids)):\n",
    "        results.append((batch_control_images[k], batch_gt_images[k], batch_preds[k], batch_prompts[k]))\n",
    "    print(f\"Processed {end}/{NUM_SAMPLES}\")\n",
    "\n",
    "print(\"Plotting...\")\n",
    "fig, axes = plt.subplots(NUM_SAMPLES, 3, figsize=(15, 5 * NUM_SAMPLES))\n",
    "if NUM_SAMPLES == 1:\n",
    "    axes = np.expand_dims(axes, 0)\n",
    "for idx, (cond, gt, pred, prompt) in enumerate(results):\n",
    "    axes[idx, 0].imshow(cond)\n",
    "    axes[idx, 0].set_title(\"Seg Mask\", fontsize=10)\n",
    "    axes[idx, 0].axis(\"off\")\n",
    "    axes[idx, 1].imshow(gt)\n",
    "    axes[idx, 1].set_title(\"Ground Truth\", fontsize=10)\n",
    "    axes[idx, 1].axis(\"off\")\n",
    "    axes[idx, 2].imshow(pred)\n",
    "    axes[idx, 2].set_title(f\"Pred: {prompt[:60]}...\", fontsize=10)\n",
    "    axes[idx, 2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
