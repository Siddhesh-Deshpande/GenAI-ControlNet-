{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:54:33.575189Z","iopub.execute_input":"2025-12-13T18:54:33.575476Z","iopub.status.idle":"2025-12-13T18:56:11.318007Z","shell.execute_reply.started":"2025-12-13T18:54:33.575449Z","shell.execute_reply":"2025-12-13T18:56:11.317243Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile train_seg.py\nimport os\nimport torch\nimport numpy as np\nimport random \nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom pycocotools.coco import COCO\nfrom torchvision import transforms\nimport bitsandbytes as bnb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# Accelerate & Diffusers\nfrom accelerate import Accelerator\nfrom diffusers import (\n    StableDiffusionControlNetPipeline, \n    ControlNetModel, \n    DDPMScheduler,\n    AutoencoderKL,\n    UNet2DConditionModel\n)\nfrom transformers import AutoTokenizer, CLIPTextModel\n\n# --- CONFIGURATION ---\nclass Config:\n    COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\n    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n    \n    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n    OUTPUT_DIR = \"/kaggle/working/controlnet-coco-seg\"\n    \n    RESUME_FROM_CHECKPOINT = \"latest\"\n    \n    # Hyperparameters\n    RESOLUTION = 512\n    BATCH_SIZE = 8          \n    GRAD_ACCUM_STEPS = 1\n    LEARNING_RATE = 1e-4    # High LR to wake up zero-convolutions\n    NUM_EPOCHS = 10  \n    \n    LOG_INTERVAL = 100       \n    LOG_BATCH_SIZE = 8\n    SAVE_INTERVAL = 1000    \n    MAX_SAMPLES = 20000 \n    \n    NUM_WORKERS = 1   \n    \n    PROMPT_DROPOUT_PROB = 0.4 \n\n# --- DATASET CLASS ---\nclass COCOSegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n        self.img_dir = img_dir\n        self.coco = COCO(ann_file)\n        self.img_ids = self.coco.getImgIds()\n        self.tokenizer = tokenizer\n        self.size = size\n        \n        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n        \n        if max_samples:\n            self.img_ids = self.img_ids[:max_samples]\n\n        self.image_transforms = transforms.Compose([\n            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]), \n        ])\n        \n        self.cond_transforms = transforms.Compose([\n            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n            transforms.CenterCrop(size),\n            transforms.ToTensor(), \n        ])\n        \n        self.color_map = self._generate_color_map()\n\n    def _generate_color_map(self):\n        cats = self.coco.loadCats(self.coco.getCatIds())\n        palette = {}\n        for cat in cats:\n            import hashlib\n            hash_object = hashlib.md5(str(cat['id']).encode())\n            hex_hash = hash_object.hexdigest()\n            r = int(hex_hash[0:2], 16)\n            g = int(hex_hash[2:4], 16)\n            b = int(hex_hash[4:6], 16)\n            palette[cat['id']] = (r, g, b)\n        return palette\n\n    def draw_segmentation_map(self, img_shape, anns):\n        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n        for ann in anns:\n            cat_id = ann['category_id']\n            color = self.color_map.get(cat_id, (255, 255, 255))\n            binary_mask = self.coco.annToMask(ann)\n            mask[binary_mask == 1] = color\n        return Image.fromarray(mask)\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_info = self.coco.loadImgs(img_id)[0]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        control_image = self.draw_segmentation_map(image.size, anns)\n        \n        cat_ids = [ann['category_id'] for ann in anns]\n        cats = self.coco.loadCats(cat_ids)\n        cat_names = list(set([cat['name'] for cat in cats]))\n        \n        # --- HIGH DROPOUT LOGIC ---\n        if random.random() < Config.PROMPT_DROPOUT_PROB:\n            text_prompt = \"\"\n        else:\n            text_prompt = f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n        \n        return {\n            \"pixel_values\": self.image_transforms(image),\n            \"conditioning_pixel_values\": self.cond_transforms(control_image),\n            \"input_ids\": self.tokenizer(\n                text_prompt, max_length=self.tokenizer.model_max_length, \n                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n            ).input_ids[0],\n            \"raw_prompt\": text_prompt \n        }\n\n# --- VALIDATION HELPER ---\ndef log_validation(accelerator, controlnet, unet, vae, text_encoder, tokenizer, val_batch, step):\n    if not accelerator.is_main_process: return\n\n    try:\n        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n            Config.MODEL_ID,\n            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n            unet=unet, controlnet=controlnet,\n            safety_checker=None, torch_dtype=torch.float16\n        ).to(accelerator.device)\n        pipeline.set_progress_bar_config(disable=True)\n        \n        log_images = []\n        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n        \n        for i in range(num_samples):\n            # Force prompt to exist for validation visualization\n            prompt = val_batch[\"raw_prompt\"][i]\n            if prompt == \"\": prompt = \"A photorealistic image of the scene\"\n\n            gt_tensor = val_batch[\"pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n            gt_image = (gt_tensor / 2 + 0.5).clamp(0, 1)\n            gt_image = transforms.ToPILImage()(gt_image)\n\n            cond_tensor = val_batch[\"conditioning_pixel_values\"][i].to(accelerator.device, dtype=torch.float16)\n            cond_image = transforms.ToPILImage()(cond_tensor)\n\n            # 1. With Control\n            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n            with torch.autocast(\"cuda\"):\n                pred_image = pipeline(\n                    prompt, \n                    image=cond_image, \n                    num_inference_steps=20, \n                    generator=generator,\n                    controlnet_conditioning_scale=1.0 \n                ).images[0]\n\n            # 2. Without Control\n            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n            with torch.autocast(\"cuda\"):\n                base_image = pipeline(\n                    prompt, \n                    image=cond_image, \n                    num_inference_steps=20, \n                    generator=generator,\n                    controlnet_conditioning_scale=0.0\n                ).images[0]\n            \n            log_images.append(wandb.Image(cond_image, caption=f\"#{i} Seg Map\"))\n            log_images.append(wandb.Image(gt_image, caption=f\"#{i} Truth\"))\n            log_images.append(wandb.Image(pred_image, caption=f\"#{i} With Control\"))\n            log_images.append(wandb.Image(base_image, caption=f\"#{i} No Control\"))\n        \n        wandb.log({\"validation\": log_images})\n        del pipeline\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Skipping validation log due to error: {e}\")\n\n# --- MAIN FUNCTION ---\ndef main():\n    accelerator = Accelerator(\n        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n        mixed_precision=\"fp16\",\n        log_with=\"wandb\",\n    )\n    \n    if accelerator.is_main_process:\n        try:\n            user_secrets = UserSecretsClient()\n            wandb.login(key=user_secrets.get_secret(\"wandb\"))\n            accelerator.init_trackers(\"controlnet-coco-seg\", config=Config.__dict__)\n        except Exception as e:\n            print(f\"WandB init warning: {e}\")\n\n    if accelerator.is_main_process: print(\"Loading models...\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n    \n    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.float16)\n    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.float16)\n    \n    controlnet = ControlNetModel.from_unet(unet)\n    \n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    unet.requires_grad_(False)\n    \n    controlnet.train()\n    controlnet.enable_gradient_checkpointing()\n    unet.enable_gradient_checkpointing()\n\n    if accelerator.is_main_process: print(\"Loading dataset...\")\n    dataset = COCOSegmentationDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=Config.NUM_WORKERS,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    val_batch = next(iter(train_dataloader))\n    optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=Config.LEARNING_RATE)\n\n    controlnet, optimizer, train_dataloader = accelerator.prepare(\n        controlnet, optimizer, train_dataloader\n    )\n    \n    vae.to(accelerator.device)\n    text_encoder.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # --- RESUME LOGIC ---\n    global_step = 0\n    first_epoch = 0\n    \n    if Config.RESUME_FROM_CHECKPOINT:\n        if Config.RESUME_FROM_CHECKPOINT == \"latest\":\n            dirs = os.listdir(Config.OUTPUT_DIR)\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n            path = dirs[-1] if len(dirs) > 0 else None\n            \n            if path:\n                accelerator.print(f\"Resuming from latest checkpoint: {path}\")\n                accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n                global_step = int(path.split(\"-\")[1])\n                first_epoch = global_step // len(train_dataloader)\n            else:\n                accelerator.print(\"No checkpoint found. Starting from scratch.\")\n        else:\n            accelerator.print(f\"Resuming from checkpoint: {Config.RESUME_FROM_CHECKPOINT}\")\n            accelerator.load_state(Config.RESUME_FROM_CHECKPOINT)\n            global_step = int(Config.RESUME_FROM_CHECKPOINT.split(\"-\")[-1])\n            first_epoch = global_step // len(train_dataloader)\n\n    if accelerator.is_main_process: print(f\"Starting training from Step {global_step}, Epoch {first_epoch}...\")\n    \n    for epoch in range(first_epoch, Config.NUM_EPOCHS):\n        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n        \n        for batch in train_dataloader:\n            with accelerator.accumulate(controlnet):\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n                latents = latents * vae.config.scaling_factor\n                \n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n                \n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n                control_cond = batch[\"conditioning_pixel_values\"].to(dtype=torch.float16)\n                \n                down_res, mid_res = controlnet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    controlnet_cond=control_cond,\n                    return_dict=False,\n                )\n                \n                model_pred = unet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    down_block_additional_residuals=[r.to(dtype=torch.float16) for r in down_res],\n                    mid_block_additional_residual=mid_res.to(dtype=torch.float16),\n                ).sample\n                \n                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n                \n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n\n            if accelerator.sync_gradients:\n                global_step += 1\n                progress_bar.update(1)\n                \n                if accelerator.is_main_process:\n                    wandb.log({\"train_loss\": loss.item(), \"global_step\": global_step})\n                    \n                    if global_step % Config.LOG_INTERVAL == 0:\n                        log_validation(accelerator, accelerator.unwrap_model(controlnet), unet, vae, text_encoder, tokenizer, val_batch, global_step)\n                        \n                    if global_step % Config.SAVE_INTERVAL == 0:\n                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n                        accelerator.save_state(save_path)\n    \n    if accelerator.is_main_process:\n        accelerator.unwrap_model(controlnet).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_controlnet_seg\"))\n        print(\"Training Finished.\")\n        accelerator.end_training()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:34:17.419973Z","iopub.execute_input":"2025-12-13T22:34:17.420255Z","iopub.status.idle":"2025-12-13T22:34:17.429766Z","shell.execute_reply.started":"2025-12-13T22:34:17.420235Z","shell.execute_reply":"2025-12-13T22:34:17.429132Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_seg.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes=2 --mixed-precision=fp16 train_seg.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom diffusers import (\n    StableDiffusionControlNetPipeline,\n    ControlNetModel,\n    UniPCMultistepScheduler,\n)\n\n# -----------------------------\n# 1) Config\n# -----------------------------\nVAL_IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\nVAL_ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\nMODEL_PATH = \"/kaggle/working/controlnet-coco-seg/final_controlnet_seg\"\nBASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n\nNUM_SAMPLES = 5\nRESOLUTION = 512\nSEED_BASE = 42\n\n# If outputs don't follow the mask, try 1.5-2.0\nCONTROLNET_COND_SCALE = 1.0\n\n# -----------------------------\n# 2) Helpers (match training)\n# -----------------------------\ndef generate_color_map(coco):\n    cats = coco.loadCats(coco.getCatIds())\n    palette = {}\n    import hashlib\n    for cat in cats:\n        h = hashlib.md5(str(cat[\"id\"]).encode()).hexdigest()\n        palette[cat[\"id\"]] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n    return palette\n\ndef draw_segmentation_map(img_wh, anns, coco, color_map):\n    # img_wh = (W, H)\n    mask = np.zeros((img_wh[1], img_wh[0], 3), dtype=np.uint8)\n    anns = sorted(anns, key=lambda x: x.get(\"area\", 0.0), reverse=True)\n    for ann in anns:\n        cat_id = ann[\"category_id\"]\n        color = color_map.get(cat_id, (255, 255, 255))\n        binary_mask = coco.annToMask(ann)\n        mask[binary_mask == 1] = color\n    return Image.fromarray(mask)\n\ndef build_prompt(coco, anns):\n    cat_ids = [ann[\"category_id\"] for ann in anns]\n    cats = coco.loadCats(cat_ids)\n    cat_names = sorted(list(set([cat[\"name\"] for cat in cats])))\n    if not cat_names:\n        return \"A photorealistic image\"\n    return f\"A photorealistic image containing {', '.join(cat_names)}\"\n\n@torch.no_grad()\ndef conditioning_effect_test(pipe, prompt, control_image, steps=10, seed=123):\n    \"\"\"\n    Quick sanity check: compare output with real control vs black control.\n    Returns MAD in pixel space (0..255).\n    \"\"\"\n    gen = torch.Generator(device=\"cuda\").manual_seed(seed)\n    real = pipe(\n        prompt,\n        image=control_image,\n        num_inference_steps=steps,\n        generator=gen,\n        controlnet_conditioning_scale=CONTROLNET_COND_SCALE,\n    ).images[0]\n\n    zero_img = Image.new(\"RGB\", control_image.size, (0, 0, 0))\n    gen = torch.Generator(device=\"cuda\").manual_seed(seed)\n    zero = pipe(\n        prompt,\n        image=zero_img,\n        num_inference_steps=steps,\n        generator=gen,\n        controlnet_conditioning_scale=CONTROLNET_COND_SCALE,\n    ).images[0]\n\n    a = np.asarray(real).astype(np.float32)\n    b = np.asarray(zero).astype(np.float32)\n    mad = float(np.mean(np.abs(a - b)))\n    return mad, real, zero\n\n# -----------------------------\n# 3) Load COCO + model\n# -----------------------------\nprint(\"Loading COCO...\")\ncoco = COCO(VAL_ANN_FILE)\ncolor_map = generate_color_map(coco)\n\nimg_ids = coco.getImgIds()\nimg_ids = [img_id for img_id in img_ids if len(coco.getAnnIds(imgIds=img_id)) > 0]\n\nnp.random.seed(42)\nselected_ids = np.random.choice(img_ids, NUM_SAMPLES, replace=False)\n\nprint(f\"Loading ControlNet from {MODEL_PATH}...\")\ncontrolnet = ControlNetModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16)\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    BASE_MODEL,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(\"cuda\")\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.set_progress_bar_config(disable=True)\n\n# NOTE: cpu offload is fine for Kaggle memory limits\npipe.enable_model_cpu_offload()\n\n# -----------------------------\n# 4) Inference\n# -----------------------------\nresults = []\nprint(\"Generating...\")\n\nfor i, img_id in enumerate(selected_ids):\n    img_info = coco.loadImgs(int(img_id))[0]\n    img_path = os.path.join(VAL_IMG_DIR, img_info[\"file_name\"])\n\n    gt_image = Image.open(img_path).convert(\"RGB\").resize((RESOLUTION, RESOLUTION))\n\n    ann_ids = coco.getAnnIds(imgIds=int(img_id))\n    anns = coco.loadAnns(ann_ids)\n\n    control_image = draw_segmentation_map((RESOLUTION, RESOLUTION), anns, coco, color_map)\n    prompt = build_prompt(coco, anns)\n\n    generator = torch.Generator(device=\"cuda\").manual_seed(SEED_BASE + i)\n    with torch.inference_mode():\n        pred_image = pipe(\n            prompt,\n            image=control_image,\n            num_inference_steps=20,\n            generator=generator,\n            controlnet_conditioning_scale=CONTROLNET_COND_SCALE,\n        ).images[0]\n\n    # Run the conditioning-vs-zero check only once (first sample) to avoid slowing everything\n    extra = None\n    if i == 0:\n        try:\n            mad, pred_real_10, pred_zero_10 = conditioning_effect_test(\n                pipe, prompt, control_image, steps=10, seed=999\n            )\n            print(f\"[cond-effect] MAD(real cond vs ZERO cond) = {mad:.2f} (0..255 scale)\")\n            extra = (pred_zero_10, mad)\n        except Exception as e:\n            print(f\"[cond-effect] skipped due to error: {e}\")\n\n    results.append((control_image, gt_image, pred_image, prompt, extra))\n    print(f\"Processed {i+1}/{NUM_SAMPLES}\")\n\n# -----------------------------\n# 5) Plot\n# -----------------------------\nprint(\"Plotting...\")\nhas_extra = results[0][4] is not None\nncols = 4 if has_extra else 3\n\nfig, axes = plt.subplots(NUM_SAMPLES, ncols, figsize=(5 * ncols, 5 * NUM_SAMPLES))\n\n# Handle NUM_SAMPLES == 1 case\nif NUM_SAMPLES == 1:\n    axes = np.expand_dims(axes, 0)\n\nfor idx, (cond, gt, pred, prompt, extra) in enumerate(results):\n    axes[idx, 0].imshow(cond)\n    axes[idx, 0].set_title(\"Seg Mask\", fontsize=10)\n    axes[idx, 0].axis(\"off\")\n\n    axes[idx, 1].imshow(gt)\n    axes[idx, 1].set_title(\"Ground Truth\", fontsize=10)\n    axes[idx, 1].axis(\"off\")\n\n    axes[idx, 2].imshow(pred)\n    axes[idx, 2].set_title(f\"Pred: {prompt[:60]}...\", fontsize=10)\n    axes[idx, 2].axis(\"off\")\n\n    if ncols == 4:\n        if extra is not None:\n            pred_zero_10, mad = extra\n            axes[idx, 3].imshow(pred_zero_10)\n            axes[idx, 3].set_title(f\"Pred (ZERO cond)\\nMAD={mad:.2f}\", fontsize=10)\n            axes[idx, 3].axis(\"off\")\n        else:\n            axes[idx, 3].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:33:25.718099Z","iopub.execute_input":"2025-12-13T22:33:25.718809Z","iopub.status.idle":"2025-12-13T22:33:41.739867Z","shell.execute_reply.started":"2025-12-13T22:33:25.718780Z","shell.execute_reply":"2025-12-13T22:33:41.738785Z"}},"outputs":[{"name":"stderr","text":"2025-12-13 22:33:31.400254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765665211.422434      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765665211.429306      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading COCO...\nloading annotations into memory...\nDone (t=0.65s)\ncreating index...\nindex created!\nLoading ControlNet from /kaggle/working/controlnet-coco-seg/final_controlnet_seg...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    391\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/controlnet-coco-seg/final_controlnet_seg'. Use `repo_type` argument if needed.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3992434625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading ControlNet from {MODEL_PATH}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mcontrolnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mControlNetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;31m# load config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         config, unused_kwargs, commit_hash = cls.load_config(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m                     \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     \u001b[0;34mf\" in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /kaggle/working/controlnet-coco-seg/final_controlnet_seg is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'."],"ename":"OSError","evalue":"We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /kaggle/working/controlnet-coco-seg/final_controlnet_seg is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}