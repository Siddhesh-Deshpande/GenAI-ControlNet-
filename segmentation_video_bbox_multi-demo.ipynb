{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f68dca8-a878-4b1f-a1ad-ce7f62cc7a7c",
    "_uuid": "17b2ad54-5218-4856-9b2d-5a69300fb07b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-16T14:34:07.094636Z",
     "iopub.status.busy": "2025-12-16T14:34:07.093869Z",
     "iopub.status.idle": "2025-12-16T14:34:37.375937Z",
     "shell.execute_reply": "2025-12-16T14:34:37.375132Z",
     "shell.execute_reply.started": "2025-12-16T14:34:07.094605Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Output dir\n",
    "OUTPUT_DIR = \"/kaggle/working/models\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    \"ritishshrirao/controlnet-coco-bbox-filled\",\n",
    "    \"ritishshrirao/controlnet-coco-multi\",\n",
    "    \"ritishshrirao/animatediff-controlnet-coco-segmentation\",\n",
    "    \"ritishshrirao/Controlnet_SD1.5_coco_segmentation\"\n",
    "]\n",
    "\n",
    "# Download\n",
    "def download_model(repo_id, output_dir):\n",
    "    print(f\"\\nDownloading repository: {repo_id}\")\n",
    "    local_dir = snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        cache_dir=output_dir,\n",
    "        force_download=True\n",
    "    )\n",
    "    print(f\"Downloaded to: {local_dir}\")\n",
    "    return local_dir\n",
    "\n",
    "downloaded_dirs = {}\n",
    "for model in models:\n",
    "    local_path = download_model(model, OUTPUT_DIR)\n",
    "    downloaded_dirs[model] = local_path\n",
    "\n",
    "print(\"\\nAll models downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module imports and common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T15:10:37.343548Z",
     "iopub.status.busy": "2025-12-16T15:10:37.343028Z",
     "iopub.status.idle": "2025-12-16T15:10:48.185875Z",
     "shell.execute_reply": "2025-12-16T15:10:48.185240Z",
     "shell.execute_reply.started": "2025-12-16T15:10:37.343524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel\n",
    ")\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "# COCO paths\n",
    "COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\n",
    "IMG_DIR = os.path.join(COCO_ROOT, \"val2017\")\n",
    "ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_val2017.json\")\n",
    "\n",
    "BASE_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Image transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cond_transform = transforms.Compose([\n",
    "    transforms.Resize(512, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation controlnet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:37:39.427602Z",
     "iopub.status.busy": "2025-12-16T14:37:39.426973Z",
     "iopub.status.idle": "2025-12-16T14:38:32.849717Z",
     "shell.execute_reply": "2025-12-16T14:38:32.848537Z",
     "shell.execute_reply.started": "2025-12-16T14:37:39.427579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UNet2DConditionModel\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Config\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "CONTROLNET_DIR = \"/kaggle/working/models/models--ritishshrirao--Controlnet_SD1.5_coco_segmentation/snapshots/ecc3bbd29715db6a3a972dbb07e2e2429d3e3a1c\"\n",
    "\n",
    "IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "\n",
    "NUM_SAMPLES = 8\n",
    "BATCH_SIZE = 8\n",
    "RESOLUTION = 512\n",
    "\n",
    "print(\"Loading base UNet...\")\n",
    "unet = UNet2DConditionModel.from_pretrained(BASE_MODEL, subfolder=\"unet\")\n",
    "\n",
    "print(\"Building ControlNet from UNet...\")\n",
    "controlnet = ControlNetModel.from_unet(unet)\n",
    "\n",
    "# Load checkpoint manually\n",
    "ckpt_path = os.path.join(CONTROLNET_DIR, \"diffusion_pytorch_model.safetensors\")\n",
    "state_dict = load_file(ckpt_path, device=\"cpu\")\n",
    "cleaned = {k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k: v for k, v in state_dict.items()}\n",
    "\n",
    "missing, unexpected = controlnet.load_state_dict(cleaned, strict=False)\n",
    "print(f\"Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "controlnet = controlnet.to(DEVICE, dtype=DTYPE)\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "def build_prompt(coco, anns):\n",
    "    names = sorted({c[\"name\"] for c in coco.loadCats([a[\"category_id\"] for a in anns])})\n",
    "    return f\"A photorealistic image containing {', '.join(names)}\" if names else \"A photorealistic image\"\n",
    "\n",
    "def build_seg_map(coco, img_info, anns):\n",
    "    mask = np.zeros((img_info[\"height\"], img_info[\"width\"], 3), dtype=np.uint8)\n",
    "    for ann in sorted(anns, key=lambda x: x[\"area\"], reverse=True):\n",
    "        cid = ann[\"category_id\"]\n",
    "        color = ((cid * 37) % 255, (cid * 17) % 255, (cid * 29) % 255)\n",
    "        mask[coco.annToMask(ann) == 1] = color\n",
    "    return Image.fromarray(mask)\n",
    "\n",
    "coco = COCO(ANN_FILE)\n",
    "img_ids = [i for i in coco.getImgIds() if len(coco.getAnnIds(imgIds=i)) > 0]\n",
    "selected_ids = np.random.choice(img_ids, NUM_SAMPLES, replace=False)\n",
    "\n",
    "results = []\n",
    "\n",
    "for start in range(0, NUM_SAMPLES, BATCH_SIZE):\n",
    "    batch_ids = selected_ids[start:start + BATCH_SIZE]\n",
    "\n",
    "    prompts, cond_imgs, gt_imgs = [], [], []\n",
    "\n",
    "    for img_id in batch_ids:\n",
    "        info = coco.loadImgs(int(img_id))[0]\n",
    "        img = Image.open(os.path.join(IMG_DIR, info[\"file_name\"])).convert(\"RGB\")\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=int(img_id)))\n",
    "\n",
    "        seg_map = build_seg_map(coco, info, anns).resize((RESOLUTION, RESOLUTION), Image.NEAREST)\n",
    "\n",
    "        prompts.append(build_prompt(coco, anns))\n",
    "        cond_imgs.append(seg_map)\n",
    "        gt_imgs.append(img.resize((RESOLUTION, RESOLUTION)))\n",
    "\n",
    "    with torch.autocast(\"cuda\", dtype=DTYPE):\n",
    "        preds = pipe(\n",
    "            prompt=prompts,\n",
    "            image=cond_imgs,\n",
    "            num_inference_steps=25,\n",
    "            controlnet_conditioning_scale=1.0\n",
    "        ).images\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        results.append((cond_imgs[i], gt_imgs[i], preds[i], prompts[i]))\n",
    "\n",
    "fig, axes = plt.subplots(len(results), 3, figsize=(15, 5 * len(results)))\n",
    "if len(results) == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, (cond, gt, pred, prompt) in enumerate(results):\n",
    "    axes[i, 0].imshow(cond)\n",
    "    axes[i, 0].set_title(\"Segmentation Map\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(gt)\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(pred)\n",
    "    axes[i, 2].set_title(prompt[:60])\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding box controlnet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:55:33.987811Z",
     "iopub.status.busy": "2025-12-16T14:55:33.987496Z",
     "iopub.status.idle": "2025-12-16T14:56:13.385896Z",
     "shell.execute_reply": "2025-12-16T14:56:13.382377Z",
     "shell.execute_reply.started": "2025-12-16T14:55:33.987770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "import hashlib\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "CONTROLNET_PATH = \"/kaggle/working/models/models--ritishshrirao--controlnet-coco-bbox-filled/snapshots/af6076a87396d1a9437d2b02cc0a6add2298920c\"\n",
    "\n",
    "IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "\n",
    "NUM_SAMPLES = 8\n",
    "BATCH_SIZE = 8\n",
    "RESOLUTION = 512\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=DTYPE).to(DEVICE)\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "def color_map(coco):\n",
    "    cmap = {}\n",
    "    for c in coco.loadCats(coco.getCatIds()):\n",
    "        h = hashlib.md5(str(c[\"id\"]).encode()).hexdigest()\n",
    "        cmap[c[\"id\"]] = tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return cmap\n",
    "\n",
    "def draw_bbox(img_size, anns, cmap):\n",
    "    w, h = img_size\n",
    "    canvas = Image.new(\"RGB\", (w, h))\n",
    "    d = ImageDraw.Draw(canvas)\n",
    "    for a in sorted(anns, key=lambda x: x[\"area\"], reverse=True):\n",
    "        x, y, bw, bh = a[\"bbox\"]\n",
    "        d.rectangle([x, y, x+bw, y+bh], fill=cmap[a[\"category_id\"]])\n",
    "    return canvas\n",
    "\n",
    "def build_prompt(coco, anns):\n",
    "    names = sorted({c[\"name\"] for c in coco.loadCats([a[\"category_id\"] for a in anns])})\n",
    "    return f\"A photorealistic image containing {', '.join(names)}\" if names else \"A photorealistic image\"\n",
    "\n",
    "coco = COCO(ANN_FILE)\n",
    "cmap = color_map(coco)\n",
    "img_ids = [i for i in coco.getImgIds() if len(coco.getAnnIds(imgIds=i)) > 0]\n",
    "selected = np.random.choice(img_ids, NUM_SAMPLES, replace=False)\n",
    "\n",
    "results = []\n",
    "\n",
    "for start in range(0, NUM_SAMPLES, BATCH_SIZE):\n",
    "    batch_ids = selected[start:start+BATCH_SIZE]\n",
    "\n",
    "    prompts, cond_imgs, gt_imgs = [], [], []\n",
    "\n",
    "    for img_id in batch_ids:\n",
    "        info = coco.loadImgs(int(img_id))[0]\n",
    "        img = Image.open(os.path.join(IMG_DIR, info[\"file_name\"])).convert(\"RGB\")\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=int(img_id)))\n",
    "\n",
    "        bbox_img = draw_bbox(img.size, anns, cmap).resize((RESOLUTION, RESOLUTION), Image.NEAREST)\n",
    "\n",
    "        prompts.append(build_prompt(coco, anns))\n",
    "        cond_imgs.append(bbox_img)\n",
    "        gt_imgs.append(img.resize((RESOLUTION, RESOLUTION)))\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        preds = pipe(\n",
    "            prompt=prompts,\n",
    "            image=cond_imgs,\n",
    "            num_inference_steps=20,\n",
    "            controlnet_conditioning_scale=1.0\n",
    "        ).images\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        results.append((cond_imgs[i], gt_imgs[i], preds[i], prompts[i]))\n",
    "\n",
    "fig, axes = plt.subplots(len(results), 3, figsize=(15, 5*len(results)))\n",
    "if len(results) == 1: axes = axes[None]\n",
    "\n",
    "for i, (cond, gt, pred, prompt) in enumerate(results):\n",
    "    axes[i, 0].imshow(cond); axes[i, 0].set_title(\"Filled BBox\"); axes[i, 0].axis(\"off\")\n",
    "    axes[i, 1].imshow(gt); axes[i, 1].set_title(\"Ground Truth\"); axes[i, 1].axis(\"off\")\n",
    "    axes[i, 2].imshow(pred); axes[i, 2].set_title(prompt[:60]); axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-controlnet demo (Segmentation + bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T15:54:40.279558Z",
     "iopub.status.busy": "2025-12-16T15:54:40.278857Z",
     "iopub.status.idle": "2025-12-16T15:55:11.553447Z",
     "shell.execute_reply": "2025-12-16T15:55:11.552519Z",
     "shell.execute_reply.started": "2025-12-16T15:54:40.279534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "import hashlib\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    MultiControlNetModel,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    PNDMScheduler\n",
    ")\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "BBOX_PATH = \"/kaggle/working/models/models--ritishshrirao--controlnet-coco-multi/snapshots/8963e8536dc8e7c2c0fe16d1eeab179149871994/bbox\"\n",
    "SEG_PATH = \"/kaggle/working/models/models--ritishshrirao--controlnet-coco-multi/snapshots/8963e8536dc8e7c2c0fe16d1eeab179149871994/segmentation\"\n",
    "\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "\n",
    "NUM_SAMPLES = 1\n",
    "BATCH_SIZE = 1\n",
    "RESOLUTION = 512\n",
    "\n",
    "def load_controlnet_robust(path):\n",
    "    if os.path.isfile(path):\n",
    "        path = os.path.dirname(path)\n",
    "    try:\n",
    "        cn = ControlNetModel.from_pretrained(path, torch_dtype=DTYPE)\n",
    "        return cn.to(DEVICE)\n",
    "    except Exception as e:\n",
    "        unet = UNet2DConditionModel.from_pretrained(BASE_MODEL, subfolder=\"unet\")\n",
    "        cn = ControlNetModel.from_unet(unet).to(DEVICE, dtype=DTYPE)\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No .safetensors file found in {path}\")\n",
    "        weight_path = os.path.join(path, files[0])\n",
    "        state_dict = load_file(weight_path)\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        m, u = cn.load_state_dict(state_dict, strict=False)\n",
    "        return cn\n",
    "\n",
    "controlnet_bbox = load_controlnet_robust(BBOX_PATH)\n",
    "controlnet_seg = load_controlnet_robust(SEG_PATH)\n",
    "\n",
    "multi_controlnet = MultiControlNetModel([controlnet_seg, controlnet_bbox]).to(DEVICE)\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    controlnet=multi_controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "def generate_color_map(coco):\n",
    "    palette = {}\n",
    "    for cat in coco.loadCats(coco.getCatIds()):\n",
    "        h = hashlib.md5(str(cat['id']).encode()).hexdigest()\n",
    "        palette[cat['id']] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n",
    "    return palette\n",
    "\n",
    "def training_transform(img, size=512, resample=Image.BILINEAR):\n",
    "    w, h = img.size\n",
    "    scale = size / min(w, h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    img = img.resize((new_w, new_h), resample=resample)\n",
    "    left = (new_w - size) // 2\n",
    "    top = (new_h - size) // 2\n",
    "    img = img.crop((left, top, left + size, top + size))\n",
    "    return img\n",
    "\n",
    "def draw_seg(coco, info, anns, cmap):\n",
    "    mask = np.zeros((info[\"height\"], info[\"width\"], 3), dtype=np.uint8)\n",
    "    for ann in sorted(anns, key=lambda x: x[\"area\"], reverse=True):\n",
    "        cid = ann[\"category_id\"]\n",
    "        color = cmap.get(cid, (255, 255, 255))\n",
    "        try: mask[coco.annToMask(ann) == 1] = color\n",
    "        except: continue\n",
    "    return Image.fromarray(mask)\n",
    "\n",
    "def draw_bbox(size, anns, cmap):\n",
    "    canvas = Image.new(\"RGB\", size)\n",
    "    d = ImageDraw.Draw(canvas)\n",
    "    for ann in sorted(anns, key=lambda x: x[\"area\"], reverse=True):\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        d.rectangle([x, y, x + w, y + h], fill=cmap[ann[\"category_id\"]])\n",
    "    return canvas\n",
    "\n",
    "def build_prompt(coco, anns):\n",
    "    names = sorted({c[\"name\"] for c in coco.loadCats([a[\"category_id\"] for a in anns])})\n",
    "    base = f\"A high-quality, photorealistic image containing {', '.join(names)}\"\n",
    "    return base if names else \"A high-quality photorealistic image\"\n",
    "\n",
    "coco = COCO(ANN_FILE)\n",
    "cmap = generate_color_map(coco)\n",
    "img_ids = [i for i in coco.getImgIds() if len(coco.getAnnIds(imgIds=i)) > 0]\n",
    "np.random.seed(99)\n",
    "selected = np.random.choice(img_ids, NUM_SAMPLES, replace=False)\n",
    "\n",
    "results = []\n",
    "\n",
    "for start in range(0, NUM_SAMPLES, BATCH_SIZE):\n",
    "    batch_ids = selected[start:start + BATCH_SIZE]\n",
    "    prompts, segs, bboxes, gts = [], [], [], []\n",
    "    for img_id in batch_ids:\n",
    "        info = coco.loadImgs(int(img_id))[0]\n",
    "        try:\n",
    "            raw_img = Image.open(os.path.join(IMG_DIR, info[\"file_name\"])).convert(\"RGB\")\n",
    "        except: continue\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=int(img_id)))\n",
    "        r_seg = draw_seg(coco, info, anns, cmap)\n",
    "        r_bbox = draw_bbox(raw_img.size, anns, cmap)\n",
    "        seg_img = training_transform(r_seg, RESOLUTION, Image.NEAREST)\n",
    "        bbox_img = training_transform(r_bbox, RESOLUTION, Image.NEAREST)\n",
    "        gt_img = training_transform(raw_img, RESOLUTION, Image.BILINEAR)\n",
    "        prompts.append(build_prompt(coco, anns))\n",
    "        segs.append(seg_img)\n",
    "        bboxes.append(bbox_img)\n",
    "        gts.append(gt_img)\n",
    "    if not prompts: continue\n",
    "    control_inputs = [[s, b] for s, b in zip(segs, bboxes)]\n",
    "    g_cuda = torch.Generator(device=DEVICE).manual_seed(42)\n",
    "    with torch.autocast(DEVICE):\n",
    "        preds_both = pipe(\n",
    "            prompt=prompts,\n",
    "            image=control_inputs,\n",
    "            num_inference_steps=30,\n",
    "            controlnet_conditioning_scale=[1.0, 1.0],\n",
    "            generator=g_cuda\n",
    "            ).images\n",
    "        preds_seg = pipe(\n",
    "            prompt=prompts,\n",
    "            image=control_inputs,\n",
    "            num_inference_steps=30,\n",
    "            controlnet_conditioning_scale=[1.0, 0.0],\n",
    "            generator=g_cuda\n",
    "            ).images\n",
    "        preds_bbox = pipe(\n",
    "            prompt=prompts,\n",
    "            image=control_inputs,\n",
    "            num_inference_steps=30,\n",
    "            controlnet_conditioning_scale=[0.0, 1.0],\n",
    "            generator=g_cuda\n",
    "            ).images\n",
    "    for i in range(len(preds_both)):\n",
    "        results.append((segs[i], bboxes[i], gts[i], preds_both[i], preds_seg[i], preds_bbox[i]))\n",
    "\n",
    "if results:\n",
    "    rows = len(results)\n",
    "    cols = 6\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(24, 4 * rows))\n",
    "    if rows == 1: axes = axes.reshape(1, -1)\n",
    "    for i, (seg, bbox, gt, both, p_seg, p_bbox) in enumerate(results):\n",
    "        axes[i, 0].imshow(seg); axes[i, 0].set_title(\"Seg Input\")\n",
    "        axes[i, 1].imshow(bbox); axes[i, 1].set_title(\"BBox Input\")\n",
    "        axes[i, 2].imshow(gt); axes[i, 2].set_title(\"Ground Truth\")\n",
    "        axes[i, 3].imshow(both); axes[i, 3].set_title(\"Result: Both\")\n",
    "        axes[i, 4].imshow(p_seg); axes[i, 4].set_title(\"Result: Seg Only\")\n",
    "        axes[i, 5].imshow(p_bbox); axes[i, 5].set_title(\"Result: BBox Only\")\n",
    "        for ax in axes[i]: ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animatediff segmentation controlnet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T15:32:03.639087Z",
     "iopub.status.busy": "2025-12-16T15:32:03.638401Z",
     "iopub.status.idle": "2025-12-16T15:34:08.279238Z",
     "shell.execute_reply": "2025-12-16T15:34:08.278546Z",
     "shell.execute_reply.started": "2025-12-16T15:32:03.639056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.functional as TF\n",
    "from diffusers import (\n",
    "    AnimateDiffControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    MotionAdapter,\n",
    "    EulerDiscreteScheduler\n",
    ")\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "CONTROLNET_PATH = (\n",
    "    \"/kaggle/working/models/\"\n",
    "    \"models--ritishshrirao--Controlnet_SD1.5_coco_segmentation/\"\n",
    "    \"snapshots/ecc3bbd29715db6a3a972dbb07e2e2429d3e3a1c\"\n",
    ")\n",
    "\n",
    "MOTION_WEIGHTS_FILE = (\n",
    "    \"/kaggle/working/models/\"\n",
    "    \"models--ritishshrirao--animatediff-controlnet-coco-segmentation/\"\n",
    "    \"snapshots/fe676e774aed9b78a9268932ab810e2745447fcf/\"\n",
    "    \"diffusion_pytorch_model.safetensors\"\n",
    ")\n",
    "MOTION_WEIGHTS_DIR = os.path.dirname(MOTION_WEIGHTS_FILE)\n",
    "\n",
    "IMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\n",
    "ANN_FILE = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\n",
    "\n",
    "RESOLUTION = 512\n",
    "NUM_FRAMES = 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "def generate_color_map(coco):\n",
    "    palette = {}\n",
    "    for cat in coco.loadCats(coco.getCatIds()):\n",
    "        cat_id = cat['id']\n",
    "        r = (cat_id * 37) % 255\n",
    "        g = (cat_id * 17 + 128) % 255\n",
    "        b = (cat_id * 123 + 55) % 255\n",
    "        palette[cat_id] = (r, g, b)\n",
    "    return palette\n",
    "\n",
    "def create_segmentation_mask(coco, img_id, img_shape, color_map):\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "    anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "    cat_names = []\n",
    "    for ann in anns:\n",
    "        cat_id = ann['category_id']\n",
    "        color = color_map.get(cat_id, (255, 255, 255))\n",
    "        binary_mask = coco.annToMask(ann)\n",
    "        mask[binary_mask == 1] = color\n",
    "        cat_info = coco.loadCats(cat_id)[0]\n",
    "        cat_names.append(cat_info['name'])\n",
    "    return Image.fromarray(mask), list(set(cat_names))\n",
    "\n",
    "def apply_synthetic_motion(pil_img, motion_type=\"zoom_in\"):\n",
    "    frames = []\n",
    "    target_zoom = 1.15 if \"zoom\" in motion_type else 1.0\n",
    "    target_pan_x = 30 if \"pan\" in motion_type else 0.0\n",
    "    target_pan_y = 0.0\n",
    "    for i in range(NUM_FRAMES):\n",
    "        progress = i / max(1, (NUM_FRAMES - 1))\n",
    "        curr_zoom = 1.0 + (target_zoom - 1.0) * progress\n",
    "        curr_tx = target_pan_x * progress\n",
    "        curr_ty = target_pan_y * progress\n",
    "        img_t = TF.affine(\n",
    "            pil_img, \n",
    "            angle=0, \n",
    "            translate=(curr_tx, curr_ty), \n",
    "            scale=curr_zoom, \n",
    "            shear=0, \n",
    "            interpolation=TF.InterpolationMode.NEAREST, \n",
    "            fill=0\n",
    "        )\n",
    "        img_t = TF.resize(img_t, (RESOLUTION, RESOLUTION), interpolation=TF.InterpolationMode.NEAREST)\n",
    "        frames.append(img_t)\n",
    "    return frames\n",
    "\n",
    "def main():\n",
    "    adapter = MotionAdapter.from_pretrained(MOTION_WEIGHTS_DIR, torch_dtype=DTYPE)\n",
    "    controlnet = ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=DTYPE)\n",
    "    pipe = AnimateDiffControlNetPipeline.from_pretrained(\n",
    "        SD_MODEL_ID,\n",
    "        motion_adapter=adapter,\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=DTYPE\n",
    "    ).to(DEVICE)\n",
    "    pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
    "        pipe.scheduler.config, \n",
    "        timestep_spacing=\"trailing\",\n",
    "        beta_schedule=\"linear\"\n",
    "    )\n",
    "    pipe.enable_vae_slicing()\n",
    "\n",
    "    coco = COCO(ANN_FILE)\n",
    "    color_map = generate_color_map(coco)\n",
    "    img_ids = coco.getImgIds()\n",
    "    valid_ids = [id for id in img_ids if len(coco.getAnnIds(imgIds=id)) > 0]\n",
    "    sample_id = random.choice(valid_ids)\n",
    "    img_info = coco.loadImgs(sample_id)[0]\n",
    "    orig_image = Image.open(os.path.join(IMG_DIR, img_info['file_name'])).convert(\"RGB\")\n",
    "    control_mask, categories = create_segmentation_mask(coco, sample_id, orig_image.size, color_map)\n",
    "    prompt = f\"A photorealistic high-quality cinematic video of {', '.join(categories)}, 4k, trending on artstation\"\n",
    "    negative_prompt = \"bad quality, distorted, low resolution, watermark, cartoon, sketch\"\n",
    "    conditioning_frames = apply_synthetic_motion(control_mask, motion_type=\"pan_right_zoom\")\n",
    "    seed = random.randint(0, 100000)\n",
    "    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_frames=NUM_FRAMES,\n",
    "        conditioning_frames=conditioning_frames,\n",
    "        controlnet_conditioning_scale=1.0,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "        width=RESOLUTION,\n",
    "        height=RESOLUTION\n",
    "    )\n",
    "    frames = output.frames[0]\n",
    "    out_filename = f\"output_{sample_id}_seed{seed}.gif\"\n",
    "    export_to_gif(frames, out_filename)\n",
    "    debug_frames = []\n",
    "    for mask, res in zip(conditioning_frames, frames):\n",
    "        mask_resized = mask.resize(res.size)\n",
    "        new_img = Image.new('RGB', (res.width * 2, res.height))\n",
    "        new_img.paste(mask_resized, (0, 0))\n",
    "        new_img.paste(res, (res.width, 0))\n",
    "        debug_frames.append(new_img)\n",
    "    export_to_gif(debug_frames, f\"debug_{sample_id}_seed{seed}.gif\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T15:31:57.641328Z",
     "iopub.status.busy": "2025-12-16T15:31:57.640600Z",
     "iopub.status.idle": "2025-12-16T15:31:58.065518Z",
     "shell.execute_reply": "2025-12-16T15:31:58.064693Z",
     "shell.execute_reply.started": "2025-12-16T15:31:57.641300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rm *.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T15:02:15.189842Z",
     "iopub.status.busy": "2025-12-16T15:02:15.189073Z",
     "iopub.status.idle": "2025-12-16T15:02:15.927923Z",
     "shell.execute_reply": "2025-12-16T15:02:15.927366Z",
     "shell.execute_reply.started": "2025-12-16T15:02:15.189815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-16T15:36:04.192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
