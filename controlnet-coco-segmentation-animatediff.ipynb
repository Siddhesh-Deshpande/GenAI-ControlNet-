{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install wandb pycocotools kagglehub huggingface_hub bitsandbytes wandb[media]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hf auth login --token $HF_TOKEN\n",
        "\n",
        "# mkdir -p /mnt/coco/data\n",
        "# mkdir -p /mnt/coco/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import inspect\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "from diffusers import (\n",
        "    ControlNetModel,\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    UniPCMultistepScheduler,\n",
        "    MotionAdapter,\n",
        ")\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "BASE_PATH = \"/mnt/coco/data\"\n",
        "ANN_FILE = os.path.join(BASE_PATH, \"annotations/instances_val2017.json\")\n",
        "IMG_DIR  = os.path.join(BASE_PATH, \"val2017\")\n",
        "if not os.path.exists(ANN_FILE):\n",
        "    print(f\"Missing val set at {ANN_FILE}; falling back to train.\")\n",
        "    ANN_FILE = os.path.join(BASE_PATH, \"annotations/instances_train2017.json\")\n",
        "    IMG_DIR  = os.path.join(BASE_PATH, \"train2017\")\n",
        "\n",
        "MODEL_ID       = \"runwayml/stable-diffusion-v1-5\"\n",
        "CONTROLNET_ID  = \"ritishshrirao/Controlnet_SD1.5_coco_segmentation\"\n",
        "MOTION_ADAPTER = \"guoyww/animatediff-motion-adapter-v1-5-2\"\n",
        "\n",
        "MOTION_MODULE_PATH = None\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "DTYPE  = torch.bfloat16\n",
        "\n",
        "NUM_FRAMES = 8\n",
        "H = W = 512\n",
        "SEED = 42\n",
        "NEG = \"cartoon, drawing, low quality, bad quality, distorted, blurry\"\n",
        "\n",
        "def get_md5_palette(coco):\n",
        "    palette = {}\n",
        "    for cat in coco.loadCats(coco.getCatIds()):\n",
        "        h = hashlib.md5(str(cat[\"id\"]).encode()).hexdigest()\n",
        "        palette[cat[\"id\"]] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n",
        "    return palette\n",
        "\n",
        "def create_control_mask(coco, img_wh, ann_ids, palette):\n",
        "    mask = np.zeros((img_wh[1], img_wh[0], 3), dtype=np.uint8)\n",
        "    anns = coco.loadAnns(ann_ids)\n",
        "    anns = sorted(anns, key=lambda x: x[\"area\"], reverse=True)\n",
        "    for ann in anns:\n",
        "        cat_id = ann[\"category_id\"]\n",
        "        color = palette.get(cat_id, (255, 255, 255))\n",
        "        binary = coco.annToMask(ann)\n",
        "        mask[binary == 1] = color\n",
        "    return Image.fromarray(mask)\n",
        "\n",
        "def build_prompt(coco, anns):\n",
        "    cat_names = list(set([coco.loadCats([a[\"category_id\"]])[0][\"name\"] for a in anns]))\n",
        "    if not cat_names:\n",
        "        return \"A photorealistic high quality image\"\n",
        "    return f\"A photorealistic high quality image of {', '.join(cat_names)}\"\n",
        "\n",
        "def pick_first_available_kwarg(pipe_call, candidates):\n",
        "    sig = inspect.signature(pipe_call)\n",
        "    params = set(sig.parameters.keys())\n",
        "    for c in candidates:\n",
        "        if c in params:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "print(f\"Loading COCO from {ANN_FILE}\")\n",
        "coco = COCO(ANN_FILE)\n",
        "palette = get_md5_palette(coco)\n",
        "\n",
        "img_ids = [i for i in coco.getImgIds() if len(coco.getAnnIds(imgIds=i)) > 0]\n",
        "img_id = random.choice(img_ids)\n",
        "\n",
        "img_info = coco.loadImgs(img_id)[0]\n",
        "ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "prompt = build_prompt(coco, anns)\n",
        "control_mask = create_control_mask(coco, (img_info[\"width\"], img_info[\"height\"]), ann_ids, palette)\n",
        "control_mask = control_mask.resize((W, H), Image.NEAREST)\n",
        "\n",
        "print(f\"img_id={img_id}\")\n",
        "print(f\"prompt={prompt}\")\n",
        "\n",
        "print(\"Loading SD ControlNet (image) pipeline...\")\n",
        "controlnet_img = ControlNetModel.from_pretrained(CONTROLNET_ID, torch_dtype=DTYPE)\n",
        "pipe_img = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    controlnet=controlnet_img,\n",
        "    torch_dtype=DTYPE,\n",
        "    safety_checker=None,\n",
        ").to(DEVICE)\n",
        "pipe_img.scheduler = UniPCMultistepScheduler.from_config(pipe_img.scheduler.config)\n",
        "pipe_img.enable_vae_slicing()\n",
        "\n",
        "gen = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
        "out_img = pipe_img(\n",
        "    prompt=prompt,\n",
        "    image=control_mask,\n",
        "    negative_prompt=NEG,\n",
        "    num_inference_steps=20,\n",
        "    generator=gen,\n",
        ").images[0]\n",
        "\n",
        "print(\"Loading AnimateDiff + ControlNet (video) pipeline...\")\n",
        "\n",
        "adapter = MotionAdapter.from_pretrained(MOTION_ADAPTER, torch_dtype=DTYPE)\n",
        "controlnet_vid = ControlNetModel.from_pretrained(CONTROLNET_ID, torch_dtype=DTYPE)\n",
        "\n",
        "pipe_vid = None\n",
        "try:\n",
        "    from diffusers import AnimateDiffControlNetPipeline\n",
        "    pipe_vid = AnimateDiffControlNetPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        motion_adapter=adapter,\n",
        "        controlnet=controlnet_vid,\n",
        "        torch_dtype=DTYPE,\n",
        "        safety_checker=None,\n",
        "    ).to(DEVICE)\n",
        "    print(\"Using AnimateDiffControlNetPipeline\")\n",
        "except Exception as e:\n",
        "    from diffusers import AnimateDiffPipeline\n",
        "    pipe_vid = AnimateDiffPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        motion_adapter=adapter,\n",
        "        torch_dtype=DTYPE,\n",
        "        safety_checker=None,\n",
        "    ).to(DEVICE)\n",
        "    pipe_vid.controlnet = controlnet_vid\n",
        "    print(\"Using AnimateDiffPipeline + pipe.controlnet attachment\")\n",
        "    print(f\"   (Reason: {type(e).__name__}: {e})\")\n",
        "\n",
        "pipe_vid.scheduler = UniPCMultistepScheduler.from_config(pipe_vid.scheduler.config)\n",
        "pipe_vid.enable_vae_slicing()\n",
        "\n",
        "if MOTION_MODULE_PATH is not None:\n",
        "    print(f\"Loading motion modules from {MOTION_MODULE_PATH}\")\n",
        "    pipe_vid.unet.load_motion_modules(MOTION_MODULE_PATH)\n",
        "\n",
        "control_list = [control_mask.copy() for _ in range(NUM_FRAMES)]\n",
        "gen2 = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
        "\n",
        "control_kw = pick_first_available_kwarg(\n",
        "    pipe_vid.__call__,\n",
        "    candidates=[\n",
        "        \"controlnet_conditioning_image\",\n",
        "        \"controlnet_conditioning_images\",\n",
        "        \"conditioning_image\",\n",
        "        \"conditioning_images\",\n",
        "        \"conditioning_frame\",\n",
        "        \"conditioning_frames\",\n",
        "        \"image\",\n",
        "        \"images\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(f\"Video pipeline control kwarg detected: {control_kw}\")\n",
        "if control_kw is None:\n",
        "    raise TypeError(\n",
        "        \"Could not find a control-image kwarg on this pipeline. \"\n",
        "        \"Print inspect.signature(pipe_vid.__call__) to see supported args.\"\n",
        "    )\n",
        "\n",
        "call_kwargs = dict(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=NEG,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    num_inference_steps=20,\n",
        "    generator=gen2,\n",
        "    controlnet_conditioning_scale=1.0,\n",
        ")\n",
        "call_kwargs[control_kw] = control_list\n",
        "\n",
        "with torch.autocast(\"cuda\", dtype=DTYPE):\n",
        "    vid = pipe_vid(**call_kwargs)\n",
        "\n",
        "video_frames = None\n",
        "if hasattr(vid, \"frames\") and vid.frames is not None:\n",
        "    video_frames = vid.frames[0]\n",
        "elif hasattr(vid, \"images\") and vid.images is not None:\n",
        "    video_frames = vid.images\n",
        "else:\n",
        "    raise AttributeError(\"Pipeline output had neither .frames nor .images.\")\n",
        "\n",
        "os.makedirs(\"./debug_out\", exist_ok=True)\n",
        "mp4_path = \"./debug_out/animatediff_controlnet_singlemask.mp4\"\n",
        "export_to_video(video_frames, mp4_path, fps=8)\n",
        "print(f\"Saved video to: {mp4_path}\")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1); plt.imshow(control_mask); plt.title(\"Control mask\"); plt.axis(\"off\")\n",
        "plt.subplot(1, 3, 2); plt.imshow(out_img); plt.title(\"SD+ControlNet image\"); plt.axis(\"off\")\n",
        "plt.subplot(1, 3, 3); plt.imshow(video_frames[0]); plt.title(\"AnimateDiff+ControlNet (frame 0)\"); plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "DEST_DIR = \"/mnt/coco/data\"\n",
        "TRAIN_DIR_TARGET = os.path.join(DEST_DIR, \"train2017\")\n",
        "ANN_DIR_TARGET = os.path.join(DEST_DIR, \"annotations\")\n",
        "\n",
        "def setup_data():\n",
        "    if os.path.exists(TRAIN_DIR_TARGET) and os.path.exists(ANN_DIR_TARGET):\n",
        "        print(f\"Data found in {DEST_DIR}. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(\"Data not found. Downloading via KaggleHub...\")\n",
        "    path = kagglehub.dataset_download(\"awsaf49/coco-2017-dataset\")\n",
        "    print(f\"Downloaded to cache: {path}\")\n",
        "\n",
        "    source_root = os.path.join(path, \"coco2017\")\n",
        "    if not os.path.exists(source_root):\n",
        "        source_root = path\n",
        "\n",
        "    print(f\"Moving files to {DEST_DIR} ...\")\n",
        "\n",
        "    src_train = os.path.join(source_root, \"train2017\")\n",
        "    if os.path.exists(src_train):\n",
        "        if os.path.exists(TRAIN_DIR_TARGET): shutil.rmtree(TRAIN_DIR_TARGET)\n",
        "        shutil.move(src_train, TRAIN_DIR_TARGET)\n",
        "        print(\"Moved train2017\")\n",
        "    else:\n",
        "        print(f\"Could not find train2017 in {source_root}\")\n",
        "\n",
        "    src_ann = os.path.join(source_root, \"annotations\")\n",
        "    if os.path.exists(src_ann):\n",
        "        if os.path.exists(ANN_DIR_TARGET): shutil.rmtree(ANN_DIR_TARGET)\n",
        "        shutil.move(src_ann, ANN_DIR_TARGET)\n",
        "        print(\"Moved annotations\")\n",
        "    else:\n",
        "        print(f\"Could not find annotations in {source_root}\")\n",
        "\n",
        "    print(\"Data setup complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%writefile train_video_seg.py\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import transforms\n",
        "import wandb\n",
        "import gc\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    UNet2DConditionModel,\n",
        "    ControlNetModel,\n",
        "    MotionAdapter,\n",
        "    DDPMScheduler,\n",
        "    AnimateDiffPipeline,\n",
        "    AnimateDiffControlNetPipeline,\n",
        "    )\n",
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "class Config:\n",
        "    COCO_ROOT = \"/mnt/coco/data\"\n",
        "    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n",
        "    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n",
        "    OUTPUT_DIR = \"/mnt/coco/models/animatediff_coco_finetune\"\n",
        "    SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
        "    CONTROLNET_ID = \"ritishshrirao/Controlnet_SD1.5_coco_segmentation\"\n",
        "    MOTION_ADAPTER_ID = \"guoyww/animatediff-motion-adapter-v1-5-2\"\n",
        "    RESOLUTION = 512\n",
        "    NUM_FRAMES = 16\n",
        "    BATCH_SIZE = 4\n",
        "    GRAD_ACCUM_STEPS = 1\n",
        "    LEARNING_RATE = 2e-5\n",
        "    NUM_EPOCHS = 10\n",
        "    SAVE_INTERVAL = 500\n",
        "    LOG_VIDEO_INTERVAL = 100\n",
        "    MAX_SAMPLES = 50000\n",
        "    NUM_WORKERS = 4\n",
        "    ZOOM_RANGE = (1.0, 1.15)\n",
        "    PAN_RANGE = 30\n",
        "\n",
        "class COCOSyntheticVideoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, ann_file, tokenizer, size=512, frames=16, max_samples=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.img_ids = self.coco.getImgIds()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "        self.frames = frames\n",
        "        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
        "        if max_samples: self.img_ids = self.img_ids[:max_samples]\n",
        "        self.color_map = self._generate_color_map()\n",
        "        self.norm = transforms.Normalize([0.5], [0.5])\n",
        "\n",
        "    def _generate_color_map(self):\n",
        "        palette = {}\n",
        "        for cat in self.coco.loadCats(self.coco.getCatIds()):\n",
        "            cat_id = cat['id']\n",
        "            r = (cat_id * 37) % 255\n",
        "            g = (cat_id * 17 + 128) % 255\n",
        "            b = (cat_id * 123 + 55) % 255\n",
        "            palette[cat_id] = (r, g, b)\n",
        "        return palette\n",
        "\n",
        "    def draw_segmentation_map(self, img_shape, anns):\n",
        "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
        "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
        "        for ann in anns:\n",
        "            cat_id = ann['category_id']\n",
        "            color = self.color_map.get(cat_id, (255, 255, 255))\n",
        "            binary_mask = self.coco.annToMask(ann)\n",
        "            mask[binary_mask == 1] = color\n",
        "        return Image.fromarray(mask)\n",
        "\n",
        "    def apply_synthetic_motion(self, img, seg_map):\n",
        "        frames_img = []\n",
        "        frames_seg = []\n",
        "        mode = random.randint(0, 3)\n",
        "        target_zoom = random.uniform(*Config.ZOOM_RANGE) if mode in [0, 3] else 1.0\n",
        "        target_pan_x = random.uniform(0, Config.PAN_RANGE) if mode in [1, 3] else 0.0\n",
        "        target_pan_y = random.uniform(0, Config.PAN_RANGE) if mode in [2, 3] else 0.0\n",
        "        for i in range(self.frames):\n",
        "            progress = i / max(1, (self.frames - 1))\n",
        "            curr_zoom = 1.0 + (target_zoom - 1.0) * progress\n",
        "            curr_tx, curr_ty = target_pan_x * progress, target_pan_y * progress\n",
        "            def transform_frame(pil_img, is_mask):\n",
        "                interp = TF.InterpolationMode.NEAREST if is_mask else TF.InterpolationMode.BILINEAR\n",
        "                img_t = TF.affine(pil_img, angle=0, translate=(curr_tx, curr_ty), scale=curr_zoom, shear=0, interpolation=interp, fill=0)\n",
        "                img_t = TF.resize(img_t, (self.size, self.size), interpolation=interp)\n",
        "                tensor = TF.to_tensor(img_t)\n",
        "                if not is_mask: tensor = self.norm(tensor)\n",
        "                return tensor\n",
        "            frames_img.append(transform_frame(img, False))\n",
        "            frames_seg.append(transform_frame(seg_map, True))\n",
        "        return torch.stack(frames_img), torch.stack(frames_seg)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_id = self.img_ids[idx]\n",
        "            img_info = self.coco.loadImgs(img_id)[0]\n",
        "            img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "            control_image = self.draw_segmentation_map(image.size, anns)\n",
        "            pixel_values, condition_values = self.apply_synthetic_motion(image, control_image)\n",
        "            cat_names = list(set([cat['name'] for cat in self.coco.loadCats([ann['category_id'] for ann in anns])]))\n",
        "            text_prompt = f\"A photorealistic high-quality image of {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n",
        "            return {\n",
        "                \"pixel_values\": pixel_values,\n",
        "                \"condition_values\": condition_values,\n",
        "                \"input_ids\": self.tokenizer(text_prompt, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0],\n",
        "                \"prompt_text\": text_prompt\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return self.__getitem__(random.randint(0, len(self)-1))\n",
        "\n",
        "def log_validation(accelerator, unet, controlnet, adapter, vae, text_encoder, tokenizer, val_batch):\n",
        "    if not accelerator.is_main_process: return\n",
        "    print(\"Running Validation...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    unet_unwrapped = accelerator.unwrap_model(unet)\n",
        "    controlnet_unwrapped = accelerator.unwrap_model(controlnet)\n",
        "    pipeline = AnimateDiffControlNetPipeline.from_pretrained(\n",
        "        Config.SD_MODEL_ID,\n",
        "        unet=unet_unwrapped,\n",
        "        controlnet=controlnet_unwrapped,\n",
        "        motion_adapter=adapter,\n",
        "        vae=vae,\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        image_encoder=None,\n",
        "        safety_checker=None,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    ).to(accelerator.device)\n",
        "    pipeline.enable_vae_slicing()\n",
        "    wandb_videos = []\n",
        "    batch_size = val_batch[\"pixel_values\"].shape[0]\n",
        "    for idx in range(batch_size):\n",
        "        try:\n",
        "            sample_cond = val_batch[\"condition_values\"][idx]\n",
        "            prompt = val_batch[\"prompt_text\"][idx]\n",
        "            pil_controls = []\n",
        "            for i in range(sample_cond.shape[0]):\n",
        "                frame = sample_cond[i].float()\n",
        "                pil_controls.append(transforms.ToPILImage()(frame))\n",
        "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + idx)\n",
        "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "                output = pipeline(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=\"bad quality, distorted, low resolution, watermark\",\n",
        "                    num_frames=Config.NUM_FRAMES,\n",
        "                    controlnet_conditioning_scale=1.0,\n",
        "                    conditioning_frames=pil_controls,\n",
        "                    num_inference_steps=20,\n",
        "                    generator=generator\n",
        "            )\n",
        "            frames = output.frames[0]\n",
        "            combined_frames = []\n",
        "            for i, frame in enumerate(frames):\n",
        "                ctrl = pil_controls[i].resize(frame.size)\n",
        "                new_img = Image.new('RGB', (frame.width * 2, frame.height))\n",
        "                new_img.paste(ctrl, (0, 0))\n",
        "                new_img.paste(frame, (frame.width, 0))\n",
        "                combined_frames.append(np.array(new_img))\n",
        "            combined_frames = np.stack(combined_frames).transpose(0, 3, 1, 2)\n",
        "            wandb_videos.append(wandb.Video(combined_frames, fps=8, format=\"mp4\", caption=f\"{idx}: {prompt}\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Validation failed for sample {idx}: {e}\")\n",
        "    if len(wandb_videos) > 0:\n",
        "        wandb.log({\"val_videos\": wandb_videos})\n",
        "    del pipeline\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def main():\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n",
        "        mixed_precision=\"bf16\",\n",
        "        log_with=\"wandb\",\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "        config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith(\"__\")}\n",
        "        wandb.init(project=\"animatediff-coco-seg\", config=config_dict)\n",
        "    adapter = MotionAdapter.from_pretrained(Config.MOTION_ADAPTER_ID)\n",
        "    controlnet = ControlNetModel.from_pretrained(Config.CONTROLNET_ID)\n",
        "    tmp_pipeline = AnimateDiffPipeline.from_pretrained(\n",
        "        Config.SD_MODEL_ID,\n",
        "        motion_adapter=adapter,\n",
        "        controlnet=controlnet,\n",
        "    )\n",
        "    unet = tmp_pipeline.unet\n",
        "    vae = tmp_pipeline.vae\n",
        "    text_encoder = tmp_pipeline.text_encoder\n",
        "    tokenizer = tmp_pipeline.tokenizer\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(Config.SD_MODEL_ID, subfolder=\"scheduler\")\n",
        "    del tmp_pipeline\n",
        "    vae.requires_grad_(False)\n",
        "    text_encoder.requires_grad_(False)\n",
        "    controlnet.requires_grad_(False)\n",
        "    unet.requires_grad_(False)\n",
        "    motion_params = []\n",
        "    for name, param in unet.named_parameters():\n",
        "        if \"motion_modules\" in name:\n",
        "            param.requires_grad = True\n",
        "            motion_params.append(param)\n",
        "            param.data = param.data.to(torch.float32)\n",
        "    unet.enable_gradient_checkpointing()\n",
        "    controlnet.enable_gradient_checkpointing()\n",
        "    dataset = COCOSyntheticVideoDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, size=Config.RESOLUTION, frames=Config.NUM_FRAMES, max_samples=Config.MAX_SAMPLES)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
        "    val_batch = next(iter(dataloader))\n",
        "    optimizer = bnb.optim.AdamW8bit(motion_params, lr=Config.LEARNING_RATE)\n",
        "    unet, optimizer, dataloader, controlnet = accelerator.prepare(unet, optimizer, dataloader, controlnet)\n",
        "    vae.to(accelerator.device, dtype=torch.bfloat16)\n",
        "    text_encoder.to(accelerator.device, dtype=torch.bfloat16)\n",
        "    global_step = 0\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        unet.train()\n",
        "        progress_bar = tqdm(total=len(dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
        "        for batch in dataloader:\n",
        "            with accelerator.accumulate(unet):\n",
        "                pixel_values = batch[\"pixel_values\"].to(dtype=torch.bfloat16)\n",
        "                condition_values = batch[\"condition_values\"].to(dtype=torch.bfloat16)\n",
        "                input_ids = batch[\"input_ids\"]\n",
        "                b, f, c, h, w = pixel_values.shape\n",
        "                pixel_values_reshaped = pixel_values.view(-1, c, h, w)\n",
        "                latents = vae.encode(pixel_values_reshaped).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "                latents = latents.view(b, f, -1, h // 8, w // 8).permute(0, 2, 1, 3, 4)\n",
        "                noise = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=latents.device).long()\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "                noisy_latents.requires_grad_(True)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "                \n",
        "                # Flatten frames into batch for 2D ControlNet: (b, f, ...) -> (b*f, ...)\n",
        "                noisy_latents_flat = noisy_latents.permute(0, 2, 1, 3, 4).reshape(b * f, -1, h // 8, w // 8)\n",
        "                condition_flat = condition_values.view(b * f, c, h, w)\n",
        "                encoder_hidden_states_rep = encoder_hidden_states.repeat_interleave(f, dim=0)\n",
        "                timesteps_rep = timesteps.repeat_interleave(f)\n",
        "                \n",
        "                # Extract control residuals from 2D spatial features\n",
        "                down_block_res, mid_block_res = controlnet(\n",
        "                    noisy_latents_flat, \n",
        "                    timesteps_rep, \n",
        "                    encoder_hidden_states=encoder_hidden_states_rep, \n",
        "                    controlnet_cond=condition_flat, \n",
        "                    return_dict=False,\n",
        "                )\n",
        "                \n",
        "                # Inject control residuals into UNet\n",
        "                noise_pred = unet(\n",
        "                    noisy_latents,\n",
        "                    timesteps,\n",
        "                    encoder_hidden_states=encoder_hidden_states_rep, \n",
        "                    down_block_additional_residuals=[r.to(dtype=torch.bfloat16) for r in down_block_res],\n",
        "                    mid_block_additional_residual=mid_block_res.to(dtype=torch.bfloat16)\n",
        "                ).sample\n",
        "                loss = torch.nn.functional.mse_loss(noise_pred.float(), noise.float())\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            if accelerator.sync_gradients:\n",
        "                global_step += 1\n",
        "                progress_bar.update(1)\n",
        "                if accelerator.is_main_process:\n",
        "                    wandb.log({\"train_loss\": loss.item()})\n",
        "                    if global_step % Config.LOG_VIDEO_INTERVAL == 0:\n",
        "                        log_validation(accelerator, unet, controlnet, adapter, vae, text_encoder, tokenizer, val_batch)\n",
        "                    if global_step % Config.SAVE_INTERVAL == 0:\n",
        "                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
        "                        accelerator.unwrap_model(unet).save_motion_modules(save_path)\n",
        "    if accelerator.is_main_process:\n",
        "        final_path = os.path.join(Config.OUTPUT_DIR, \"final_motion_module\")\n",
        "        accelerator.unwrap_model(unet).save_motion_modules(final_path)\n",
        "        accelerator.end_training()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch \\\n",
        "    --num_processes=1 \\\n",
        "    --mixed_precision=bf16 \\\n",
        "    train_video_seg.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "import os\n",
        "\n",
        "HF_USERNAME = \"ritishshrirao\"\n",
        "REPO_NAME = \"animatediff-controlnet-coco-segmentation\"\n",
        "LOCAL_CHECKPOINT_DIR = \"/mnt/coco/models/animatediff_coco_finetune/checkpoint-500\"\n",
        "REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "api = HfApi()\n",
        "api.create_repo(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    private=False\n",
        ")\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=LOCAL_CHECKPOINT_DIR,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload AnimateDiff motion module fine-tuned on COCO segmentation\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
