{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1225433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"✓ Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"✓ Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7291e",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your dataset generation parameters here:\n",
    "\n",
    "### ⏱️ **Time Estimation for Kaggle (12hr limit):**\n",
    "- **Qwen-VL-Chat inference**: ~3-5 seconds per image (with short captions)\n",
    "- **5,000 train images**: ~4-7 hours\n",
    "- **500 val images**: ~25-42 minutes\n",
    "- **Total estimated time**: ~4.5-8 hours (fits within 12hr limit!)\n",
    "\n",
    "**Optimizations to speed up:**\n",
    "- Shorter captions (70 tokens max) = faster generation\n",
    "- Batch processing where possible\n",
    "- Skip failed downloads quickly\n",
    "- GPU acceleration (T4/P100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38161966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Generation Configuration\n",
    "class Config:\n",
    "    # Get notebook directory for absolute paths\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "    \n",
    "    # Paths (using absolute paths to avoid directory issues)\n",
    "    COCO_ROOT = os.path.join(NOTEBOOK_DIR, 'coco_data')\n",
    "    OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, 'dataset_output')\n",
    "    \n",
    "    # Dataset sizes\n",
    "    TARGET_TRAIN = 5000  # Training images\n",
    "    TARGET_VAL = 500     # Validation images\n",
    "    \n",
    "    # Quality filtering thresholds\n",
    "    MIN_KEYPOINTS = 10   # Minimum visible keypoints (out of 17)\n",
    "    MIN_PERSON_AREA = 5000  # Minimum person area in pixels²\n",
    "    \n",
    "    # Caption generation\n",
    "    MAX_CAPTION_TOKENS = 70  # CLIP limit is 77, we use 70 for safety\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Model - Using Qwen2-VL-2B (proven to work, no OOM)\n",
    "    MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    MODEL_TYPE = \"qwen2-vl\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Working Directory: {config.NOTEBOOK_DIR}\")\n",
    "print(f\"COCO Root: {config.COCO_ROOT}\")\n",
    "print(f\"Output Directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Target Train Images: {config.TARGET_TRAIN}\")\n",
    "print(f\"Target Val Images: {config.TARGET_VAL}\")\n",
    "print(f\"Quality Thresholds:\")\n",
    "print(f\"  - Min Keypoints: {config.MIN_KEYPOINTS}/17\")\n",
    "print(f\"  - Min Person Area: {config.MIN_PERSON_AREA}px²\")\n",
    "print(f\"Caption Settings:\")\n",
    "print(f\"  - Max Tokens: {config.MAX_CAPTION_TOKENS}\")\n",
    "print(f\"  - Device: {config.DEVICE}\")\n",
    "print(f\"  - Model: {config.MODEL_NAME}\")\n",
    "print(f\"  - Model Type: {config.MODEL_TYPE}\")\n",
    "print(f\"\\n✓ Using Qwen2-VL-2B (proven working, no OOM issues)\")\n",
    "print(f\"  - Smaller than Qwen-VL-Chat\")\n",
    "print(f\"  - Disk cache system for captions\")\n",
    "print(f\"  - Can unload model after caption generation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.COCO_ROOT, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'train2017'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'val2017'), exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Directories created/verified:\")\n",
    "print(f\"  - {config.COCO_ROOT}\")\n",
    "print(f\"  - {config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af652a",
   "metadata": {},
   "source": [
    "## Step 1: Load COCO Annotations\n",
    "\n",
    "Download and load COCO 2017 annotations for person keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "ann_dir = os.path.join(config.COCO_ROOT, 'annotations')\n",
    "train_ann_file = os.path.join(ann_dir, 'person_keypoints_train2017.json')\n",
    "val_ann_file = os.path.join(ann_dir, 'person_keypoints_val2017.json')\n",
    "\n",
    "# Download annotations if they don't exist\n",
    "if not os.path.exists(train_ann_file) or not os.path.exists(val_ann_file):\n",
    "    print(\"⚠️  COCO annotations not found! Downloading...\")\n",
    "    \n",
    "    import zipfile\n",
    "    import urllib.request\n",
    "    \n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    zip_path = os.path.join(config.COCO_ROOT, 'annotations_trainval2017.zip')\n",
    "    \n",
    "    print(f\"Downloading from: {ann_url}\")\n",
    "    print(\"This may take a few minutes (~252 MB)...\")\n",
    "    \n",
    "    # Download with progress\n",
    "    urllib.request.urlretrieve(ann_url, zip_path)\n",
    "    print(\"✓ Download complete! Extracting...\")\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(config.COCO_ROOT)\n",
    "    \n",
    "    # Cleanup zip file\n",
    "    os.remove(zip_path)\n",
    "    print(\"✓ Extraction complete!\")\n",
    "\n",
    "# Now load COCO annotations\n",
    "print(\"\\nLoading COCO annotations...\")\n",
    "coco_train = COCO(train_ann_file)\n",
    "coco_val = COCO(val_ann_file)\n",
    "print(\"✓ COCO annotations loaded successfully!\")\n",
    "print(f\"  - Train images: {len(coco_train.getImgIds())}\")\n",
    "print(f\"  - Val images: {len(coco_val.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469a17b",
   "metadata": {},
   "source": [
    "## Step 2: Filter High-Quality Pose Images\n",
    "\n",
    "Filter COCO dataset for images with high-quality pose annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_high_quality_images(coco, split='train', min_keypoints=10, min_area=5000, max_images=None):\n",
    "    \"\"\"\n",
    "    Filter COCO dataset for high-quality pose images\n",
    "    \n",
    "    Args:\n",
    "        coco: COCO API instance\n",
    "        split: 'train' or 'val'\n",
    "        min_keypoints: Minimum visible keypoints (out of 17)\n",
    "        min_area: Minimum person area in pixels\n",
    "        max_images: Maximum number of images to return\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered image IDs with quality scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILTERING {split.upper()} IMAGES FOR HIGH-QUALITY POSES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Criteria:\")\n",
    "    print(f\"  - Minimum keypoints: {min_keypoints}/17\")\n",
    "    print(f\"  - Minimum person area: {min_area}px²\")\n",
    "    print(f\"  - No crowd annotations\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    cat_ids = coco.getCatIds(catNms=['person'])\n",
    "    all_img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "    \n",
    "    quality_images = []\n",
    "    \n",
    "    for img_id in tqdm(all_img_ids, desc=f\"Filtering {split} images\"):\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids, iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        best_quality_score = 0\n",
    "        best_keypoint_count = 0\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'keypoints' not in ann:\n",
    "                continue\n",
    "            \n",
    "            num_keypoints = ann.get('num_keypoints', 0)\n",
    "            person_area = ann.get('area', 0)\n",
    "            \n",
    "            # Quality filtering\n",
    "            if (num_keypoints >= min_keypoints and \n",
    "                person_area >= min_area and \n",
    "                ann.get('iscrowd', 0) == 0):\n",
    "                \n",
    "                # Quality score: combine keypoints and area\n",
    "                quality_score = num_keypoints * 1.0 + (person_area / 10000) * 0.5\n",
    "                \n",
    "                if quality_score > best_quality_score:\n",
    "                    best_quality_score = quality_score\n",
    "                    best_keypoint_count = num_keypoints\n",
    "        \n",
    "        if best_quality_score > 0:\n",
    "            quality_images.append({\n",
    "                'image_id': img_id,\n",
    "                'quality_score': best_quality_score,\n",
    "                'keypoints': best_keypoint_count\n",
    "            })\n",
    "    \n",
    "    # Sort by quality score (best first)\n",
    "    quality_images.sort(key=lambda x: x['quality_score'], reverse=True)\n",
    "    \n",
    "    # Limit to max_images if specified\n",
    "    if max_images and len(quality_images) > max_images:\n",
    "        quality_images = quality_images[:max_images]\n",
    "    \n",
    "    print(f\"\\n✓ Filtered {len(quality_images)} high-quality images from {len(all_img_ids)} total\")\n",
    "    print(f\"  - Average keypoints: {np.mean([img['keypoints'] for img in quality_images]):.1f}/17\")\n",
    "    print(f\"  - Quality range: {quality_images[-1]['quality_score']:.2f} to {quality_images[0]['quality_score']:.2f}\")\n",
    "    \n",
    "    return quality_images\n",
    "\n",
    "# Check if COCO annotations are loaded\n",
    "if 'coco_train' not in globals() or 'coco_val' not in globals():\n",
    "    print(\"⚠️  ERROR: COCO annotations not loaded!\")\n",
    "    print(\"Please run Cell 5 (Step 1: Load COCO Annotations) first.\")\n",
    "    raise NameError(\"coco_train and coco_val are not defined. Run the previous cells in order.\")\n",
    "\n",
    "# Filter train and val images\n",
    "train_images = filter_high_quality_images(\n",
    "    coco_train, \n",
    "    split='train',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_TRAIN\n",
    ")\n",
    "\n",
    "val_images = filter_high_quality_images(\n",
    "    coco_val, \n",
    "    split='val',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_VAL\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FILTERING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Train: {len(train_images)} images selected\")\n",
    "print(f\"✓ Val: {len(val_images)} images selected\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08231a",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Caption Generator\n",
    "\n",
    "Load Qwen-VL-Chat model for generating CLIP-compatible captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies for Qwen2-VL-2B\n",
    "import subprocess\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "print(\"Installing dependencies for Qwen2-VL-2B...\")\n",
    "\n",
    "# First, clear any existing GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU memory cleared\")\n",
    "\n",
    "# Install compatible versions\n",
    "packages = [\n",
    "    'transformers>=4.45.0',\n",
    "    'accelerate',\n",
    "    'qwen-vl-utils',\n",
    "    'pillow',\n",
    "    'torchvision'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"\\n✓ All dependencies installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690852af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Status Check\n",
    "import gc\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPU MEMORY STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Force garbage collection and clear cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"Total GPU Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Get current memory usage\n",
    "    current_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = props.total_memory / 1e9\n",
    "    free = total - (reserved)\n",
    "    \n",
    "    print(f\"\\nCurrent Memory Usage:\")\n",
    "    print(f\"  - Allocated: {current_allocated:.2f} GB\")\n",
    "    print(f\"  - Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  - Free: {free:.2f} GB\")\n",
    "    \n",
    "    if current_allocated > 10:\n",
    "        print(f\"\\n⚠️  WARNING: High GPU memory usage!\")\n",
    "        print(\"Clearing unneeded variables...\")\n",
    "        # Try to identify and clear large objects\n",
    "        if 'train_images' in globals() or 'val_images' in globals():\n",
    "            print(\"  - Keeping image lists (needed for caption generation)\")\n",
    "        if 'coco_train' in globals() or 'coco_val' in globals():\n",
    "            print(\"  - Keeping COCO objects (needed for image info)\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available!\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionGenerator:\n",
    "    \"\"\"Ultra-optimized caption generator for Kaggle - minimal memory footprint\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2-VL-2B-Instruct\"):\n",
    "        \"\"\"Initialize with aggressive memory management\"\"\"\n",
    "        import gc\n",
    "        from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        # Pre-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Load processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Load model in fp16 for memory efficiency\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        ).eval()\n",
    "        \n",
    "        print(f\"✓ Model loaded successfully\")\n",
    "        \n",
    "        # Post-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        \"\"\"Generate caption with minimal memory usage\"\"\"\n",
    "        import gc\n",
    "        from PIL import Image as PILImage\n",
    "        \n",
    "        try:\n",
    "            # Pre-process cleanup\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Load and resize image (critical for memory)\n",
    "            image = PILImage.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Aggressive downsizing - max 768 pixels\n",
    "            if max(image.size) > 768:\n",
    "                ratio = 768 / max(image.size)\n",
    "                new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "                image = image.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            \n",
    "            # Create prompt\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image for image generation. Include: main subjects, their positions, colors, lighting, mood, background, and style. Be specific and vivid.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process with minimal overhead\n",
    "            text_prompt = self.processor.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=text_prompt,\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=60,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            caption = self.processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean caption\n",
    "            caption = caption.strip()\n",
    "            \n",
    "            # Remove common prefixes\n",
    "            for prefix in ['assistant:', '<|assistant|>', 'sure ', 'here ', 'the image']:\n",
    "                if caption.lower().startswith(prefix):\n",
    "                    caption = caption[len(prefix):].strip()\n",
    "            \n",
    "            # Enforce 70-token limit (hard cutoff)\n",
    "            words = caption.split()\n",
    "            if len(words) > 70:\n",
    "                caption = ' '.join(words[:70])\n",
    "            \n",
    "            # Ensure proper ending\n",
    "            if caption and not caption.endswith(('.', '!', '?')):\n",
    "                caption += '.'\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del image, inputs, output_ids, text_prompt, conversation\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            return caption if len(caption) > 5 else \"A person in a scene.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error: {str(e)[:40]}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return \"A person in a scene.\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Free all GPU memory\"\"\"\n",
    "        import gc\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"✓ Model cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Caption Generator\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING CAPTION GENERATOR\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loading Qwen2-VL-2B model (this will take 2-3 minutes)...\")\n",
    "print()\n",
    "\n",
    "caption_gen = CaptionGenerator(model_name=config.MODEL_NAME)\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"✓ CAPTION GENERATOR READY!\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d831948",
   "metadata": {},
   "source": [
    "## Step 4: Generate Training Captions\n",
    "\n",
    "Download training images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training Captions - Ultra-optimized\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "if 'caption_gen' not in globals():\n",
    "    raise NameError(\"Run Cell 13 (Initialize Caption Generator) first!\")\n",
    "\n",
    "train_captions = {}\n",
    "img_dir_train = os.path.join(config.COCO_ROOT, 'train2017')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GENERATING {len(train_images)} TRAINING CAPTIONS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(train_images, desc=\"Train\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_train.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_train, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            train_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "    \n",
    "    # Periodic cleanup\n",
    "    if (success + failed) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n✓ Generated: {success}\")\n",
    "print(f\"✗ Failed: {failed}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8cfcd",
   "metadata": {},
   "source": [
    "## Step 5: Generate Validation Captions\n",
    "\n",
    "Download validation images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0113035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Validation Captions - Ultra-optimized\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "if 'caption_gen' not in globals():\n",
    "    raise NameError(\"Run Cell 13 (Initialize Caption Generator) first!\")\n",
    "\n",
    "val_captions = {}\n",
    "img_dir_val = os.path.join(config.COCO_ROOT, 'val2017')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GENERATING {len(val_images)} VALIDATION CAPTIONS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(val_images, desc=\"Val\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_val, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            val_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "    \n",
    "    # Periodic cleanup\n",
    "    if (success + failed) % 50 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n✓ Generated: {success}\")\n",
    "print(f\"✗ Failed: {failed}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d388969",
   "metadata": {},
   "source": [
    "## Step 6: Save Caption Files\n",
    "\n",
    "Save the generated captions to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset & Cleanup\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Cleanup model\n",
    "if 'caption_gen' in globals():\n",
    "    caption_gen.cleanup()\n",
    "\n",
    "# Save captions\n",
    "train_out = os.path.join(config.OUTPUT_DIR, 'train_captions.json')\n",
    "val_out = os.path.join(config.OUTPUT_DIR, 'val_captions.json')\n",
    "\n",
    "with open(train_out, 'w') as f:\n",
    "    json.dump(train_captions, f, indent=2)\n",
    "\n",
    "with open(val_out, 'w') as f:\n",
    "    json.dump(val_captions, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train: {len(train_captions)} captions → {train_out}\")\n",
    "print(f\"Val: {len(val_captions)} captions → {val_out}\")\n",
    "print(f\"\\nStats:\")\n",
    "print(f\"  Total: {len(train_captions) + len(val_captions)} images\")\n",
    "if train_captions:\n",
    "    avg_train = sum(len(c.split()) for c in train_captions.values()) / len(train_captions)\n",
    "    print(f\"  Avg caption (train): {avg_train:.1f} tokens\")\n",
    "if val_captions:\n",
    "    avg_val = sum(len(c.split()) for c in val_captions.values()) / len(val_captions)\n",
    "    print(f\"  Avg caption (val): {avg_val:.1f} tokens\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd201e",
   "metadata": {},
   "source": [
    "## Step 7: Verify Dataset Quality\n",
    "\n",
    "Check a few samples to verify caption quality and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8271f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Dataset (Quick Check Only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE CAPTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if train_captions:\n",
    "    for i, (fname, cap) in enumerate(list(train_captions.items())[:2]):\n",
    "        print(f\"\\nTrain[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")\n",
    "\n",
    "if val_captions:\n",
    "    for i, (fname, cap) in enumerate(list(val_captions.items())[:2]):\n",
    "        print(f\"\\nVal[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")\n",
    "\n",
    "print(\"\\n✅ Ready for ControlNet training!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
