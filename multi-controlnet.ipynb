{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install diffusers transformers accelerate bitsandbytes wandb pycocotools kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# REPO_ID = \"ritishshrirao/controlnet-coco-multi\"\n",
    "# LOCAL_DIR = \"pretrained_weights\"\n",
    "\n",
    "# def download_models():\n",
    "#     os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "    \n",
    "#     print(f\"Downloading weights from {REPO_ID}...\")\n",
    "    \n",
    "#     # 1. Download Segmentation Weights\n",
    "#     print(\"... Fetching Segmentation Model\")\n",
    "#     seg_path = hf_hub_download(\n",
    "#         repo_id=REPO_ID,\n",
    "#         filename=\"diffusion_pytorch_model.safetensors\",\n",
    "#         subfolder=\"segmentation\",\n",
    "#         local_dir=LOCAL_DIR\n",
    "#     )\n",
    "    \n",
    "#     # 2. Download BBox Weights\n",
    "#     print(\"... Fetching BBox Model\")\n",
    "#     bbox_path = hf_hub_download(\n",
    "#         repo_id=REPO_ID,\n",
    "#         filename=\"diffusion_pytorch_model.safetensors\",\n",
    "#         subfolder=\"bbox\",\n",
    "#         local_dir=LOCAL_DIR\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\nDownload Complete.\")\n",
    "#     print(f\"Segmentation: {seg_path}\")\n",
    "#     print(f\"BBox: {bbox_path}\")\n",
    "\n",
    "# download_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_multi_controlnet1.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "\n",
    "# Imports\n",
    "from accelerate import Accelerator\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    ControlNetModel, \n",
    "    MultiControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# TF32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = \"data\"\n",
    "    COCO_ROOT = os.path.join(DATA_ROOT, \"coco2017\")\n",
    "    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n",
    "    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n",
    "    \n",
    "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "    OUTPUT_DIR = \"controlnet-coco-multi-h100-new\"\n",
    "    \n",
    "    # Resume\n",
    "    # Pretrained weights\n",
    "    PRETRAINED_WEIGHTS_DIR = \"pretrained_weights\" \n",
    "    \n",
    "    # Resume checkpoint\n",
    "    RESUME_FROM_CHECKPOINT = \"latest\"\n",
    "    \n",
    "    # Hyperparams\n",
    "    RESOLUTION = 512\n",
    "    BATCH_SIZE = 64\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    LEARNING_RATE = 2e-5   \n",
    "    NUM_EPOCHS = 15\n",
    "    \n",
    "    # Logging\n",
    "    LOG_INTERVAL = 200       \n",
    "    LOG_BATCH_SIZE = 8\n",
    "    SAVE_INTERVAL = 1000\n",
    "    MAX_SAMPLES = None\n",
    "    \n",
    "    # Data\n",
    "    NUM_WORKERS = 16\n",
    "    \n",
    "    # Dropout\n",
    "    PROMPT_DROPOUT_PROB = 0.4\n",
    "    PROB_SEG_ONLY = 0.35\n",
    "    PROB_BBOX_ONLY = 0.35\n",
    "\n",
    "# Download\n",
    "def download_file(url, save_path):\n",
    "    if os.path.exists(save_path): return\n",
    "    print(f'Downloading {url} to {save_path}...')\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        for data in response.iter_content(1024):\n",
    "            file.write(data)\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    print(f'Extracting {zip_path}...')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def setup_coco_dataset():\n",
    "    os.makedirs(Config.DATA_ROOT, exist_ok=True)\n",
    "    os.makedirs(Config.COCO_ROOT, exist_ok=True)\n",
    "    \n",
    "    train_images_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "    annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    \n",
    "    zip_train = os.path.join(Config.DATA_ROOT, \"train2017.zip\")\n",
    "    zip_ann = os.path.join(Config.DATA_ROOT, \"annotations.zip\")\n",
    "    \n",
    "    if not os.path.exists(Config.TRAIN_IMG_DIR):\n",
    "        download_file(train_images_url, zip_train)\n",
    "        unzip_file(zip_train, Config.COCO_ROOT)\n",
    "        if os.path.exists(zip_train): os.remove(zip_train)\n",
    "        \n",
    "    if not os.path.exists(os.path.dirname(Config.TRAIN_ANN_FILE)):\n",
    "        download_file(annotations_url, zip_ann)\n",
    "        unzip_file(zip_ann, Config.COCO_ROOT)\n",
    "        if os.path.exists(zip_ann): os.remove(zip_ann)\n",
    "\n",
    "# Dataset\n",
    "class COCOMultiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "        if max_samples: self.img_ids = self.img_ids[:max_samples]\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]), \n",
    "        ])\n",
    "        \n",
    "        self.cond_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "        \n",
    "        self.color_map = self._generate_color_map()\n",
    "\n",
    "    def _generate_color_map(self):\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        palette = {}\n",
    "        for cat in cats:\n",
    "            import hashlib\n",
    "            h = hashlib.md5(str(cat['id']).encode()).hexdigest()\n",
    "            palette[cat['id']] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n",
    "        return palette\n",
    "\n",
    "    def draw_segmentation_map(self, img_shape, anns):\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        for ann in anns:\n",
    "            color = self.color_map.get(ann['category_id'], (255, 255, 255))\n",
    "            binary_mask = self.coco.annToMask(ann)\n",
    "            mask[binary_mask == 1] = color\n",
    "        return Image.fromarray(mask)\n",
    "\n",
    "    def draw_bbox_map(self, img_shape, anns):\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        canvas = Image.fromarray(mask)\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            color = self.color_map.get(ann['category_id'], (255, 255, 255))\n",
    "            draw.rectangle([x, y, x+w, y+h], fill=color, outline=None)\n",
    "        return canvas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        try:\n",
    "            image_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n",
    "        seg_image = self.draw_segmentation_map(image.size, anns)\n",
    "        bbox_image = self.draw_bbox_map(image.size, anns)\n",
    "        \n",
    "        cat_ids = [ann['category_id'] for ann in anns]\n",
    "        cat_names = list(set([cat['name'] for cat in self.coco.loadCats(cat_ids)]))\n",
    "        text_prompt = \"\" if random.random() < Config.PROMPT_DROPOUT_PROB else \\\n",
    "                      f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": self.image_transforms(image),\n",
    "            \"seg_pixel_values\": self.cond_transforms(seg_image),\n",
    "            \"bbox_pixel_values\": self.cond_transforms(bbox_image),\n",
    "            \"input_ids\": self.tokenizer(\n",
    "                text_prompt, max_length=self.tokenizer.model_max_length, \n",
    "                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "            ).input_ids[0],\n",
    "            \"raw_prompt\": text_prompt\n",
    "        }\n",
    "\n",
    "# Validation\n",
    "def log_validation(accelerator, controlnet_seg, controlnet_bbox, unet, vae, text_encoder, tokenizer, val_batch, step):\n",
    "    if not accelerator.is_main_process: return\n",
    "    print(f'Running Validation at step {step}...')\n",
    "\n",
    "    try:\n",
    "        multi_controlnet = MultiControlNetModel([controlnet_seg, controlnet_bbox])\n",
    "        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            Config.MODEL_ID,\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "            unet=unet, controlnet=multi_controlnet,\n",
    "            safety_checker=None, torch_dtype=torch.bfloat16\n",
    "        ).to(accelerator.device)\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        log_images = []\n",
    "        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n",
    "        \n",
    "        def to_pil(tensor):\n",
    "            tensor = tensor.detach().cpu().float()\n",
    "            if tensor.shape[0] == 3: tensor = tensor.permute(1, 2, 0)\n",
    "            if tensor.min() < 0: tensor = (tensor + 1) / 2.0\n",
    "            tensor = tensor.clamp(0, 1).numpy()\n",
    "            return Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            prompt = val_batch[\"raw_prompt\"][i] or \"A photorealistic image\"\n",
    "            gt_image = to_pil(val_batch[\"pixel_values\"][i])\n",
    "            seg_image = to_pil(val_batch[\"seg_pixel_values\"][i])\n",
    "            bbox_image = to_pil(val_batch[\"bbox_pixel_values\"][i])\n",
    "            \n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                pred_both = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[1.0, 1.0]).images[0]\n",
    "                pred_seg = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[1.0, 0.0]).images[0]\n",
    "                pred_bbox = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[0.0, 1.0]).images[0]\n",
    "\n",
    "            log_images.extend([\n",
    "                wandb.Image(seg_image, caption=f\"{i} Seg In\"),\n",
    "                wandb.Image(bbox_image, caption=f\"{i} BBox In\"),\n",
    "                wandb.Image(gt_image, caption=f\"{i} Truth\"),\n",
    "                wandb.Image(pred_both, caption=f\"{i} Both\"),\n",
    "                wandb.Image(pred_seg, caption=f\"{i} Seg Only\"),\n",
    "                wandb.Image(pred_bbox, caption=f\"{i} BBox Only\")\n",
    "            ])\n",
    "        \n",
    "        tracker = accelerator.get_tracker(\"wandb\")\n",
    "        if tracker: tracker.log({\"validation\": log_images}, step=step)\n",
    "        \n",
    "        del pipeline, multi_controlnet\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping validation log due to error: {e}\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    setup_coco_dataset()\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n",
    "        mixed_precision=\"bf16\",\n",
    "        log_with=\"wandb\",\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        wandb_key = os.getenv(\"wandb\")\n",
    "        if wandb_key: wandb.login(key=wandb_key)\n",
    "        cfg_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith(\"__\")}\n",
    "        accelerator.init_trackers(\"controlnet-coco-multi-h100\", config=cfg_dict)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    \n",
    "    # Load weights\n",
    "    seg_path = os.path.join(Config.PRETRAINED_WEIGHTS_DIR, \"segmentation\")\n",
    "    bbox_path = os.path.join(Config.PRETRAINED_WEIGHTS_DIR, \"bbox\")\n",
    "    \n",
    "    print(\"Initializing ControlNets...\")\n",
    "    controlnet_seg = ControlNetModel.from_unet(unet)\n",
    "    controlnet_bbox = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    loaded_pretrained = False\n",
    "    \n",
    "    full_checkpoint_exists = False\n",
    "    if os.path.exists(Config.OUTPUT_DIR):\n",
    "        dirs = [d for d in os.listdir(Config.OUTPUT_DIR) if d.startswith(\"checkpoint\")]\n",
    "        if dirs: full_checkpoint_exists = True\n",
    "\n",
    "    if not full_checkpoint_exists and os.path.exists(seg_path) and os.path.exists(bbox_path):\n",
    "        if accelerator.is_main_process:\n",
    "            print(f'No full checkpoint found. Loading Pretrained Weights from {Config.PRETRAINED_WEIGHTS_DIR}...')\n",
    "            \n",
    "            try:\n",
    "                controlnet_seg = ControlNetModel.from_pretrained(seg_path)\n",
    "                print('Segmentation Weights Loaded.')\n",
    "                \n",
    "                controlnet_bbox = ControlNetModel.from_pretrained(bbox_path)\n",
    "                print('BBox Weights Loaded.')\n",
    "                loaded_pretrained = True\n",
    "            except Exception as e:\n",
    "                print(f'Failed to load pretrained weights: {e}')\n",
    "                print('Falling back to scratch training.')\n",
    "\n",
    "    controlnet_seg.train()\n",
    "    controlnet_bbox.train()\n",
    "    controlnet_seg.enable_gradient_checkpointing()\n",
    "    controlnet_bbox.enable_gradient_checkpointing()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "    dataset = COCOMultiDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    val_batch = next(iter(train_dataloader))\n",
    "    params = list(controlnet_seg.parameters()) + list(controlnet_bbox.parameters())\n",
    "    optimizer = bnb.optim.AdamW8bit(params, lr=Config.LEARNING_RATE)\n",
    "\n",
    "    controlnet_seg, controlnet_bbox, optimizer, train_dataloader = accelerator.prepare(\n",
    "        controlnet_seg, controlnet_bbox, optimizer, train_dataloader\n",
    "    )\n",
    "    \n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    unet.to(accelerator.device)\n",
    "\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume\n",
    "    if Config.RESUME_FROM_CHECKPOINT == \"latest\" and os.path.exists(Config.OUTPUT_DIR):\n",
    "        dirs = [d for d in os.listdir(Config.OUTPUT_DIR) if d.startswith(\"checkpoint\")]\n",
    "        if dirs:\n",
    "            path = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "            accelerator.print(f'Resuming full training state from {path}')\n",
    "            accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "            start_epoch = global_step // len(train_dataloader)\n",
    "        elif loaded_pretrained:\n",
    "            accelerator.print(f'Starting new run, initialized with weights from {Config.PRETRAINED_WEIGHTS_DIR}')\n",
    "        else:\n",
    "            accelerator.print('No checkpoint or pretrained weights found. Starting from scratch.')\n",
    "\n",
    "    if accelerator.is_main_process: print(f'Starting training from Step {global_step}...')\n",
    "    \n",
    "    for epoch in range(start_epoch, Config.NUM_EPOCHS):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            with accelerator.accumulate([controlnet_seg, controlnet_bbox]):\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.bfloat16)).latent_dist.sample() * vae.config.scaling_factor\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                \n",
    "                # Logic\n",
    "                r = random.random()\n",
    "                \n",
    "                active_down = []\n",
    "                active_mid = []\n",
    "\n",
    "                # Segmentation\n",
    "                if r < Config.PROB_SEG_ONLY or r >= (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY):\n",
    "                    real_seg = batch[\"seg_pixel_values\"].to(dtype=torch.bfloat16)\n",
    "                    d, m = controlnet_seg(\n",
    "                        noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states, \n",
    "                        controlnet_cond=real_seg, return_dict=False\n",
    "                    )\n",
    "                    active_down.append(d)\n",
    "                    active_mid.append(m)\n",
    "\n",
    "                # BBox\n",
    "                if (r >= Config.PROB_SEG_ONLY and r < (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY)) or r >= (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY):\n",
    "                    real_bbox = batch[\"bbox_pixel_values\"].to(dtype=torch.bfloat16)\n",
    "                    d, m = controlnet_bbox(\n",
    "                        noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states, \n",
    "                        controlnet_cond=real_bbox, return_dict=False\n",
    "                    )\n",
    "                    active_down.append(d)\n",
    "                    active_mid.append(m)\n",
    "\n",
    "                # Aggregate\n",
    "                if len(active_down) == 0:\n",
    "                    down_block_res = None\n",
    "                    mid_block_res = None\n",
    "                else:\n",
    "                    down_block_res = active_down[0]\n",
    "                    mid_block_res = active_mid[0]\n",
    "                    for i in range(1, len(active_down)):\n",
    "                        down_block_res = [a + b for a, b in zip(down_block_res, active_down[i])]\n",
    "                        mid_block_res = mid_block_res + active_mid[i]\n",
    "                \n",
    "                if down_block_res is not None:\n",
    "                    down_block_res = [res.to(dtype=torch.bfloat16) for res in down_block_res]\n",
    "                    mid_block_res = mid_block_res.to(dtype=torch.bfloat16)\n",
    "\n",
    "                model_pred = unet(\n",
    "                    noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states,\n",
    "                    down_block_additional_residuals=down_block_res,\n",
    "                    mid_block_additional_residual=mid_block_res,\n",
    "                ).sample\n",
    "                \n",
    "                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                global_step += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if accelerator.is_main_process:\n",
    "                    accelerator.log({\"train_loss\": loss.item()}, step=global_step)\n",
    "                    if global_step % Config.LOG_INTERVAL == 0:\n",
    "                        u_seg = accelerator.unwrap_model(controlnet_seg)\n",
    "                        u_bbox = accelerator.unwrap_model(controlnet_bbox)\n",
    "                        log_validation(accelerator, u_seg, u_bbox, unet, vae, text_encoder, tokenizer, val_batch, global_step)\n",
    "                    if global_step % Config.SAVE_INTERVAL == 0:\n",
    "                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        try:\n",
    "                            checkpoints = sorted([d for d in os.listdir(Config.OUTPUT_DIR) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split(\"-\")[1]))\n",
    "                            if len(checkpoints) > 2:\n",
    "                                for ckpt in checkpoints[:-2]: shutil.rmtree(os.path.join(Config.OUTPUT_DIR, ckpt))\n",
    "                        except Exception: pass\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.unwrap_model(controlnet_seg).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_seg\"))\n",
    "        accelerator.unwrap_model(controlnet_bbox).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_bbox\"))\n",
    "        accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --mixed_precision=bf16 train_multi_controlnet1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile upload_hf.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from diffusers import ControlNetModel, UNet2DConditionModel\n",
    "\n",
    "# Config\n",
    "HF_USERNAME = \"ritishshrirao\"\n",
    "REPO_NAME = \"controlnet-coco-multi\"\n",
    "MODEL_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "BASE_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Checkpoint\n",
    "CHECKPOINT_PATH = (\n",
    "    \"/teamspace/studios/this_studio/\"\n",
    "    \"controlnet-coco-multi-h100-new/checkpoint-10000\"\n",
    ")\n",
    "\n",
    "# Utils\n",
    "def load_weights_to_model(model, weights_path):\n",
    "    print(f\"Loading weights from {weights_path}...\")\n",
    "\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(weights_path)\n",
    "\n",
    "    if weights_path.endswith(\".safetensors\"):\n",
    "        state_dict = load_file(weights_path)\n",
    "    else:\n",
    "        state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
    "\n",
    "    # Remove DDP prefix\n",
    "    cleaned = {}\n",
    "    for k, v in state_dict.items():\n",
    "        cleaned[k[7:]] = v if k.startswith(\"module.\") else v\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing keys: {len(missing)}\")\n",
    "    if unexpected:\n",
    "        print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "    print(\"Weights loaded.\")\n",
    "    return model\n",
    "\n",
    "# Main\n",
    "def extract_and_upload():\n",
    "    hf_token = os.getenv(\"HF\")\n",
    "    if not hf_token:\n",
    "        raise RuntimeError(\"HF environment variable not set\")\n",
    "\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        raise RuntimeError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "\n",
    "    print(f\"Processing Checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "    seg_weights = os.path.join(CHECKPOINT_PATH, \"model.safetensors\")\n",
    "    bbox_weights = os.path.join(CHECKPOINT_PATH, \"model_1.safetensors\")\n",
    "\n",
    "    if not os.path.exists(seg_weights) or not os.path.exists(bbox_weights):\n",
    "        raise RuntimeError(\"Expected model.safetensors and model_1.safetensors\")\n",
    "\n",
    "    print(\"Initializing model architecture...\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        BASE_MODEL_ID, subfolder=\"unet\"\n",
    "    )\n",
    "\n",
    "    c_seg = ControlNetModel.from_unet(unet)\n",
    "    c_bbox = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    c_seg = load_weights_to_model(c_seg, seg_weights)\n",
    "    c_bbox = load_weights_to_model(c_bbox, bbox_weights)\n",
    "\n",
    "    export_dir = \"temp_clean_models\"\n",
    "    if os.path.exists(export_dir):\n",
    "        shutil.rmtree(export_dir)\n",
    "\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Saving clean safetensors models...\")\n",
    "    c_seg.save_pretrained(\n",
    "        os.path.join(export_dir, \"segmentation\"),\n",
    "        safe_serialization=True,\n",
    "    )\n",
    "    c_bbox.save_pretrained(\n",
    "        os.path.join(export_dir, \"bbox\"),\n",
    "        safe_serialization=True,\n",
    "    )\n",
    "\n",
    "    api = HfApi(token=hf_token)\n",
    "\n",
    "    print(f\"Creating / checking repo: {MODEL_ID}\")\n",
    "    create_repo(\n",
    "        repo_id=MODEL_ID,\n",
    "        exist_ok=True,\n",
    "        private=False,\n",
    "        token=hf_token,\n",
    "    )\n",
    "\n",
    "    print(\"Uploading to Hugging Face Hub...\")\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(export_dir, \"segmentation\"),\n",
    "        repo_id=MODEL_ID,\n",
    "        path_in_repo=\"segmentation\",\n",
    "        commit_message=\"Upload segmentation ControlNet\",\n",
    "    )\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(export_dir, \"bbox\"),\n",
    "        repo_id=MODEL_ID,\n",
    "        path_in_repo=\"bbox\",\n",
    "        commit_message=\"Upload bbox ControlNet\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nUpload complete!\")\n",
    "    print(f\"https://huggingface.co/{MODEL_ID}/tree/main\")\n",
    "\n",
    "    shutil.rmtree(export_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python upload_hf.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
