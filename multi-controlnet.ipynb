{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.49.0)\n",
      "Requirement already satisfied: wandb in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.23.1)\n",
      "Requirement already satisfied: pycocotools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: kagglehub in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: importlib_metadata in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (3.20.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (0.36.0)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (0.7.0)\n",
      "Requirement already satisfied: Pillow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from diffusers) (12.0.0)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->diffusers) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->diffusers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->diffusers) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->diffusers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->diffusers) (0.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (1.2.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: pydantic<3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (2.12.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->diffusers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from anyio->httpx<1.0.0->diffusers) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from importlib_metadata->diffusers) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install diffusers transformers accelerate bitsandbytes wandb pycocotools kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_multi_controlnet1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_multi_controlnet1.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "\n",
    "# Accelerate & Diffusers\n",
    "from accelerate import Accelerator\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    ControlNetModel, \n",
    "    MultiControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# Enable TF32 for H100 speedup\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = \"data\"\n",
    "    COCO_ROOT = os.path.join(DATA_ROOT, \"coco2017\")\n",
    "    TRAIN_IMG_DIR = os.path.join(COCO_ROOT, \"train2017\")\n",
    "    TRAIN_ANN_FILE = os.path.join(COCO_ROOT, \"annotations/instances_train2017.json\")\n",
    "    \n",
    "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "    OUTPUT_DIR = \"controlnet-coco-multi-h100-new\"\n",
    "    \n",
    "    # Optional: Path to previously trained single ControlNets to jumpstart training\n",
    "    PRETRAINED_SEG_PATH = None \n",
    "    PRETRAINED_BBOX_PATH = None\n",
    "    \n",
    "    # Checkpoints\n",
    "    RESUME_FROM_CHECKPOINT = \"latest\"\n",
    "    \n",
    "    # Hyperparameters\n",
    "    RESOLUTION = 512\n",
    "    BATCH_SIZE = 64\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    LEARNING_RATE = 2e-5   \n",
    "    NUM_EPOCHS = 15        \n",
    "    \n",
    "    # Logging\n",
    "    LOG_INTERVAL = 200       \n",
    "    LOG_BATCH_SIZE = 8\n",
    "    SAVE_INTERVAL = 1000\n",
    "    MAX_SAMPLES = None\n",
    "    \n",
    "    # Data Loading\n",
    "    NUM_WORKERS = 16\n",
    "    \n",
    "    # Dropout Probabilities\n",
    "    PROMPT_DROPOUT_PROB = 0.4\n",
    "    PROB_SEG_ONLY = 0.35\n",
    "    PROB_BBOX_ONLY = 0.35\n",
    "\n",
    "# --- DOWNLOAD UTILS ---\n",
    "def download_file(url, save_path):\n",
    "    if os.path.exists(save_path): return\n",
    "    print(f\"Downloading {url} to {save_path}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        for data in response.iter_content(1024):\n",
    "            file.write(data)\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    print(f\"Extracting {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def setup_coco_dataset():\n",
    "    os.makedirs(Config.DATA_ROOT, exist_ok=True)\n",
    "    os.makedirs(Config.COCO_ROOT, exist_ok=True)\n",
    "    \n",
    "    train_images_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "    annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    \n",
    "    zip_train = os.path.join(Config.DATA_ROOT, \"train2017.zip\")\n",
    "    zip_ann = os.path.join(Config.DATA_ROOT, \"annotations.zip\")\n",
    "    \n",
    "    if not os.path.exists(Config.TRAIN_IMG_DIR):\n",
    "        download_file(train_images_url, zip_train)\n",
    "        unzip_file(zip_train, Config.COCO_ROOT)\n",
    "        if os.path.exists(zip_train): os.remove(zip_train)\n",
    "        \n",
    "    if not os.path.exists(os.path.dirname(Config.TRAIN_ANN_FILE)):\n",
    "        download_file(annotations_url, zip_ann)\n",
    "        unzip_file(zip_ann, Config.COCO_ROOT)\n",
    "        if os.path.exists(zip_ann): os.remove(zip_ann)\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "class COCOMultiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, tokenizer, size=512, max_samples=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.img_ids = [img_id for img_id in self.img_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "        if max_samples: self.img_ids = self.img_ids[:max_samples]\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]), \n",
    "        ])\n",
    "        \n",
    "        self.cond_transforms = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "        \n",
    "        self.color_map = self._generate_color_map()\n",
    "\n",
    "    def _generate_color_map(self):\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        palette = {}\n",
    "        for cat in cats:\n",
    "            import hashlib\n",
    "            h = hashlib.md5(str(cat['id']).encode()).hexdigest()\n",
    "            palette[cat['id']] = (int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))\n",
    "        return palette\n",
    "\n",
    "    def draw_segmentation_map(self, img_shape, anns):\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        for ann in anns:\n",
    "            color = self.color_map.get(ann['category_id'], (255, 255, 255))\n",
    "            binary_mask = self.coco.annToMask(ann)\n",
    "            mask[binary_mask == 1] = color\n",
    "        return Image.fromarray(mask)\n",
    "\n",
    "    def draw_bbox_map(self, img_shape, anns):\n",
    "        mask = np.zeros((img_shape[1], img_shape[0], 3), dtype=np.uint8)\n",
    "        canvas = Image.fromarray(mask)\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            color = self.color_map.get(ann['category_id'], (255, 255, 255))\n",
    "            draw.rectangle([x, y, x+w, y+h], fill=color, outline=None)\n",
    "        return canvas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        try:\n",
    "            image_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n",
    "        seg_image = self.draw_segmentation_map(image.size, anns)\n",
    "        bbox_image = self.draw_bbox_map(image.size, anns)\n",
    "        \n",
    "        cat_ids = [ann['category_id'] for ann in anns]\n",
    "        cat_names = list(set([cat['name'] for cat in self.coco.loadCats(cat_ids)]))\n",
    "        text_prompt = \"\" if random.random() < Config.PROMPT_DROPOUT_PROB else \\\n",
    "                      f\"A photorealistic image containing {', '.join(cat_names)}\" if cat_names else \"A photorealistic image\"\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": self.image_transforms(image),\n",
    "            \"seg_pixel_values\": self.cond_transforms(seg_image),\n",
    "            \"bbox_pixel_values\": self.cond_transforms(bbox_image),\n",
    "            \"input_ids\": self.tokenizer(\n",
    "                text_prompt, max_length=self.tokenizer.model_max_length, \n",
    "                padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "            ).input_ids[0],\n",
    "            \"raw_prompt\": text_prompt\n",
    "        }\n",
    "\n",
    "# --- VALIDATION HELPER ---\n",
    "def log_validation(accelerator, controlnet_seg, controlnet_bbox, unet, vae, text_encoder, tokenizer, val_batch, step):\n",
    "    if not accelerator.is_main_process: return\n",
    "    print(f\"Running Validation at step {step}...\")\n",
    "\n",
    "    try:\n",
    "        multi_controlnet = MultiControlNetModel([controlnet_seg, controlnet_bbox])\n",
    "        pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            Config.MODEL_ID,\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "            unet=unet, controlnet=multi_controlnet,\n",
    "            safety_checker=None, torch_dtype=torch.bfloat16\n",
    "        ).to(accelerator.device)\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        \n",
    "        log_images = []\n",
    "        num_samples = min(len(val_batch[\"raw_prompt\"]), Config.LOG_BATCH_SIZE)\n",
    "        \n",
    "        def to_pil(tensor):\n",
    "            tensor = tensor.detach().cpu().float()\n",
    "            if tensor.shape[0] == 3: tensor = tensor.permute(1, 2, 0)\n",
    "            if tensor.min() < 0: tensor = (tensor + 1) / 2.0\n",
    "            tensor = tensor.clamp(0, 1).numpy()\n",
    "            return Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            prompt = val_batch[\"raw_prompt\"][i] or \"A photorealistic image\"\n",
    "            gt_image = to_pil(val_batch[\"pixel_values\"][i])\n",
    "            seg_image = to_pil(val_batch[\"seg_pixel_values\"][i])\n",
    "            bbox_image = to_pil(val_batch[\"bbox_pixel_values\"][i])\n",
    "            \n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(42 + i)\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                pred_both = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[1.0, 1.0]).images[0]\n",
    "                pred_seg = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[1.0, 0.0]).images[0]\n",
    "                pred_bbox = pipeline(prompt, image=[seg_image, bbox_image], num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[0.0, 1.0]).images[0]\n",
    "\n",
    "            log_images.extend([\n",
    "                wandb.Image(seg_image, caption=f\"{i} Seg In\"),\n",
    "                wandb.Image(bbox_image, caption=f\"{i} BBox In\"),\n",
    "                wandb.Image(gt_image, caption=f\"{i} Truth\"),\n",
    "                wandb.Image(pred_both, caption=f\"{i} Both\"),\n",
    "                wandb.Image(pred_seg, caption=f\"{i} Seg Only\"),\n",
    "                wandb.Image(pred_bbox, caption=f\"{i} BBox Only\")\n",
    "            ])\n",
    "        \n",
    "        tracker = accelerator.get_tracker(\"wandb\")\n",
    "        if tracker: tracker.log({\"validation\": log_images}, step=step)\n",
    "        \n",
    "        del pipeline, multi_controlnet\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping validation log due to error: {e}\")\n",
    "\n",
    "# --- MAIN FUNCTION ---\n",
    "def main():\n",
    "    setup_coco_dataset()\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=Config.GRAD_ACCUM_STEPS,\n",
    "        mixed_precision=\"bf16\",\n",
    "        log_with=\"wandb\",\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        wandb_key = os.getenv(\"wandb\")\n",
    "        if wandb_key: wandb.login(key=wandb_key)\n",
    "        cfg_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith(\"__\")}\n",
    "        accelerator.init_trackers(\"controlnet-coco-multi-h100\", config=cfg_dict)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, subfolder=\"tokenizer\", use_fast=False)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_ID, subfolder=\"scheduler\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "    vae = AutoencoderKL.from_pretrained(Config.MODEL_ID, subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_ID, subfolder=\"unet\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    \n",
    "    # Initialize Models\n",
    "    if Config.PRETRAINED_SEG_PATH:\n",
    "        controlnet_seg = ControlNetModel.from_pretrained(Config.PRETRAINED_SEG_PATH)\n",
    "    else:\n",
    "        controlnet_seg = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    if Config.PRETRAINED_BBOX_PATH:\n",
    "        controlnet_bbox = ControlNetModel.from_pretrained(Config.PRETRAINED_BBOX_PATH)\n",
    "    else:\n",
    "        controlnet_bbox = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    controlnet_seg.train()\n",
    "    controlnet_bbox.train()\n",
    "    controlnet_seg.enable_gradient_checkpointing()\n",
    "    controlnet_bbox.enable_gradient_checkpointing()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "    dataset = COCOMultiDataset(Config.TRAIN_IMG_DIR, Config.TRAIN_ANN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    val_batch = next(iter(train_dataloader))\n",
    "    params = list(controlnet_seg.parameters()) + list(controlnet_bbox.parameters())\n",
    "    optimizer = bnb.optim.AdamW8bit(params, lr=Config.LEARNING_RATE)\n",
    "\n",
    "    controlnet_seg, controlnet_bbox, optimizer, train_dataloader = accelerator.prepare(\n",
    "        controlnet_seg, controlnet_bbox, optimizer, train_dataloader\n",
    "    )\n",
    "    \n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    unet.to(accelerator.device)\n",
    "\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if Config.RESUME_FROM_CHECKPOINT == \"latest\" and os.path.exists(Config.OUTPUT_DIR):\n",
    "        dirs = [d for d in os.listdir(Config.OUTPUT_DIR) if d.startswith(\"checkpoint\")]\n",
    "        if dirs:\n",
    "            path = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "            accelerator.print(f\"Resuming from {path}\")\n",
    "            accelerator.load_state(os.path.join(Config.OUTPUT_DIR, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "            start_epoch = global_step // len(train_dataloader)\n",
    "\n",
    "    if accelerator.is_main_process: print(f\"Starting training from Step {global_step}...\")\n",
    "    \n",
    "    for epoch in range(start_epoch, Config.NUM_EPOCHS):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            with accelerator.accumulate([controlnet_seg, controlnet_bbox]):\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.bfloat16)).latent_dist.sample() * vae.config.scaling_factor\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                \n",
    "                # --- SIMPLIFIED LOGIC ---\n",
    "                r = random.random()\n",
    "                \n",
    "                # Keep track of active residuals in lists\n",
    "                active_down = []\n",
    "                active_mid = []\n",
    "\n",
    "                # 1. SEGMENTATION\n",
    "                # Logic: Run if we are in \"Seg Only\" (<0.35) OR \"Both\" (>0.7)\n",
    "                if r < Config.PROB_SEG_ONLY or r >= (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY):\n",
    "                    real_seg = batch[\"seg_pixel_values\"].to(dtype=torch.bfloat16)\n",
    "                    d, m = controlnet_seg(\n",
    "                        noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states, \n",
    "                        controlnet_cond=real_seg, return_dict=False\n",
    "                    )\n",
    "                    active_down.append(d)\n",
    "                    active_mid.append(m)\n",
    "\n",
    "                # 2. BBOX\n",
    "                # Logic: Run if we are in \"BBox Only\" (0.35 - 0.7) OR \"Both\" (>0.7)\n",
    "                if (r >= Config.PROB_SEG_ONLY and r < (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY)) or r >= (Config.PROB_SEG_ONLY + Config.PROB_BBOX_ONLY):\n",
    "                    real_bbox = batch[\"bbox_pixel_values\"].to(dtype=torch.bfloat16)\n",
    "                    d, m = controlnet_bbox(\n",
    "                        noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states, \n",
    "                        controlnet_cond=real_bbox, return_dict=False\n",
    "                    )\n",
    "                    active_down.append(d)\n",
    "                    active_mid.append(m)\n",
    "\n",
    "                # --- AGGREGATION (NO \"IF TENSOR\" CHECKS) ---\n",
    "                if len(active_down) == 0:\n",
    "                    # Both dropped (shouldn't happen with current probs, but safe)\n",
    "                    down_block_res = None\n",
    "                    mid_block_res = None\n",
    "                else:\n",
    "                    # Sum up whatever is in the list\n",
    "                    down_block_res = active_down[0]\n",
    "                    mid_block_res = active_mid[0]\n",
    "                    \n",
    "                    for i in range(1, len(active_down)):\n",
    "                        down_block_res = [a + b for a, b in zip(down_block_res, active_down[i])]\n",
    "                        mid_block_res = mid_block_res + active_mid[i]\n",
    "                \n",
    "                # CAST TO BF16\n",
    "                if down_block_res is not None:\n",
    "                    down_block_res = [res.to(dtype=torch.bfloat16) for res in down_block_res]\n",
    "                    mid_block_res = mid_block_res.to(dtype=torch.bfloat16)\n",
    "\n",
    "                # UNET FORWARD\n",
    "                model_pred = unet(\n",
    "                    noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states,\n",
    "                    down_block_additional_residuals=down_block_res,\n",
    "                    mid_block_additional_residual=mid_block_res,\n",
    "                ).sample\n",
    "                \n",
    "                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                global_step += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if accelerator.is_main_process:\n",
    "                    accelerator.log({\"train_loss\": loss.item()}, step=global_step)\n",
    "                    if global_step % Config.LOG_INTERVAL == 0:\n",
    "                        u_seg = accelerator.unwrap_model(controlnet_seg)\n",
    "                        u_bbox = accelerator.unwrap_model(controlnet_bbox)\n",
    "                        log_validation(accelerator, u_seg, u_bbox, unet, vae, text_encoder, tokenizer, val_batch, global_step)\n",
    "                    if global_step % Config.SAVE_INTERVAL == 0:\n",
    "                        save_path = os.path.join(Config.OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        try:\n",
    "                            checkpoints = sorted([d for d in os.listdir(Config.OUTPUT_DIR) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split(\"-\")[1]))\n",
    "                            if len(checkpoints) > 2:\n",
    "                                for ckpt in checkpoints[:-2]: shutil.rmtree(os.path.join(Config.OUTPUT_DIR, ckpt))\n",
    "                        except Exception: pass\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.unwrap_model(controlnet_seg).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_seg\"))\n",
    "        accelerator.unwrap_model(controlnet_bbox).save_pretrained(os.path.join(Config.OUTPUT_DIR, \"final_bbox\"))\n",
    "        accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/bin/accelerate\", line 3, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/accelerate/accelerator.py\", line 31, in <module>\n",
      "    import torch\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/__init__.py\", line 416, in <module>\n",
      "    from torch._C import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --mixed_precision=bf16 train_multi_controlnet1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload_hf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload_hf.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from diffusers import ControlNetModel, UNet2DConditionModel\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "HF_USERNAME = \"ritishshrirao\"\n",
    "REPO_NAME = \"controlnet-coco-multi\"\n",
    "MODEL_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "# Path where your training checkpoints are\n",
    "LOCAL_OUTPUT_DIR = \"/teamspace/studios/this_studio/controlnet-coco-multi-h100-new\"\n",
    "BASE_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "def get_latest_checkpoint(base_path):\n",
    "    if not os.path.exists(base_path): return None\n",
    "    checkpoints = [d for d in os.listdir(base_path) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints: return None\n",
    "    # Sort by integer step\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    return os.path.join(base_path, checkpoints[-1])\n",
    "\n",
    "def load_weights_to_model(model, bin_path):\n",
    "    print(f\"Loading weights from {bin_path}...\")\n",
    "    # Load state dict (map_location='cpu' to save VRAM)\n",
    "    state_dict = torch.load(bin_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Fix keys if they were saved with 'module.' prefix (DistributedDataParallel)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "            \n",
    "    # Load into model\n",
    "    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "    if len(missing) > 0:\n",
    "        print(f\"âš ï¸ Warning: Missing keys: {len(missing)}\")\n",
    "    print(\"Weights loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "def extract_and_upload():\n",
    "    # 1. Setup Auth\n",
    "    hf_token = os.getenv(\"HF\")\n",
    "    if not hf_token:\n",
    "        print(\"âŒ Error: 'HF' environment variable not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Find Checkpoint\n",
    "    latest_ckpt_path = get_latest_checkpoint(LOCAL_OUTPUT_DIR)\n",
    "    if not latest_ckpt_path:\n",
    "        print(\"âŒ No checkpoints found.\")\n",
    "        return\n",
    "    print(f\"ðŸ“‚ Processing Checkpoint: {latest_ckpt_path}\")\n",
    "\n",
    "    # 3. Identify Weight Files\n",
    "    # In accelerate, the order passed to prepare() determines the index.\n",
    "    # Script order: prepare(controlnet_seg, controlnet_bbox, ...)\n",
    "    seg_weights_path = os.path.join(latest_ckpt_path, \"pytorch_model_0.bin\")\n",
    "    bbox_weights_path = os.path.join(latest_ckpt_path, \"pytorch_model_1.bin\")\n",
    "\n",
    "    if not os.path.exists(seg_weights_path) or not os.path.exists(bbox_weights_path):\n",
    "        print(f\"âŒ Could not find model bins in {latest_ckpt_path}. Expected pytorch_model_0.bin and pytorch_model_1.bin\")\n",
    "        return\n",
    "\n",
    "    # 4. Initialize Clean Models\n",
    "    print(\"ðŸ—ï¸  Initializing model architecture...\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_ID, subfolder=\"unet\")\n",
    "    \n",
    "    # Create empty ControlNets\n",
    "    c_seg = ControlNetModel.from_unet(unet)\n",
    "    c_bbox = ControlNetModel.from_unet(unet)\n",
    "    \n",
    "    # 5. Load Weights\n",
    "    c_seg = load_weights_to_model(c_seg, seg_weights_path)\n",
    "    c_bbox = load_weights_to_model(c_bbox, bbox_weights_path)\n",
    "\n",
    "    # 6. Save Clean Versions (Safetensors)\n",
    "    export_dir = \"temp_clean_models\"\n",
    "    if os.path.exists(export_dir): shutil.rmtree(export_dir)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    print(\"ðŸ’¾ Saving clean safetensors models...\")\n",
    "    c_seg.save_pretrained(os.path.join(export_dir, \"segmentation\"), safe_serialization=True)\n",
    "    c_bbox.save_pretrained(os.path.join(export_dir, \"bbox\"), safe_serialization=True)\n",
    "\n",
    "    # 7. Upload to Hugging Face\n",
    "    api = HfApi(token=hf_token)\n",
    "    \n",
    "    print(f\"ðŸŒ Creating/Checking Repo: {MODEL_ID}\")\n",
    "    try:\n",
    "        create_repo(repo_id=MODEL_ID, exist_ok=True, private=False, token=hf_token)\n",
    "    except Exception as e:\n",
    "        print(f\"Repo info: {e}\")\n",
    "\n",
    "    print(\"ðŸš€ Uploading to Hugging Face Hub...\")\n",
    "    try:\n",
    "        # Upload Segmentation\n",
    "        print(\"... Uploading Segmentation ControlNet\")\n",
    "        api.upload_folder(\n",
    "            folder_path=os.path.join(export_dir, \"segmentation\"),\n",
    "            repo_id=MODEL_ID,\n",
    "            path_in_repo=\"segmentation\",\n",
    "            commit_message=\"Upload clean segmentation weights\"\n",
    "        )\n",
    "        \n",
    "        # Upload BBox\n",
    "        print(\"... Uploading BBox ControlNet\")\n",
    "        api.upload_folder(\n",
    "            folder_path=os.path.join(export_dir, \"bbox\"),\n",
    "            repo_id=MODEL_ID,\n",
    "            path_in_repo=\"bbox\",\n",
    "            commit_message=\"Upload clean bbox weights\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… Success! Models available at:\")\n",
    "        print(f\"ðŸ‘‰ https://huggingface.co/{MODEL_ID}/tree/main\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Upload Failed: {e}\")\n",
    "    finally:\n",
    "        # Cleanup temp folder\n",
    "        if os.path.exists(export_dir):\n",
    "            shutil.rmtree(export_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Processing Checkpoint: /teamspace/studios/this_studio/controlnet-coco-multi-h100-new/checkpoint-3000\n",
      "âŒ Could not find model bins in /teamspace/studios/this_studio/controlnet-coco-multi-h100-new/checkpoint-3000. Expected pytorch_model_0.bin and pytorch_model_1.bin\n"
     ]
    }
   ],
   "source": [
    "!python upload_hf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.safetensors  model_1.safetensors\toptimizer.bin  random_states_0.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls /teamspace/studios/this_studio/controlnet-coco-multi-h100-new/checkpoint-3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
