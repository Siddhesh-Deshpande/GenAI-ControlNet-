{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62138dd9",
   "metadata": {},
   "source": [
    "# ControlNet Inference on Training Data\n",
    "\n",
    "Test your trained ControlNet model on training data to verify it learned properly.\n",
    "\n",
    "**Files used:**\n",
    "- `config.json` - ControlNet model configuration\n",
    "- `diffusion_pytorch_model.safetensors` - Trained ControlNet weights\n",
    "- `train_caption.json` - Training captions and image associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130be708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Diffusers and transformers\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832942bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and Extract Weights-ControlNet Folder\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the zip file\n",
    "ZIP_PATH = Path('/content/drive/My Drive/Weights-ControlNet.zip')\n",
    "\n",
    "# Extract location\n",
    "EXTRACT_PATH = Path('/content/weights_extracted')\n",
    "WEIGHTS_FOLDER = EXTRACT_PATH / \"Weights-ControlNet\"\n",
    "\n",
    "# Check if zip file exists\n",
    "if ZIP_PATH.exists():\n",
    "    print(f\"✓ Found zip file: {ZIP_PATH}\")\n",
    "    print(f\"  File size: {ZIP_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Extract the zip file\n",
    "    print(\"\\nExtracting Weights-ControlNet.zip...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_PATH)\n",
    "    print(\"✓ Extraction complete!\")\n",
    "else:\n",
    "    print(f\"❌ Zip file not found at: {ZIP_PATH}\")\n",
    "    print(\"   Make sure 'Weights-ControlNet.zip' exists in your Google Drive root\")\n",
    "\n",
    "# Look for files in the extracted folder\n",
    "CONFIG_PATH = WEIGHTS_FOLDER / \"config.json\"\n",
    "WEIGHTS_PATH = WEIGHTS_FOLDER / \"diffusion_pytorch_model.safetensors\"\n",
    "CAPTIONS_PATH = WEIGHTS_FOLDER / \"train_caption.json\"\n",
    "\n",
    "# Create COCO data directory in Colab runtime\n",
    "BASE_DIR = Path('/content')\n",
    "COCO_DATA_DIR = BASE_DIR / \"coco_data\"\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"\\n✓ Device: {DEVICE}\")\n",
    "print(f\"✓ Data type: {DTYPE}\")\n",
    "print(f\"✓ Extracted folder: {WEIGHTS_FOLDER}\")\n",
    "print(f\"\\nFile paths:\")\n",
    "print(f\"  Config: {CONFIG_PATH} (exists: {CONFIG_PATH.exists()})\")\n",
    "print(f\"  Weights: {WEIGHTS_PATH} (exists: {WEIGHTS_PATH.exists()})\")\n",
    "print(f\"  Captions: {CAPTIONS_PATH} (exists: {CAPTIONS_PATH.exists()})\")\n",
    "\n",
    "# Verify all files exist\n",
    "if not all([CONFIG_PATH.exists(), WEIGHTS_PATH.exists(), CAPTIONS_PATH.exists()]):\n",
    "    print(\"\\n⚠️ WARNING: Not all required files found!\")\n",
    "    print(f\"Expected files in: {WEIGHTS_FOLDER}\")\n",
    "    \n",
    "    # List what's in the extracted folder\n",
    "    print(f\"\\nContents of {WEIGHTS_FOLDER}:\")\n",
    "    if WEIGHTS_FOLDER.exists():\n",
    "        try:\n",
    "            for item in WEIGHTS_FOLDER.iterdir():\n",
    "                size_info = \"\"\n",
    "                if item.is_file():\n",
    "                    size_info = f\" ({item.stat().st_size / (1024**2):.2f} MB)\"\n",
    "                print(f\"  - {item.name}{size_info}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error listing contents: {e}\")\n",
    "    else:\n",
    "        print(f\"  Folder not found!\")\n",
    "        print(f\"\\nContents of {EXTRACT_PATH}:\")\n",
    "        try:\n",
    "            for item in EXTRACT_PATH.iterdir():\n",
    "                print(f\"  - {item.name}\")\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    print(\"\\n✓ All required files found! Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Configuration File\n",
    "print(\"Loading configuration...\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"✓ Config loaded\")\n",
    "print(f\"  Conditioning channels: {config.get('conditioning_channels', 'N/A')}\")\n",
    "print(f\"  Cross attention dim: {config.get('cross_attention_dim', 'N/A')}\")\n",
    "print(f\"  Block out channels: {config.get('block_out_channels', 'N/A')}\")\n",
    "print(f\"  Diffusers version: {config.get('_diffusers_version', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Captions\n",
    "print(\"Loading training captions...\")\n",
    "with open(CAPTIONS_PATH, 'r') as f:\n",
    "    train_captions = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(train_captions)} training captions\")\n",
    "\n",
    "# Show sample captions\n",
    "print(\"\\nSample captions:\")\n",
    "for i, (img_name, caption) in enumerate(list(train_captions.items())[:3]):\n",
    "    print(f\"\\n{i+1}. Image: {img_name}\")\n",
    "    print(f\"   Caption (first 150 chars): {caption[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained ControlNet Weights\n",
    "print(\"Loading trained ControlNet model...\")\n",
    "\n",
    "# Verify file size\n",
    "import os\n",
    "weights_size_mb = os.path.getsize(WEIGHTS_PATH) / (1024**2)\n",
    "print(f\"Weights file size: {weights_size_mb:.2f} MB\")\n",
    "\n",
    "if weights_size_mb < 100:\n",
    "    print(\"\\n⚠️ WARNING: File size is suspiciously small!\")\n",
    "    print(\"   The safetensors file may be corrupted or incomplete during upload.\")\n",
    "    print(\"   Try uploading again or using a different method.\")\n",
    "\n",
    "# Try to load weights from safetensors\n",
    "try:\n",
    "    state_dict = load_file(str(WEIGHTS_PATH))\n",
    "    print(f\"✓ Loaded {len(state_dict)} weight tensors\")\n",
    "    print(f\"  Sample keys: {list(state_dict.keys())[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading safetensors: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Re-upload the diffusion_pytorch_model.safetensors file\")\n",
    "    print(\"2. Make sure the file uploaded completely (check file size)\")\n",
    "    print(\"3. Ensure the file is not corrupted\")\n",
    "    print(\"\\nFor now, let's verify the file integrity...\")\n",
    "    \n",
    "    # Check file header\n",
    "    try:\n",
    "        with open(WEIGHTS_PATH, 'rb') as f:\n",
    "            header = f.read(100)\n",
    "            print(f\"File starts with: {header[:50]}\")\n",
    "    except:\n",
    "        print(\"Cannot read file - it may be completely corrupted\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "# Load ControlNet from config\n",
    "controlnet = ControlNetModel.from_config(config)\n",
    "print(\"✓ ControlNet model created from config\")\n",
    "\n",
    "# Load the trained weights into ControlNet\n",
    "controlnet.load_state_dict(state_dict)\n",
    "print(\"✓ Trained weights loaded into ControlNet\")\n",
    "\n",
    "# Move to device\n",
    "controlnet = controlnet.to(DEVICE, dtype=DTYPE)\n",
    "controlnet.eval()\n",
    "print(f\"✓ ControlNet moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380df305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Diffusion Pipeline\n",
    "print(\"Initializing Stable Diffusion + ControlNet pipeline...\")\n",
    "\n",
    "# Use the trained ControlNet with Stable Diffusion v1.5\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=DTYPE,\n",
    "    safety_checker=None,  # Disable safety checker for faster inference\n",
    ")\n",
    "pipe = pipe.to(DEVICE)\n",
    "\n",
    "print(\"✓ Pipeline initialized\")\n",
    "\n",
    "# Enable memory-efficient attention if xformers is available\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"✓ XFormers memory efficient attention enabled\")\n",
    "except:\n",
    "    print(\"⚠️  XFormers not available (this is okay, just uses more memory)\")\n",
    "\n",
    "print(\"\\n✓ Ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8434118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create pose skeleton from COCO keypoints\n",
    "def create_pose_skeleton(keypoints, width, height):\n",
    "    \"\"\"Create a human pose skeleton from keypoint array\"\"\"\n",
    "    pose_img = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # COCO skeleton connections\n",
    "    skeleton = [\n",
    "        (0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6),\n",
    "        (5, 7), (7, 9), (6, 8), (8, 10), (5, 11), (6, 12),\n",
    "        (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)\n",
    "    ]\n",
    "    \n",
    "    line_thickness = max(2, int(min(width, height) / 100))\n",
    "    circle_radius = max(3, int(min(width, height) / 80))\n",
    "    \n",
    "    # Draw bones\n",
    "    for start_idx, end_idx in skeleton:\n",
    "        if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
    "            x1, y1, v1 = keypoints[start_idx]\n",
    "            x2, y2, v2 = keypoints[end_idx]\n",
    "            \n",
    "            if v1 > 0 and v2 > 0:\n",
    "                cv2.line(pose_img, (int(x1), int(y1)), (int(x2), int(y2)), \n",
    "                        255, line_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for x, y, v in keypoints:\n",
    "        if v > 0:\n",
    "            cv2.circle(pose_img, (int(x), int(y)), circle_radius, 255, -1)\n",
    "    \n",
    "    return pose_img\n",
    "\n",
    "print(\"✓ Helper function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Load COCO dataset\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "print(\"Setting up COCO dataset...\")\n",
    "\n",
    "# Create COCO data directory if it doesn't exist\n",
    "COCO_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ann_dir = COCO_DATA_DIR / \"annotations\"\n",
    "train_img_dir = COCO_DATA_DIR / \"train2017\"\n",
    "\n",
    "ann_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download COCO annotations if not present\n",
    "ann_file = ann_dir / \"person_keypoints_train2017.json\"\n",
    "\n",
    "if not ann_file.exists():\n",
    "    print(\"Downloading COCO 2017 pose annotations (~252MB)...\")\n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    zip_path = COCO_DATA_DIR / \"annotations.zip\"\n",
    "    \n",
    "    response = requests.get(ann_url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(zip_path, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading annotations\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(\"Extracting annotations...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(COCO_DATA_DIR)\n",
    "    \n",
    "    zip_path.unlink()\n",
    "    print(\"✓ Annotations downloaded and extracted\")\n",
    "else:\n",
    "    print(f\"✓ Annotations already exist at {ann_file}\")\n",
    "\n",
    "# Load COCO annotations\n",
    "print(\"Loading COCO annotations...\")\n",
    "coco = COCO(str(ann_file))\n",
    "print(f\"✓ COCO annotations loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download COCO image\n",
    "def download_coco_image(img_info, img_dir):\n",
    "    \"\"\"Download a single COCO image if not already present\"\"\"\n",
    "    img_path = img_dir / img_info['file_name']\n",
    "    \n",
    "    if img_path.exists():\n",
    "        return img_path\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(img_info['coco_url'], timeout=10)\n",
    "        img_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return img_path\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Failed to download {img_info['file_name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate images for training data samples\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING IMAGES FOR TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select random training samples to test\n",
    "num_samples = 5\n",
    "sample_keys = list(train_captions.keys())[:num_samples]\n",
    "\n",
    "print(f\"\\nGenerating images for {len(sample_keys)} training samples...\")\n",
    "\n",
    "for sample_idx, img_filename in enumerate(sample_keys):\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"Sample {sample_idx + 1}/{len(sample_keys)}: {img_filename}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Get caption\n",
    "    caption = train_captions[img_filename]\n",
    "    print(f\"Caption: {caption[:100]}...\")\n",
    "    \n",
    "    # Try to get pose from COCO\n",
    "    try:\n",
    "        # Extract image ID from filename\n",
    "        img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n",
    "        \n",
    "        # Get image info and pose\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_width, img_height = img_info['width'], img_info['height']\n",
    "        \n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Get pose keypoints\n",
    "        keypoints = None\n",
    "        for ann in anns:\n",
    "            if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                break\n",
    "        \n",
    "        if keypoints is None:\n",
    "            print(\"⚠️  No pose keypoints found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Create pose skeleton\n",
    "        pose_map = create_pose_skeleton(keypoints, img_width, img_height)\n",
    "        pose_map_pil = Image.fromarray(pose_map)\n",
    "        pose_map_pil = pose_map_pil.resize((512, 512))\n",
    "        \n",
    "        # Convert pose to tensor\n",
    "        pose_tensor = transforms.ToTensor()(pose_map_pil).unsqueeze(0).to(DEVICE, dtype=DTYPE)\n",
    "        \n",
    "        print(f\"✓ Pose skeleton created: {pose_tensor.shape}\")\n",
    "        \n",
    "        # Generate image with ControlNet\n",
    "        print(\"Generating image...\")\n",
    "        with torch.no_grad():\n",
    "            generator = torch.Generator(device=DEVICE).manual_seed(42)\n",
    "            output = pipe(\n",
    "                prompt=caption,\n",
    "                image=pose_tensor,\n",
    "                num_inference_steps=30,\n",
    "                generator=generator,\n",
    "                guidance_scale=7.5,\n",
    "            ).images[0]\n",
    "        \n",
    "        print(\"✓ Image generated successfully!\")\n",
    "        \n",
    "        # Download and load original image\n",
    "        print(\"Downloading training image...\")\n",
    "        img_path = download_coco_image(img_info, train_img_dir)\n",
    "        \n",
    "        if img_path:\n",
    "            original_img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Display results\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            \n",
    "            # Original image\n",
    "            axes[0].imshow(original_img)\n",
    "            axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Pose skeleton\n",
    "            axes[1].imshow(pose_map, cmap='gray')\n",
    "            axes[1].set_title('Pose Skeleton', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # Generated image\n",
    "            axes[2].imshow(output)\n",
    "            axes[2].set_title('Generated Image\\n(with trained ControlNet)', fontsize=12, fontweight='bold')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            axes[3].imshow(original_img)\n",
    "            axes[3].imshow(pose_map, cmap='hot', alpha=0.3)\n",
    "            axes[3].set_title('Original + Pose', fontsize=12, fontweight='bold')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            plt.suptitle(f'Training Sample: {img_filename}\\n\\\"{caption[:80]}...\\\"', \n",
    "                        fontsize=14, fontweight='bold', y=1.00)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Just show pose and generated image\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            axes[0].imshow(pose_map, cmap='gray')\n",
    "            axes[0].set_title('Pose Skeleton', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(output)\n",
    "            axes[1].set_title('Generated Image', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.suptitle(f'{img_filename}\\n\\\"{caption[:80]}...\\\"', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing sample: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference on multiple training samples\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH INFERENCE ON TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate images for a batch of training samples\n",
    "batch_size = 3\n",
    "num_batches = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    print(f\"\\nBatch {batch_idx + 1}/{num_batches}\")\n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    batch_keys = sample_keys[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    batch_results = []\n",
    "    \n",
    "    for img_filename in batch_keys:\n",
    "        caption = train_captions[img_filename]\n",
    "        \n",
    "        try:\n",
    "            # Get pose from COCO\n",
    "            img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n",
    "            img_info = coco.loadImgs(img_id)[0]\n",
    "            img_width, img_height = img_info['width'], img_info['height']\n",
    "            \n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id, catIds=coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            keypoints = None\n",
    "            for ann in anns:\n",
    "                if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                    keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                    break\n",
    "            \n",
    "            if keypoints is None:\n",
    "                continue\n",
    "            \n",
    "            # Create pose skeleton\n",
    "            pose_map = create_pose_skeleton(keypoints, img_width, img_height)\n",
    "            pose_map_pil = Image.fromarray(pose_map).resize((512, 512))\n",
    "            pose_tensor = transforms.ToTensor()(pose_map_pil).unsqueeze(0).to(DEVICE, dtype=DTYPE)\n",
    "            \n",
    "            # Download original training image\n",
    "            print(f\"  Downloading {img_filename}...\")\n",
    "            img_path = download_coco_image(img_info, train_img_dir)\n",
    "            original_img = None\n",
    "            if img_path:\n",
    "                original_img = Image.open(img_path).convert('RGB')\n",
    "                original_img = original_img.resize((512, 512))\n",
    "            \n",
    "            # Generate image\n",
    "            with torch.no_grad():\n",
    "                generator = torch.Generator(device=DEVICE).manual_seed(42 + batch_idx)\n",
    "                output = pipe(\n",
    "                    prompt=caption,\n",
    "                    image=pose_tensor,\n",
    "                    num_inference_steps=30,\n",
    "                    generator=generator,\n",
    "                    guidance_scale=7.5,\n",
    "                ).images[0]\n",
    "            \n",
    "            batch_results.append({\n",
    "                'filename': img_filename,\n",
    "                'caption': caption,\n",
    "                'original': original_img,\n",
    "                'pose': pose_map,\n",
    "                'generated': output\n",
    "            })\n",
    "            \n",
    "            results.append(batch_results[-1])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  Skipped {img_filename}: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(results)} images successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2768ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch results\n",
    "if results:\n",
    "    print(\"Displaying batch results...\\n\")\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        if result['original'] is not None:\n",
    "            # 4-panel comparison: Original, Pose, Generated, Overlay\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            \n",
    "            # Original image\n",
    "            axes[0].imshow(result['original'])\n",
    "            axes[0].set_title('Original Training Image', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Pose skeleton\n",
    "            axes[1].imshow(result['pose'], cmap='gray')\n",
    "            axes[1].set_title('Pose Skeleton Conditioning', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # Generated image\n",
    "            axes[2].imshow(result['generated'])\n",
    "            axes[2].set_title('ControlNet Generated Image', fontsize=12, fontweight='bold')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Overlay: Original + Pose\n",
    "            axes[3].imshow(result['original'])\n",
    "            axes[3].imshow(result['pose'], cmap='hot', alpha=0.3)\n",
    "            axes[3].set_title('Original + Pose Overlay', fontsize=12, fontweight='bold')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            caption_short = result['caption'][:90]\n",
    "            plt.suptitle(f\"Result {idx+1}: {result['filename']}\\n\\\"{caption_short}...\\\"\", \n",
    "                        fontsize=13, fontweight='bold', y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            # 3-panel comparison: Pose, Generated, and info\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Pose skeleton\n",
    "            axes[0].imshow(result['pose'], cmap='gray')\n",
    "            axes[0].set_title('Pose Skeleton Conditioning', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Generated image\n",
    "            axes[1].imshow(result['generated'])\n",
    "            axes[1].set_title('ControlNet Generated Image', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            caption_short = result['caption'][:90]\n",
    "            plt.suptitle(f\"Result {idx+1}: {result['filename']}\\n\\\"{caption_short}...\\\"\", \n",
    "                        fontsize=13, fontweight='bold', y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"⚠️  No results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics and analysis\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal training samples tested: {len(results)}\")\n",
    "print(f\"Successful generations: {len(results)}\")\n",
    "print(f\"Success rate: {100.0 * len(results) / num_samples:.1f}%\")\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  - Base model: Stable Diffusion v1.5\")\n",
    "print(f\"  - ControlNet conditioning channels: {config.get('conditioning_channels')}\")\n",
    "print(f\"  - Cross-attention dimensions: {config.get('cross_attention_dim')}\")\n",
    "print(f\"  - Training data captions: {len(train_captions)}\")\n",
    "\n",
    "print(\"\\nInference Settings:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Dtype: {DTYPE}\")\n",
    "print(f\"  - Inference steps: 30\")\n",
    "print(f\"  - Guidance scale: 7.5\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Inference complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
