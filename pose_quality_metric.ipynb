{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f2e8c4",
   "metadata": {},
   "source": [
    "# COCO Pose Estimation Quality Metric\n",
    "\n",
    "**Evaluating ControlNet-Generated Poses using COCO Dataset Keypoints**\n",
    "\n",
    "This notebook compares poses from ControlNet-generated images against ground truth COCO keypoints using:\n",
    "1. **COCO API** to extract ground truth keypoints from dataset\n",
    "2. **MMPose** with RTMPose model to extract keypoints from generated images\n",
    "3. **OKS (Object Keypoint Similarity)** metric for comparison\n",
    "4. Additional metrics: mAP@OKS, PCK, PCKh\n",
    "\n",
    "**Approach:**\n",
    "- Load first 75 images from val_captions.json\n",
    "- Extract COCO ground truth keypoints\n",
    "- Run MMPose on generated images\n",
    "- Compare using COCO evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c5d09",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec510f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install basic dependencies first\n",
    "%pip install -q numpy pandas matplotlib seaborn scipy\n",
    "%pip install -q opencv-python\n",
    "%pip install -q pycocotools\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# For Python >= 3.11 (Colab), use OpenMMLab v2 stack with openmim\n",
    "print(\"Installing OpenMMLab v2 stack (compatible with Python 3.12 + Torch 2.x)...\")\n",
    "\n",
    "# Clean up any old installations\n",
    "print(\"Cleaning up previous installations...\")\n",
    "%pip uninstall -y mmcv mmcv-full mmengine mmdet mmpose openmim 2>/dev/null || true\n",
    "\n",
    "# Upgrade pip\n",
    "print(\"Upgrading pip...\")\n",
    "%pip install -U pip setuptools wheel\n",
    "\n",
    "# Install openmim (OpenMMLab package installer - handles platform-specific wheels)\n",
    "print(\"Installing openmim...\")\n",
    "%pip install -q -U openmim\n",
    "\n",
    "# Install mmengine (required base for all v2 packages)\n",
    "print(\"Installing mmengine...\")\n",
    "%pip install -q mmengine\n",
    "\n",
    "# Use mim to install mmcv (automatically finds compatible prebuilt wheels)\n",
    "print(\"Installing mmcv 2.1.0 via mim...\")\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['mim', 'install', 'mmcv==2.1.0'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Warning: mim install had issues: {result.stderr}\")\n",
    "    print(\"Trying direct pip install as fallback...\")\n",
    "    %pip install -q \"mmcv==2.1.0\"\n",
    "\n",
    "# Install MMDetection 3.3.0\n",
    "print(\"Installing MMDetection 3.3.0...\")\n",
    "%pip install -q \"mmdet==3.3.0\"\n",
    "\n",
    "# Install MMPose 1.3.2 dependencies first\n",
    "print(\"Installing MMPose dependencies...\")\n",
    "%pip install -q json_tricks munkres pillow\n",
    "\n",
    "# Install MMPose 1.3.2 with --no-deps to skip problematic builds\n",
    "print(\"Installing MMPose 1.3.2...\")\n",
    "%pip install -q --no-deps \"mmpose==1.3.2\"\n",
    "\n",
    "print(\"‚úì Installation complete (OpenMMLab v2)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import sys\n",
    "import torch\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Torch:   {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "try:\n",
    "    import mmengine\n",
    "    print(f\"MMEngine: {mmengine.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"MMEngine import failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "try:\n",
    "    import mmcv\n",
    "    print(f\"MMCV:     {mmcv.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"MMCV import failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "try:\n",
    "    import mmdet\n",
    "    print(f\"MMDet:    {mmdet.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"MMDet import failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "try:\n",
    "    import mmpose\n",
    "    print(f\"MMPose:   {mmpose.__version__}\")\n",
    "    print(\"‚úÖ All packages loaded successfully (v2 API)\")\n",
    "except Exception as e:\n",
    "    print(f\"MMPose import failed: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try v1 APIs first; fall back to v2 inferencers on ImportError\n",
    "USE_INFERENCER = False\n",
    "try:\n",
    "    from mmdet.apis import init_detector, inference_detector\n",
    "    from mmpose.apis import inference_top_down_pose_model, init_pose_model\n",
    "    print(\"‚úì Using OpenMMLab v1 APIs (init_* / inference_*).\")\n",
    "except Exception:\n",
    "    USE_INFERENCER = True\n",
    "    from mmdet.apis import DetInferencer\n",
    "    from mmpose.apis import MMPoseInferencer\n",
    "    print(\"‚úì Using OpenMMLab v2 Inferencers (DetInferencer / MMPoseInferencer).\")\n",
    "\n",
    "# Paths and model URLs\n",
    "VAL_CAPTIONS_PATH = \"./val_captions.json\"\n",
    "GENERATED_IMAGES_PATH = \"path/to/generated/images\"  # e.g., \"./generated_images\"\n",
    "NUM_IMAGES = 75\n",
    "\n",
    "# v1 configs/checkpoints (used when USE_INFERENCER=False)\n",
    "MMPOSE_CONFIG_URL = \"https://raw.githubusercontent.com/open-mmlab/mmpose/master/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py\"\n",
    "MMPOSE_CHECKPOINT_URL = \"https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\"\n",
    "\n",
    "DETECTOR_CONFIG_URL = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/v2.28.2/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py\"\n",
    "DETECTOR_CHECKPOINT_URL = \"https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\"\n",
    "DETECTOR_SCORE_THR = 0.4  # keep person boxes above this score\n",
    "\n",
    "# v2 Inferencer model aliases (used when USE_INFERENCER=True)\n",
    "# Pose model: top-down HRNet-W48 on COCO 256x192\n",
    "POSE2D_ALIAS = \"td-hm_hrnet-w48_8xb32-210e_coco-256x192\"\n",
    "# Detector model: Faster R-CNN R50-FPN on COCO\n",
    "DET_MODEL_ALIAS = \"mmdet::faster-rcnn_r50_fpn_1x_coco\"\n",
    "\n",
    "COCO_KEYPOINT_NAMES = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "SKELETON = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4), (3, 5), (4, 6),\n",
    "    (5, 7), (7, 9), (6, 8), (8, 10), (5, 11), (6, 12),\n",
    "    (11, 13), (13, 15), (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "print(\"‚úì Imports & config loaded\")\n",
    "print(f\"  Val Captions: {VAL_CAPTIONS_PATH}\")\n",
    "print(f\"  Generated Images: {GENERATED_IMAGES_PATH}\")\n",
    "print(f\"  API Mode: {'Inferencer(v2)' if USE_INFERENCER else 'Classic(v1)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77bfe8",
   "metadata": {},
   "source": [
    "## 4. Load COCO Dataset & Extract Ground Truth Keypoints (On-the-fly)\n",
    "\n",
    "No local COCO dataset needed! We'll fetch annotations directly from COCO servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load val_captions.json\n",
    "print(\"Loading val_captions.json...\")\n",
    "with open(VAL_CAPTIONS_PATH, 'r') as f:\n",
    "    val_captions_raw = json.load(f)\n",
    "\n",
    "# Handle different JSON structures\n",
    "image_ids = []\n",
    "\n",
    "if isinstance(val_captions_raw, dict):\n",
    "    # Check if it's a filename->caption dictionary (e.g., {\"000000480936.jpg\": \"caption text\"})\n",
    "    sample_key = next(iter(val_captions_raw.keys())) if val_captions_raw else None\n",
    "    \n",
    "    if sample_key and isinstance(sample_key, str) and sample_key.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        # Format: {\"filename.jpg\": \"caption\"}\n",
    "        print(\"Detected filename->caption dictionary format\")\n",
    "        \n",
    "        # Extract image IDs from filenames\n",
    "        for filename in list(val_captions_raw.keys())[:NUM_IMAGES]:\n",
    "            # Extract numeric ID from filename like \"000000480936.jpg\"\n",
    "            # Remove extension and any leading zeros\n",
    "            basename = filename.rsplit('.', 1)[0]  # \"000000480936\"\n",
    "            try:\n",
    "                img_id = int(basename)  # Convert to integer (removes leading zeros)\n",
    "                image_ids.append(img_id)\n",
    "            except ValueError:\n",
    "                print(f\"  Warning: Could not extract ID from filename: {filename}\")\n",
    "    \n",
    "    elif 'annotations' in val_captions_raw:\n",
    "        val_captions = val_captions_raw['annotations']\n",
    "        for cap in val_captions[:NUM_IMAGES]:\n",
    "            if isinstance(cap, dict):\n",
    "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
    "                if img_id and img_id not in image_ids:\n",
    "                    image_ids.append(img_id)\n",
    "    \n",
    "    elif 'images' in val_captions_raw:\n",
    "        val_captions = val_captions_raw['images']\n",
    "        for cap in val_captions[:NUM_IMAGES]:\n",
    "            if isinstance(cap, dict):\n",
    "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
    "                if img_id and img_id not in image_ids:\n",
    "                    image_ids.append(img_id)\n",
    "    \n",
    "    else:\n",
    "        # Try to use the values if they look like a list\n",
    "        vals = list(val_captions_raw.values())\n",
    "        if vals and isinstance(vals[0], dict):\n",
    "            for cap in vals[:NUM_IMAGES]:\n",
    "                img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
    "                if img_id and img_id not in image_ids:\n",
    "                    image_ids.append(img_id)\n",
    "\n",
    "elif isinstance(val_captions_raw, list):\n",
    "    for cap in val_captions_raw[:NUM_IMAGES]:\n",
    "        if isinstance(cap, dict):\n",
    "            img_id = cap.get('image_id') or cap.get('id') or cap.get('image')\n",
    "            if img_id and img_id not in image_ids:\n",
    "                image_ids.append(img_id)\n",
    "\n",
    "if not image_ids:\n",
    "    raise ValueError(f\"Could not extract image IDs from val_captions.json. Please check the file format.\")\n",
    "\n",
    "print(f\"Total captions: {len(val_captions_raw) if isinstance(val_captions_raw, dict) else len(val_captions_raw)}\")\n",
    "print(f\"Extracted {len(image_ids)} unique image IDs from first {NUM_IMAGES} entries\")\n",
    "print(f\"Sample image IDs: {image_ids[:5]}\")\n",
    "\n",
    "# ============================================\n",
    "# Download COCO annotations on-the-fly\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Downloading COCO annotations on-the-fly...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Download COCO keypoints annotation JSON\n",
    "print(\"Downloading COCO annotations (annotations_trainval2017.zip)...\")\n",
    "print(\"(This may take 2-3 minutes, file is ~250MB)\")\n",
    "\n",
    "try:\n",
    "    # Use a cache directory\n",
    "    cache_dir = Path.home() / \".cache\" / \"coco_annotations\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cache_file = cache_dir / \"person_keypoints_val2017.json\"\n",
    "    \n",
    "    if cache_file.exists():\n",
    "        print(f\"Using cached annotations from {cache_file}\")\n",
    "        with open(cache_file, 'r') as f:\n",
    "            coco_annotations = json.load(f)\n",
    "    else:\n",
    "        # Download the zip file containing all annotations\n",
    "        zip_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "        zip_path = cache_dir / \"annotations_trainval2017.zip\"\n",
    "        \n",
    "        print(f\"Downloading annotations zip file...\")\n",
    "        \n",
    "        # Download with progress\n",
    "        def download_progress(blocknum, blocksize, totalsize):\n",
    "            downloaded = blocknum * blocksize\n",
    "            percent = min(downloaded * 100 / totalsize, 100)\n",
    "            print(f\"  Progress: {percent:.1f}% ({downloaded / 1024 / 1024:.1f}MB / {totalsize / 1024 / 1024:.1f}MB)\", end='\\r')\n",
    "        \n",
    "        urllib.request.urlretrieve(zip_url, zip_path, reporthook=download_progress)\n",
    "        print(\"\\n‚úì Download complete!\")\n",
    "        \n",
    "        # Extract the specific JSON file we need\n",
    "        print(f\"Extracting person_keypoints_val2017.json...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Extract only the file we need\n",
    "            target_file = \"annotations/person_keypoints_val2017.json\"\n",
    "            zip_ref.extract(target_file, cache_dir)\n",
    "            \n",
    "            # Move to cache location\n",
    "            extracted_path = cache_dir / target_file\n",
    "            extracted_path.rename(cache_file)\n",
    "            \n",
    "            # Clean up\n",
    "            (cache_dir / \"annotations\").rmdir()\n",
    "        \n",
    "        # Remove zip file to save space\n",
    "        zip_path.unlink()\n",
    "        print(f\"‚úì Extracted to {cache_file}\")\n",
    "        \n",
    "        # Load the annotations\n",
    "        with open(cache_file, 'r') as f:\n",
    "            coco_annotations = json.load(f)\n",
    "    \n",
    "    print(\"‚úì COCO annotations loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error downloading COCO annotations: {e}\")\n",
    "    print(\"Make sure you have internet connection\")\n",
    "    raise\n",
    "\n",
    "# ============================================\n",
    "# Extract ground truth keypoints\n",
    "# ============================================\n",
    "print(\"\\nProcessing COCO annotations...\")\n",
    "\n",
    "# Build lookup: image_id -> annotations\n",
    "coco_images = {img['id']: img for img in coco_annotations.get('images', [])}\n",
    "coco_annotations_by_img = {}\n",
    "\n",
    "for ann in coco_annotations.get('annotations', []):\n",
    "    img_id = ann['image_id']\n",
    "    if img_id not in coco_annotations_by_img:\n",
    "        coco_annotations_by_img[img_id] = []\n",
    "    coco_annotations_by_img[img_id].append(ann)\n",
    "\n",
    "print(f\"Total images in COCO: {len(coco_images)}\")\n",
    "print(f\"Total annotations in COCO: {len(coco_annotations.get('annotations', []))}\")\n",
    "\n",
    "# Extract ground truth keypoints for our images\n",
    "gt_keypoints_dict = {}  # {image_id: {ann_id: keypoints}}\n",
    "\n",
    "print(f\"\\nExtracting ground truth keypoints for {len(image_ids)} images...\")\n",
    "for idx, img_id in enumerate(image_ids):\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"  [{idx+1}/{len(image_ids)}]\", end='\\r')\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    anns = coco_annotations_by_img.get(img_id, [])\n",
    "    \n",
    "    if anns:\n",
    "        gt_keypoints_dict[img_id] = {}\n",
    "        for ann in anns:\n",
    "            if 'keypoints' in ann:\n",
    "                # COCO format: [x1, y1, v1, x2, y2, v2, ...]\n",
    "                kpts = np.array(ann['keypoints']).reshape(17, 3)  # 17 keypoints, (x, y, v)\n",
    "                gt_keypoints_dict[img_id][ann['id']] = {\n",
    "                    'keypoints': kpts,\n",
    "                    'bbox': ann.get('bbox'),\n",
    "                    'area': ann.get('area'),\n",
    "                    'iscrowd': ann.get('iscrowd', 0),\n",
    "                    'category_id': ann.get('category_id')\n",
    "                }\n",
    "\n",
    "print(f\"\\n‚úì Found ground truth for {len(gt_keypoints_dict)} images\")\n",
    "total_anns = sum(len(v) for v in gt_keypoints_dict.values())\n",
    "print(f\"  Total annotations: {total_anns}\")\n",
    "print(f\"  Images without keypoint annotations: {len(image_ids) - len(gt_keypoints_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ce29c",
   "metadata": {},
   "source": [
    "## 5. Load MMPose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ac3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Loading models...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "try:\n",
    "    cache_dir = Path.home() / \".cache\" / \"mmpose_detector\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not USE_INFERENCER:\n",
    "        # -------------------\n",
    "        # v1: Download configs\n",
    "        # -------------------\n",
    "        detector_config_path = cache_dir / \"faster_rcnn_r50_fpn_1x_coco.py\"\n",
    "        if not detector_config_path.exists():\n",
    "            urllib.request.urlretrieve(DETECTOR_CONFIG_URL, detector_config_path)\n",
    "        pose_config_path = cache_dir / \"hrnet_w48_coco_256x192.py\"\n",
    "        if not pose_config_path.exists():\n",
    "            urllib.request.urlretrieve(MMPOSE_CONFIG_URL, pose_config_path)\n",
    "\n",
    "        # -------------------\n",
    "        # v1: Download checkpoints\n",
    "        # -------------------\n",
    "        detector_ckpt_path = cache_dir / \"faster_rcnn_r50_fpn_1x_coco.pth\"\n",
    "        if not detector_ckpt_path.exists():\n",
    "            urllib.request.urlretrieve(DETECTOR_CHECKPOINT_URL, detector_ckpt_path)\n",
    "        pose_ckpt_path = cache_dir / \"hrnet_w48_coco_256x192.pth\"\n",
    "        if not pose_ckpt_path.exists():\n",
    "            urllib.request.urlretrieve(MMPOSE_CHECKPOINT_URL, pose_ckpt_path)\n",
    "\n",
    "        # -------------------\n",
    "        # v1: Init detector + pose\n",
    "        # -------------------\n",
    "        detector_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        detector_model = init_detector(\n",
    "            str(detector_config_path),\n",
    "            str(detector_ckpt_path),\n",
    "            device=detector_device\n",
    "        )\n",
    "\n",
    "        pose_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        pose_model = init_pose_model(\n",
    "            str(pose_config_path),\n",
    "            str(pose_ckpt_path),\n",
    "            device=pose_device\n",
    "        )\n",
    "        print(\"‚úì v1 models loaded successfully\")\n",
    "        print(f\"  Detector device: {detector_device}\")\n",
    "        print(f\"  Pose device: {pose_device}\")\n",
    "    else:\n",
    "        # -------------------\n",
    "        # v2: Init inferencers\n",
    "        # -------------------\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        pose_inferencer = MMPoseInferencer(\n",
    "            pose2d=POSE2D_ALIAS,\n",
    "            det_model=DET_MODEL_ALIAS,\n",
    "            device=device\n",
    "        )\n",
    "        print(\"‚úì v2 inferencers initialized successfully\")\n",
    "        print(f\"  Device: {device}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading models/inferencers: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2df5a4",
   "metadata": {},
   "source": [
    "## 7. Extract Keypoints from Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_persons(image_path: str, detector_model, score_thr: float = DETECTOR_SCORE_THR):\n",
    "    \"\"\"Run person detector and return list of bboxes [x1, y1, x2, y2, score]. (v1 path)\"\"\"\n",
    "    det_results = inference_detector(detector_model, image_path)\n",
    "    # COCO class index 0 is person\n",
    "    person_dets = det_results[0] if isinstance(det_results, (list, tuple)) else det_results\n",
    "    if person_dets is None or len(person_dets) == 0:\n",
    "        return []\n",
    "    # Keep boxes above threshold\n",
    "    keep = []\n",
    "    for det in person_dets:\n",
    "        if det[4] >= score_thr:\n",
    "            keep.append(det)\n",
    "    return keep\n",
    "\n",
    "\n",
    "def _extract_keypoints_v1(image_path: str, detector_model, pose_model, score_thr: float = DETECTOR_SCORE_THR) -> List[np.ndarray]:\n",
    "    \"\"\"Detect persons (v1) then run pose on each detected bbox.\"\"\"\n",
    "    person_bboxes = detect_persons(image_path, detector_model, score_thr)\n",
    "    if len(person_bboxes) == 0:\n",
    "        return None\n",
    "\n",
    "    # Prepare person_results for top-down pose (xyxy + score)\n",
    "    person_results = []\n",
    "    for det in person_bboxes:\n",
    "        x1, y1, x2, y2, score = det\n",
    "        person_results.append({'bbox': np.array([x1, y1, x2, y2, score])})\n",
    "\n",
    "    pose_results, _ = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        image_path,\n",
    "        person_results,\n",
    "        bbox_thr=0.0,\n",
    "        format='xyxy',\n",
    "        dataset='TopDownCocoDataset',\n",
    "        return_heatmap=False\n",
    "    )\n",
    "\n",
    "    if pose_results is None or len(pose_results) == 0:\n",
    "        return None\n",
    "\n",
    "    detections = []\n",
    "    for pr in pose_results:\n",
    "        if 'keypoints' in pr:\n",
    "            kpts = pr['keypoints']\n",
    "            if kpts is not None and len(kpts) == 17:\n",
    "                detections.append(kpts)\n",
    "\n",
    "    return detections if len(detections) > 0 else None\n",
    "\n",
    "\n",
    "def _extract_keypoints_v2(image_path: str) -> List[np.ndarray]:\n",
    "    \"\"\"Use MMPoseInferencer (v2) to get keypoints for all detected persons.\"\"\"\n",
    "    global pose_inferencer\n",
    "    detections = []\n",
    "    # Inferencer returns a generator; take first result\n",
    "    gen = pose_inferencer(image_path, return_vis=False)\n",
    "    try:\n",
    "        result = next(gen)\n",
    "    except StopIteration:\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    preds = result.get('predictions', [])\n",
    "    for inst in preds:\n",
    "        # Expect 'keypoints' (17x2) and optional 'keypoint_scores' (17,)\n",
    "        if 'keypoints' in inst:\n",
    "            pts = np.array(inst['keypoints'])\n",
    "            if pts.ndim == 2 and pts.shape[0] == 17 and pts.shape[1] == 2:\n",
    "                scores = np.array(inst.get('keypoint_scores', np.ones(17)))\n",
    "                scores = scores.reshape(17, 1)\n",
    "                kpts = np.concatenate([pts, scores], axis=1)  # (17,3)\n",
    "                detections.append(kpts)\n",
    "    return detections if len(detections) > 0 else None\n",
    "\n",
    "\n",
    "def extract_keypoints_from_generated_image(image_path: str, detector_model=None, pose_model=None, score_thr: float = DETECTOR_SCORE_THR) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Unified extractor: if USE_INFERENCER=True (v2), use MMPoseInferencer.\n",
    "    Otherwise (v1), run detector -> pose.\n",
    "    Returns list of keypoint arrays (17,3) per detected person, or None.\n",
    "    \"\"\"\n",
    "    if USE_INFERENCER:\n",
    "        return _extract_keypoints_v2(image_path)\n",
    "    else:\n",
    "        return _extract_keypoints_v1(image_path, detector_model, pose_model, score_thr)\n",
    "\n",
    "\n",
    "def compute_oks(gt_keypoints: np.ndarray, pred_keypoints: np.ndarray, bbox: np.ndarray) -> float:\n",
    "    \"\"\"Compute Object Keypoint Similarity (OKS).\"\"\"\n",
    "    sigmas = np.array([\n",
    "        .26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89\n",
    "    ]) / 10.0\n",
    "    x, y, w, h = bbox\n",
    "    scale = w * h\n",
    "    if scale <= 0:\n",
    "        return 0.0\n",
    "    dx = pred_keypoints[:, 0] - gt_keypoints[:, 0]\n",
    "    dy = pred_keypoints[:, 1] - gt_keypoints[:, 1]\n",
    "    d_squared = dx**2 + dy**2\n",
    "    visible = gt_keypoints[:, 2] > 0\n",
    "    if visible.sum() == 0:\n",
    "        return 0.0\n",
    "    oks_per_kpt = np.exp(-d_squared / (2 * scale * sigmas**2))\n",
    "    oks = (oks_per_kpt * visible).sum() / visible.sum()\n",
    "    return float(oks)\n",
    "\n",
    "\n",
    "def compute_pck(gt_keypoints: np.ndarray, pred_keypoints: np.ndarray, threshold: float = 0.2) -> Dict:\n",
    "    \"\"\"Compute Percentage of Correct Keypoints (PCK).\"\"\"\n",
    "    visible_gt = gt_keypoints[gt_keypoints[:, 2] > 0]\n",
    "    if len(visible_gt) == 0:\n",
    "        return {'pck': 0.0, 'correct_keypoints': 0, 'visible_keypoints': 0}\n",
    "    x_min, y_min = visible_gt[:, 0].min(), visible_gt[:, 1].min()\n",
    "    x_max, y_max = visible_gt[:, 0].max(), visible_gt[:, 1].max()\n",
    "    bbox_diagonal = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2)\n",
    "    if bbox_diagonal == 0:\n",
    "        bbox_diagonal = 1.0\n",
    "    dx = pred_keypoints[:, 0] - gt_keypoints[:, 0]\n",
    "    dy = pred_keypoints[:, 1] - gt_keypoints[:, 1]\n",
    "    distances = np.sqrt(dx**2 + dy**2)\n",
    "    visible = gt_keypoints[:, 2] > 0\n",
    "    num_visible = visible.sum()\n",
    "    if num_visible == 0:\n",
    "        return {'pck': 0.0, 'correct_keypoints': 0, 'visible_keypoints': 0}\n",
    "    correct = (distances <= threshold * bbox_diagonal) & visible\n",
    "    num_correct = correct.sum()\n",
    "    pck = num_correct / num_visible\n",
    "    return {\n",
    "        'pck': float(pck),\n",
    "        'correct_keypoints': int(num_correct),\n",
    "        'visible_keypoints': int(num_visible)\n",
    "    }\n",
    "\n",
    "print(\"‚úì Helper functions defined (detector + pose, v1/v2 unified)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING KEYPOINTS FROM GENERATED IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load all generated images from folder\n",
    "gen_images_dir = Path(GENERATED_IMAGES_PATH)\n",
    "if not gen_images_dir.exists():\n",
    "    raise FileNotFoundError(f\"Generated images directory not found: {GENERATED_IMAGES_PATH}\")\n",
    "\n",
    "# Collect all image files (png/jpg/jpeg, case-insensitive)\n",
    "image_extensions = ['.png', '.jpg', '.jpeg']\n",
    "generated_image_files = []\n",
    "for ext in image_extensions:\n",
    "    generated_image_files.extend(gen_images_dir.glob(f'*{ext}'))\n",
    "    generated_image_files.extend(gen_images_dir.glob(f'*{ext.upper()}'))\n",
    "\n",
    "# Deduplicate\n",
    "generated_image_files = list(dict.fromkeys(generated_image_files))\n",
    "print(f\"Found {len(generated_image_files)} generated images in {GENERATED_IMAGES_PATH}\")\n",
    "print(f\"Need matches for {len(image_ids)} validation captions\")\n",
    "\n",
    "# Helper: find file whose name contains the image id (plain or zero-padded) and has prefix 'generated_'\n",
    "def find_generated_image_for_id(img_id: int) -> Path:\n",
    "    id_plain = str(img_id)\n",
    "    id_padded = f\"{img_id:012d}\"\n",
    "    candidates = []\n",
    "    for f in generated_image_files:\n",
    "        name = f.name\n",
    "        if not name.lower().startswith(\"generated_\"):\n",
    "            continue\n",
    "        if id_plain in name or id_padded in name:\n",
    "            candidates.append(f)\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # Prefer exact plain match, then padded, else first candidate\n",
    "    for f in candidates:\n",
    "        if f\"generated_{id_plain}\" in f.name:\n",
    "            return f\n",
    "    for f in candidates:\n",
    "        if f\"generated_{id_padded}\" in f.name:\n",
    "            return f\n",
    "    return sorted(candidates)[0]\n",
    "\n",
    "# Test extraction on first image to see full error\n",
    "print(\"\\nüîç Testing extraction on first image with detector -> pose ...\")\n",
    "test_img_id = image_ids[0]\n",
    "test_img_path = find_generated_image_for_id(test_img_id)\n",
    "if test_img_path:\n",
    "    print(f\"Test image: {test_img_path}\")\n",
    "    try:\n",
    "        test_kpts_list = extract_keypoints_from_generated_image(\n",
    "            str(test_img_path), detector_model, pose_model, score_thr=DETECTOR_SCORE_THR\n",
    "        )\n",
    "        if test_kpts_list:\n",
    "            print(f\"‚úì Extracted {len(test_kpts_list)} person(s) with keypoints; first shape: {test_kpts_list[0].shape}\")\n",
    "        else:\n",
    "            print(\"‚úó Extraction returned None or empty list\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Extraction error: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"‚úó Could not find test image for ID {test_img_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "generated_keypoints_dict = {}  # {image_id: [keypoints_array_per_person]}\n",
    "missing_images = []\n",
    "extraction_failures = []\n",
    "\n",
    "print(\"\\nMatching generated images by filename substring (image id) ...\\n\")\n",
    "\n",
    "for idx, img_id in enumerate(image_ids):\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"  [{idx+1}/{len(image_ids)}] Processed\", end='\\r')\n",
    "    \n",
    "    gen_img_path = find_generated_image_for_id(img_id)\n",
    "    \n",
    "    if gen_img_path is None:\n",
    "        missing_images.append(img_id)\n",
    "        continue\n",
    "    \n",
    "    # Extract keypoints (all detected persons)\n",
    "    try:\n",
    "        kpts_list = extract_keypoints_from_generated_image(\n",
    "            str(gen_img_path), detector_model, pose_model, score_thr=DETECTOR_SCORE_THR\n",
    "        )\n",
    "        \n",
    "        if kpts_list:\n",
    "            generated_keypoints_dict[img_id] = kpts_list\n",
    "        else:\n",
    "            extraction_failures.append((img_id, \"Extraction returned None or empty\"))\n",
    "    except Exception as e:\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)}\" if str(e) else type(e).__name__\n",
    "        extraction_failures.append((img_id, error_msg))\n",
    "\n",
    "print(f\"\\n\\n‚úì Processed {len(image_ids)} target images\")\n",
    "print(f\"  Successfully extracted (>=1 person): {len(generated_keypoints_dict)}\")\n",
    "print(f\"  Missing images: {len(missing_images)}\")\n",
    "print(f\"  Extraction failures: {len(extraction_failures)}\")\n",
    "\n",
    "if missing_images[:5]:\n",
    "    print(f\"\\n  First 5 missing image IDs: {missing_images[:5]}\")\n",
    "    print(f\"  ‚ö†Ô∏è Ensure filenames include the image id, e.g., generated_<id>.png\")\n",
    "\n",
    "if extraction_failures[:5]:\n",
    "    print(f\"\\n  First 5 extraction failures:\")\n",
    "    for img_id, reason in extraction_failures[:5]:\n",
    "        print(f\"    - Image {img_id}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6e4fe",
   "metadata": {},
   "source": [
    "## 8. Compute OKS & Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING OKS & COMPARISON METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Validate that we have both GT and predictions\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Images with GT keypoints: {len(gt_keypoints_dict)}\")\n",
    "print(f\"  Images with predicted keypoints (any person): {len(generated_keypoints_dict)}\")\n",
    "\n",
    "# Find intersection\n",
    "common_images = set(gt_keypoints_dict.keys()) & set(generated_keypoints_dict.keys())\n",
    "print(f\"  Images with both GT and predictions: {len(common_images)}\")\n",
    "\n",
    "if len(common_images) == 0:\n",
    "    print(\"\\n‚úó ERROR: No images have both GT and predicted keypoints!\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  1. Generated images are in the correct folder\")\n",
    "    print(\"  2. Image naming matches one of the patterns\")\n",
    "    print(\"  3. MMPose is correctly detecting poses\")\n",
    "    raise ValueError(\"No valid image pairs for comparison\")\n",
    "\n",
    "print(f\"\\n‚úì Ready to compare {len(common_images)} image-annotation pairs\")\n",
    "\n",
    "# Helper function to align keypoints by centroid\n",
    "def align_keypoints(gt_kpts, pred_kpts):\n",
    "    \"\"\"\n",
    "    Align predicted keypoints to ground truth by translating to same centroid.\n",
    "    This accounts for person being in different location in generated image.\n",
    "\n",
    "    Returns aligned predicted keypoints.\n",
    "    \"\"\"\n",
    "    # Get visible keypoints for centroid calculation\n",
    "    gt_visible = gt_kpts[gt_kpts[:, 2] > 0]\n",
    "    pred_visible = pred_kpts[pred_kpts[:, 2] > 0]\n",
    "\n",
    "    if len(gt_visible) == 0 or len(pred_visible) == 0:\n",
    "        return pred_kpts\n",
    "\n",
    "    # Compute centroids\n",
    "    gt_centroid = gt_visible[:, :2].mean(axis=0)\n",
    "    pred_centroid = pred_visible[:, :2].mean(axis=0)\n",
    "\n",
    "    # Compute translation\n",
    "    translation = gt_centroid - pred_centroid\n",
    "\n",
    "    # Apply translation to predicted keypoints\n",
    "    aligned_pred = pred_kpts.copy()\n",
    "    aligned_pred[:, 0] += translation[0]  # x\n",
    "    aligned_pred[:, 1] += translation[1]  # y\n",
    "\n",
    "    return aligned_pred\n",
    "\n",
    "results = []\n",
    "best_generated_keypoints_dict_aligned = {}  # {image_id: best aligned kpts (17,3)}\n",
    "best_generated_keypoints_dict_raw = {}       # {image_id: best raw kpts (17,3)}\n",
    "\n",
    "total_skipped_low_vis = 0\n",
    "comparison_count = 0\n",
    "\n",
    "print(\"\\nüìç Strategy: Compare ALL detected poses to GT, take MAX OKS\")\n",
    "print(\"   ALIGN keypoints by centroid (handles different person locations)\")\n",
    "print(\"   Using GT PERSON BBOX for OKS scale calculation\\n\")\n",
    "\n",
    "for img_id in common_images:\n",
    "    gt_data = gt_keypoints_dict[img_id]\n",
    "\n",
    "    if len(gt_data) == 0:\n",
    "        continue\n",
    "\n",
    "    ann_id = list(gt_data.keys())[0]\n",
    "    gt_info = gt_data[ann_id]\n",
    "    gt_kpts = gt_info['keypoints']  # (17, 3)\n",
    "    gt_bbox = gt_info['bbox']  # [x, y, width, height]\n",
    "\n",
    "    # Skip if labeled keypoints < 10\n",
    "    visible_count = int((gt_kpts[:, 2] > 0).sum())\n",
    "    if visible_count < 10:\n",
    "        total_skipped_low_vis += 1\n",
    "        continue\n",
    "\n",
    "    # Get predictions from generated image (detector + pose pipeline from Cell 7)\n",
    "    if img_id not in generated_keypoints_dict:\n",
    "        continue\n",
    "\n",
    "    pred_kpts_list = generated_keypoints_dict[img_id]\n",
    "\n",
    "    if not pred_kpts_list or len(pred_kpts_list) == 0:\n",
    "        continue\n",
    "\n",
    "    # ============================================================\n",
    "    # Compare ALL detected poses, take MAX OKS\n",
    "    # ALIGN keypoints before comparison (critical fix!)\n",
    "    # ============================================================\n",
    "    max_oks = 0.0\n",
    "    best_pred_aligned = None\n",
    "    best_pred_raw = None\n",
    "    best_pck_result = None\n",
    "\n",
    "    for pred_kpts in pred_kpts_list:\n",
    "        # ALIGN predicted keypoints to GT centroid\n",
    "        aligned_pred = align_keypoints(gt_kpts, pred_kpts)\n",
    "\n",
    "        # Compute OKS using GT bbox and ALIGNED keypoints\n",
    "        oks = compute_oks(gt_kpts, aligned_pred, gt_bbox)\n",
    "\n",
    "        # Keep track of best match\n",
    "        if oks > max_oks:\n",
    "            max_oks = oks\n",
    "            best_pred_aligned = aligned_pred\n",
    "            best_pred_raw = pred_kpts\n",
    "            best_pck_result = compute_pck(gt_kpts, aligned_pred)\n",
    "\n",
    "    if best_pred_aligned is None:\n",
    "        continue\n",
    "\n",
    "    results.append({\n",
    "        'image_id': img_id,\n",
    "        'annotation_id': ann_id,\n",
    "        'oks': max_oks,\n",
    "        'pck': best_pck_result['pck'],\n",
    "        'correct_keypoints': best_pck_result['correct_keypoints'],\n",
    "        'visible_keypoints': best_pck_result['visible_keypoints'],\n",
    "        'num_detected_poses': len(pred_kpts_list)\n",
    "    })\n",
    "\n",
    "    best_generated_keypoints_dict_aligned[img_id] = best_pred_aligned\n",
    "    best_generated_keypoints_dict_raw[img_id] = best_pred_raw\n",
    "    comparison_count += 1\n",
    "\n",
    "print(f\"\\nCompared {comparison_count} image-annotation pairs\")\n",
    "print(f\"Skipped (labeled keypoints < 10): {total_skipped_low_vis}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä POSE PRESERVATION RESULTS\")\n",
    "print(\"   (MAX OKS across all detected poses, with centroid alignment)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOKS (Object Keypoint Similarity):\")\n",
    "print(f\"  Mean:   {results_df['oks'].mean():.4f}\")\n",
    "print(f\"  Median: {results_df['oks'].median():.4f}\")\n",
    "print(f\"  Std:    {results_df['oks'].std():.4f}\")\n",
    "print(f\"  Min:    {results_df['oks'].min():.4f}\")\n",
    "print(f\"  Max:    {results_df['oks'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPCK (Percentage of Correct Keypoints @ 0.2 threshold):\")\n",
    "print(f\"  Mean:   {results_df['pck'].mean():.4f}\")\n",
    "print(f\"  Median: {results_df['pck'].median():.4f}\")\n",
    "print(f\"  Std:    {results_df['pck'].std():.4f}\")\n",
    "\n",
    "# mAP@OKS thresholds\n",
    "print(f\"\\nmAP@OKS Thresholds:\")\n",
    "for oks_threshold in [0.5, 0.75, 0.9]:\n",
    "    mAP = (results_df['oks'] >= oks_threshold).mean()\n",
    "    print(f\"  mAP@OKS={oks_threshold:.2f}: {mAP:.4f}\")\n",
    "\n",
    "# Detection statistics\n",
    "print(f\"\\nDetection Statistics:\")\n",
    "print(f\"  Avg detected poses per image: {results_df['num_detected_poses'].mean():.2f}\")\n",
    "print(f\"  Max detected poses: {results_df['num_detected_poses'].max()}\")\n",
    "print(f\"  Images with multiple poses: {(results_df['num_detected_poses'] > 1).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11502c",
   "metadata": {},
   "source": [
    "## 9. Visualize OKS Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efead197",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# OKS histogram\n",
    "axes[0, 0].hist(results_df['oks'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(results_df['oks'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {results_df[\"oks\"].mean():.3f}')\n",
    "axes[0, 0].axvline(results_df['oks'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {results_df[\"oks\"].median():.3f}')\n",
    "axes[0, 0].set_xlabel('OKS Score', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('OKS Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# PCK histogram\n",
    "axes[0, 1].hist(results_df['pck'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(results_df['pck'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {results_df[\"pck\"].mean():.3f}')\n",
    "axes[0, 1].set_xlabel('PCK Score', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('PCK Distribution (@ 0.2 threshold)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# mAP@OKS thresholds\n",
    "oks_thresholds = [0.5, 0.75, 0.9, 0.95]\n",
    "mAPs = [(results_df['oks'] >= t).mean() for t in oks_thresholds]\n",
    "axes[1, 0].bar([f'{t:.2f}' for t in oks_thresholds], mAPs, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('OKS Threshold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('mAP (Percentage)', fontsize=12)\n",
    "axes[1, 0].set_title('mAP @ Different OKS Thresholds', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(mAPs):\n",
    "    axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "axes[1, 1].boxplot([results_df['oks'], results_df['pck']], labels=['OKS', 'PCK'],\n",
    "                   patch_artist=True, widths=0.6)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 1].set_title('OKS vs PCK Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a4be3",
   "metadata": {},
   "source": [
    "## 10. Detailed Statistics by Keypoint Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08026f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEYPOINT-LEVEL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute per-keypoint distances using ALIGNED predictions (same as metrics)\n",
    "keypoint_errors = defaultdict(list)\n",
    "\n",
    "for img_id in best_generated_keypoints_dict_aligned.keys():\n",
    "    if img_id not in gt_keypoints_dict:\n",
    "        continue\n",
    "\n",
    "    pred_kpts = best_generated_keypoints_dict_aligned[img_id]\n",
    "    gt_data = gt_keypoints_dict[img_id]\n",
    "\n",
    "    if len(gt_data) == 0:\n",
    "        continue\n",
    "\n",
    "    ann_id = list(gt_data.keys())[0]\n",
    "    gt_kpts = gt_data[ann_id]['keypoints']\n",
    "\n",
    "    for kpt_idx in range(17):\n",
    "        if gt_kpts[kpt_idx, 2] > 0:  # Visible keypoint\n",
    "            dist = np.sqrt((pred_kpts[kpt_idx, 0] - gt_kpts[kpt_idx, 0])**2 +\n",
    "                          (pred_kpts[kpt_idx, 1] - gt_kpts[kpt_idx, 1])**2)\n",
    "            keypoint_errors[COCO_KEYPOINT_NAMES[kpt_idx]].append(dist)\n",
    "\n",
    "# Print per-keypoint statistics\n",
    "print(\"\\nPer-Keypoint Euclidean Distance Statistics:\")\n",
    "print(f\"{'Keypoint':<20} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "keypoint_stats = []\n",
    "for kpt_name in COCO_KEYPOINT_NAMES:\n",
    "    if kpt_name in keypoint_errors and len(keypoint_errors[kpt_name]) > 0:\n",
    "        errors = np.array(keypoint_errors[kpt_name])\n",
    "        mean_err = errors.mean()\n",
    "        std_err = errors.std()\n",
    "        min_err = errors.min()\n",
    "        max_err = errors.max()\n",
    "\n",
    "        print(f\"{kpt_name:<20} {mean_err:<10.2f} {std_err:<10.2f} {min_err:<10.2f} {max_err:<10.2f}\")\n",
    "\n",
    "        keypoint_stats.append({\n",
    "            'keypoint': kpt_name,\n",
    "            'mean_distance': mean_err,\n",
    "            'std_distance': std_err,\n",
    "            'min_distance': min_err,\n",
    "            'max_distance': max_err,\n",
    "            'num_samples': len(errors)\n",
    "        })\n",
    "\n",
    "keypoint_stats_df = pd.DataFrame(keypoint_stats)\n",
    "\n",
    "# Plot per-keypoint errors\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(keypoint_stats_df['keypoint'], keypoint_stats_df['mean_distance'],\n",
    "        color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.errorbar(keypoint_stats_df['keypoint'], keypoint_stats_df['mean_distance'],\n",
    "             yerr=keypoint_stats_df['std_distance'], fmt='none', color='red', capsize=5, alpha=0.5)\n",
    "plt.xlabel('Keypoint', fontsize=12)\n",
    "plt.ylabel('Mean Euclidean Distance (pixels)', fontsize=12)\n",
    "plt.title('Per-Keypoint Distance from Ground Truth (aligned predictions)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438b2f0",
   "metadata": {},
   "source": [
    "## 11. Visualization: Pose Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton(image, keypoints, skeleton, title=\"\"):\n",
    "    \"\"\"Draw skeleton on image.\"\"\"\n",
    "    image = image.copy()\n",
    "    # Draw keypoints\n",
    "    for kpt_idx, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:  # Only draw if confidence > 0.3\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(image, str(kpt_idx), (int(x) + 5, int(y) - 5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "    # Draw skeleton connections\n",
    "    for start, end in skeleton:\n",
    "        if keypoints[start, 2] > 0.3 and keypoints[end, 2] > 0.3:\n",
    "            x1, y1, _ = keypoints[start]\n",
    "            x2, y2, _ = keypoints[end]\n",
    "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def find_generated_image_for_id(img_id: int) -> Path:\n",
    "    \"\"\"Locate generated image by id in GENERATED_IMAGES_PATH.\"\"\"\n",
    "    gen_dir = Path(GENERATED_IMAGES_PATH)\n",
    "    if not gen_dir.exists():\n",
    "        return None\n",
    "    id_plain = str(img_id)\n",
    "    id_padded = f\"{img_id:012d}\"\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']\n",
    "    candidates = []\n",
    "    for ext in image_extensions:\n",
    "        for f in gen_dir.glob(f\"generated_*{ext}\"):\n",
    "            name = f.name\n",
    "            if id_plain in name or id_padded in name:\n",
    "                candidates.append(f)\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # Prefer exact plain match, then padded\n",
    "    for f in candidates:\n",
    "        if f\"generated_{id_plain}\" in f.name:\n",
    "            return f\n",
    "    for f in candidates:\n",
    "        if f\"generated_{id_padded}\" in f.name:\n",
    "            return f\n",
    "    return sorted(candidates)[0]\n",
    "\n",
    "\n",
    "# Show a few examples (samples with highest and lowest OKS)\n",
    "print(\"Visualizing samples with highest and lowest OKS scores...\\n\")\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"‚úó No results to visualize. Run metric computation first.\")\n",
    "else:\n",
    "    best_indices = results_df.nlargest(3, 'oks').index\n",
    "    worst_indices = results_df.nsmallest(3, 'oks').index\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "    fig.suptitle('Pose Estimation Quality: Best vs Worst Examples', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for row, (best_idx, worst_idx) in enumerate(zip(best_indices, worst_indices)):\n",
    "        best_result = results_df.iloc[best_idx]\n",
    "        worst_result = results_df.iloc[worst_idx]\n",
    "\n",
    "        best_img_id = int(best_result['image_id'])\n",
    "        worst_img_id = int(worst_result['image_id'])\n",
    "\n",
    "        # Best example from generated image\n",
    "        best_img_path = find_generated_image_for_id(best_img_id)\n",
    "        if best_img_path and best_img_id in best_generated_keypoints_dict:\n",
    "            img = cv2.imread(str(best_img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            pred_kpts = best_generated_keypoints_dict[best_img_id]\n",
    "            img_with_pose = draw_skeleton(img_rgb, pred_kpts, SKELETON)\n",
    "            axes[row, 0].imshow(img_with_pose)\n",
    "            axes[row, 0].set_title(f'Best (OKS={best_result[\"oks\"]:.3f})',\n",
    "                                  fontsize=12, fontweight='bold', color='green')\n",
    "        else:\n",
    "            axes[row, 0].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            axes[row, 0].set_title('Best example missing', fontsize=12, color='red')\n",
    "        axes[row, 0].axis('off')\n",
    "\n",
    "        # Worst example from generated image\n",
    "        worst_img_path = find_generated_image_for_id(worst_img_id)\n",
    "        if worst_img_path and worst_img_id in best_generated_keypoints_dict:\n",
    "            img = cv2.imread(str(worst_img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            pred_kpts = best_generated_keypoints_dict[worst_img_id]\n",
    "            img_with_pose = draw_skeleton(img_rgb, pred_kpts, SKELETON)\n",
    "            axes[row, 1].imshow(img_with_pose)\n",
    "            axes[row, 1].set_title(f'Worst (OKS={worst_result[\"oks\"]:.3f})',\n",
    "                                  fontsize=12, fontweight='bold', color='red')\n",
    "        else:\n",
    "            axes[row, 1].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            axes[row, 1].set_title('Worst example missing', fontsize=12, color='red')\n",
    "        axes[row, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20995c23",
   "metadata": {},
   "source": [
    "## 12. Summary & Interpretation\n",
    "\n",
    "### Metrics Explanation:\n",
    "\n",
    "**OKS (Object Keypoint Similarity)** - Primary Metric\n",
    "- Measures how similar predicted poses are to ground truth\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- Standard COCO evaluation metric\n",
    "- Formula: OKS = Œ£(exp(-d_i¬≤/(2*s_k¬≤)) √ó vis_i) / Œ£(vis_i)\n",
    "- Considers both distance and keypoint visibility\n",
    "\n",
    "**mAP@OKS** - Secondary Metric\n",
    "- Percentage of poses with OKS ‚â• threshold\n",
    "- Commonly reported: mAP@OKS=0.5, @0.75, @0.9\n",
    "\n",
    "**PCK (Percentage of Correct Keypoints)**\n",
    "- Percentage of keypoints within distance threshold\n",
    "- Threshold often set to 0.2 √ó bounding box diagonal\n",
    "- More lenient than OKS\n",
    "\n",
    "### Why These Metrics?\n",
    "\n",
    "‚úÖ **OKS**: Industry standard for COCO pose evaluation  \n",
    "‚úÖ **mAP@OKS**: Shows performance across difficulty levels  \n",
    "‚úÖ **PCK**: Provides complementary perspective on accuracy  \n",
    "‚úÖ **Per-keypoint analysis**: Identifies which joints are harder to predict  \n",
    "\n",
    "### Interpretation Guide:\n",
    "\n",
    "| OKS Range | Quality | Interpretation |\n",
    "|-----------|---------|-----------------|\n",
    "| 0.9 - 1.0 | Excellent | Near-perfect pose estimation |\n",
    "| 0.75 - 0.9 | Very Good | High-quality results, minor errors |\n",
    "| 0.5 - 0.75 | Good | Acceptable, some keypoint errors |\n",
    "| 0.25 - 0.5 | Fair | Significant errors in some keypoints |\n",
    "| 0.0 - 0.25 | Poor | Major errors, unreliable poses |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **If OKS is low**: Generated images may lack pose detail\n",
    "2. **If specific keypoints have high error**: May need model fine-tuning\n",
    "3. **Compare with/without spatial conditioning**: Evaluate if ControlNet conditioning improves pose"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
