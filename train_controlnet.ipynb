{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77372c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úì Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"‚úì Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e753de9",
   "metadata": {},
   "source": [
    "# Memory Optimization for Kaggle\n",
    "\n",
    "**Important Notes for Kaggle Execution:**\n",
    "- Kaggle has stricter memory limits than Colab\n",
    "- Clear GPU cache between major operations\n",
    "- Use smaller batch sizes if OOM occurs\n",
    "- Monitor memory usage with `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fbd1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "class COCOKeypointDataset(Dataset):\n",
    "    def __init__(self, root_dir='./coco_data', split='train', transform=None, image_size=512, max_samples=None, download=True):\n",
    "        \"\"\"\n",
    "        Official COCO Keypoint Dataset using pycocotools with captions\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory to store COCO data\n",
    "            split (str): 'train' or 'val'\n",
    "            transform: Optional transform to be applied on images\n",
    "            image_size (int): Size to resize images to\n",
    "            max_samples (int): Optional limit on number of samples to load\n",
    "            download (bool): Whether to download annotations if not found\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # COCO 2017 URLs\n",
    "        self.annotation_urls = {\n",
    "            'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "            'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
    "        }\n",
    "        \n",
    "        # Setup paths\n",
    "        self.ann_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.img_dir = os.path.join(root_dir, f'{split}2017')\n",
    "        \n",
    "        split_name = 'train' if split == 'train' else 'val'\n",
    "        self.ann_file = os.path.join(self.ann_dir, f'person_keypoints_{split_name}2017.json')\n",
    "        self.caption_file = os.path.join(self.ann_dir, f'captions_{split_name}2017.json')\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.ann_dir, exist_ok=True)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        \n",
    "        # Download annotations if needed\n",
    "        if download and not os.path.exists(self.ann_file):\n",
    "            print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n",
    "            self._download_annotations()\n",
    "        \n",
    "        # Check if annotation file exists\n",
    "        if not os.path.exists(self.ann_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Annotation file not found: {self.ann_file}\\n\"\n",
    "                f\"Please download COCO 2017 annotations from:\\n\"\n",
    "                f\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\\n\"\n",
    "                f\"Extract to: {self.ann_dir}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading COCO {split} annotations from official dataset...\")\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        \n",
    "        # Load captions if available\n",
    "        self.coco_caps = None\n",
    "        if os.path.exists(self.caption_file):\n",
    "            print(f\"Loading COCO {split} captions...\")\n",
    "            self.coco_caps = COCO(self.caption_file)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Caption file not found: {self.caption_file}\")\n",
    "            print(\"Captions will not be available. The annotations.zip should contain both keypoints and captions.\")\n",
    "        \n",
    "        # Get all image IDs that have person annotations with keypoints\n",
    "        print(\"Filtering images with keypoint annotations...\")\n",
    "        cat_ids = self.coco.getCatIds(catNms=['person'])\n",
    "        all_img_ids = self.coco.getImgIds(catIds=cat_ids)\n",
    "        \n",
    "        self.img_ids = []\n",
    "        limit = max_samples if max_samples else len(all_img_ids)\n",
    "        \n",
    "        for img_id in all_img_ids[:limit]:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=cat_ids, iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Check if any annotation has keypoints\n",
    "            for ann in anns:\n",
    "                if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                    self.img_ids.append(img_id)\n",
    "                    break\n",
    "        \n",
    "        print(f\"Total images with keypoints: {len(self.img_ids)}\")\n",
    "        print(f\"Images will be loaded from: {self.img_dir}\")\n",
    "        \n",
    "        if len(self.img_ids) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è WARNING: No images with keypoints found!\")\n",
    "    \n",
    "    def _download_annotations(self):\n",
    "        \"\"\"Download COCO annotations\"\"\"\n",
    "        import zipfile\n",
    "        \n",
    "        url = self.annotation_urls[self.split]\n",
    "        zip_path = os.path.join(self.root_dir, 'annotations.zip')\n",
    "        \n",
    "        print(f\"Downloading from {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(zip_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        \n",
    "        print(\"Extracting annotations...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(\"Annotations downloaded successfully!\")\n",
    "    \n",
    "    def _download_image(self, img_info):\n",
    "        \"\"\"Download a single image from COCO\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            try:\n",
    "                response = requests.get(img_info['coco_url'])\n",
    "                with open(img_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {img_info['file_name']}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return img_path\n",
    "    \n",
    "    def get_captions(self, img_id):\n",
    "        \"\"\"Get text captions for an image\"\"\"\n",
    "        if self.coco_caps is None:\n",
    "            return []\n",
    "        \n",
    "        ann_ids = self.coco_caps.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco_caps.loadAnns(ann_ids)\n",
    "        return [ann['caption'] for ann in anns]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        \n",
    "        # Load image info\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        \n",
    "        # Download image if it doesn't exist\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Downloading image: {img_info['file_name']}\")\n",
    "            img_path = self._download_image(img_info)\n",
    "            if img_path is None:\n",
    "                # Return a dummy sample if download fails\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Get keypoints from first person with keypoints\n",
    "        keypoints = None\n",
    "        for ann in anns:\n",
    "            if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                break\n",
    "        \n",
    "        if keypoints is None:\n",
    "            keypoints = np.zeros((17, 3))\n",
    "        \n",
    "        # Get captions\n",
    "        captions = self.get_captions(img_id)\n",
    "        \n",
    "        # Create keypoint heatmap\n",
    "        keypoint_map = self.create_keypoint_map(keypoints, width, height)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5])\n",
    "            ])(image)\n",
    "        \n",
    "        keypoint_map = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])(keypoint_map)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'keypoints': keypoint_map,\n",
    "            'raw_keypoints': keypoints,\n",
    "            'image_id': img_id,\n",
    "            'captions': captions  # List of text descriptions\n",
    "        }\n",
    "    \n",
    "    def create_keypoint_map(self, keypoints, width, height, sigma=2):\n",
    "        \"\"\"\n",
    "        Create a keypoint heatmap from keypoint annotations\n",
    "        \"\"\"\n",
    "        keypoint_map = np.zeros((height, width), dtype=np.float32)\n",
    "        \n",
    "        for i, (x, y, v) in enumerate(keypoints):\n",
    "            if v > 0:  # visible keypoint\n",
    "                x, y = int(x), int(y)\n",
    "                if 0 <= x < width and 0 <= y < height:\n",
    "                    # Create a gaussian around the keypoint\n",
    "                    for dy in range(-sigma*3, sigma*3+1):\n",
    "                        for dx in range(-sigma*3, sigma*3+1):\n",
    "                            nx, ny = x + dx, y + dy\n",
    "                            if 0 <= nx < width and 0 <= ny < height:\n",
    "                                dist = np.sqrt(dx**2 + dy**2)\n",
    "                                keypoint_map[ny, nx] = max(\n",
    "                                    keypoint_map[ny, nx],\n",
    "                                    np.exp(-(dist**2) / (2 * sigma**2))\n",
    "                                )\n",
    "        \n",
    "        return (keypoint_map * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COCO OFFICIAL DATASET LOADER WITH CAPTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis uses the official COCO 2017 dataset with pycocotools.\")\n",
    "print(\"Includes both keypoint annotations AND text captions!\")\n",
    "print(\"\\nThe dataset will:\")\n",
    "print(\"1. Download annotations automatically (~252MB)\")\n",
    "print(\"2. Download images on-demand as you access them\")\n",
    "print(\"3. Cache everything in './coco_data' directory\")\n",
    "print(\"\\nAlternatively, you can manually download:\")\n",
    "print(\"- Annotations: http://images.cocodataset.org/annotations/annotations_trainval2017.zip\")\n",
    "print(\"- Images: http://images.cocodataset.org/zips/train2017.zip (18GB)\")\n",
    "print(\"- Images: http://images.cocodataset.org/zips/val2017.zip (1GB)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create datasets - annotations will download automatically, images download on-demand\n",
    "train_dataset = COCOKeypointDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=512,\n",
    "    max_samples=100,  # Start with 100 for testing, remove for full dataset\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = COCOKeypointDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='val',\n",
    "    image_size=512,\n",
    "    max_samples=50,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 to avoid issues with on-demand downloading\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nImages will be downloaded automatically as needed.\")\n",
    "\n",
    "# Show a sample caption\n",
    "if len(train_dataset) > 0:\n",
    "    sample = train_dataset[0]\n",
    "    if sample['captions']:\n",
    "        print(f\"\\nüìù Sample caption: \\\"{sample['captions'][0]}\\\"\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No captions available (caption file not loaded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one sample from the training dataset\n",
    "sample = train_dataset[0]\n",
    "img_id = sample['image_id']\n",
    "\n",
    "# Get annotations for this image from COCO\n",
    "ann_ids = train_dataset.coco.getAnnIds(imgIds=img_id)\n",
    "anns = train_dataset.coco.loadAnns(ann_ids)\n",
    "img_info = train_dataset.coco.loadImgs(img_id)[0]\n",
    "\n",
    "# Get captions (text prompts)\n",
    "captions = sample['captions']\n",
    "\n",
    "# Build metadata text\n",
    "metadata = f\"Image ID: {img_id}\\nFilename: {img_info['file_name']}\\n\"\n",
    "metadata += f\"Size: {img_info['width']}x{img_info['height']}\\n\"\n",
    "metadata += f\"Person annotations: {len([a for a in anns if a.get('category_id') == 1])}\\n\"\n",
    "\n",
    "for i, ann in enumerate(anns):\n",
    "    if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "        metadata += f\"\\nPerson {i+1}: {ann['num_keypoints']} keypoints\"\n",
    "        if 'area' in ann:\n",
    "            metadata += f\", area: {int(ann['area'])}\"\n",
    "        break\n",
    "\n",
    "# Convert tensors back to displayable format\n",
    "image = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "image = (image * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "keypoint_map = sample['keypoints'].squeeze().cpu().numpy()\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Original image (larger)\n",
    "ax1 = fig.add_subplot(gs[0:2, 0])\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Keypoint heatmap\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(keypoint_map, cmap='hot')\n",
    "ax2.set_title('Keypoint Heatmap', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(image)\n",
    "ax3.imshow(keypoint_map, cmap='hot', alpha=0.5)\n",
    "ax3.set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Text Captions/Prompts - TOP SECTION\n",
    "ax_captions = fig.add_subplot(gs[1, 1:3])\n",
    "ax_captions.axis('off')\n",
    "ax_captions.text(0.05, 0.95, 'üìù TEXT PROMPTS (COCO Captions):', \n",
    "                fontsize=13, fontweight='bold', va='top')\n",
    "\n",
    "if captions:\n",
    "    caption_text = \"\\n\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
    "else:\n",
    "    caption_text = \"No captions available for this image.\"\n",
    "\n",
    "ax_captions.text(0.05, 0.80, caption_text, \n",
    "                fontsize=10, va='top', wrap=True,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Metadata\n",
    "ax_meta = fig.add_subplot(gs[2, 0])\n",
    "ax_meta.axis('off')\n",
    "ax_meta.text(0.05, 0.95, 'Image Metadata:', \n",
    "            fontsize=12, fontweight='bold', va='top')\n",
    "ax_meta.text(0.05, 0.75, metadata, \n",
    "            fontsize=10, va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Keypoint details\n",
    "ax_kp = fig.add_subplot(gs[2, 1:3])\n",
    "ax_kp.axis('off')\n",
    "\n",
    "keypoint_names = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "keypoint_text = \"Keypoint Details:\\n\" + \"‚îÄ\" * 40 + \"\\n\"\n",
    "for i, (x, y, v) in enumerate(sample['raw_keypoints']):\n",
    "    if v > 0:  # visible keypoint\n",
    "        visibility = \"visible\" if v == 2 else \"occluded\"\n",
    "        keypoint_text += f\"{keypoint_names[i]:15s}: ({int(x):3d}, {int(y):3d}) - {visibility}\\n\"\n",
    "\n",
    "ax_kp.text(0.05, 0.95, keypoint_text, \n",
    "          fontsize=9, va='top', family='monospace')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Image shape: {sample['image'].shape}\")\n",
    "print(f\"Keypoint map shape: {sample['keypoints'].shape}\")\n",
    "print(f\"Number of keypoints detected: {(sample['raw_keypoints'][:, 2] > 0).sum()}\")\n",
    "print(f\"Number of captions: {len(captions)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0ae6",
   "metadata": {},
   "source": [
    "# ControlNet Training Setup\n",
    "\n",
    "This section sets up and trains a ControlNet model for pose-guided image generation using:\n",
    "- **Spatial Conditioning**: Keypoint heatmaps\n",
    "- **Text Conditioning**: COCO captions\n",
    "- **Base Model**: Stable Diffusion v1.5\n",
    "\n",
    "The training uses 1000 samples from the COCO keypoint dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install diffusers transformers accelerate xformers safetensors tensorboard\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3819a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    # Model settings\n",
    "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "    controlnet_conditioning_channels = 1  # Grayscale keypoint heatmap\n",
    "    \n",
    "    # Training settings\n",
    "    num_training_samples = 1000\n",
    "    num_epochs = 10\n",
    "    train_batch_size = 1  # Reduced for Kaggle memory constraints\n",
    "    gradient_accumulation_steps = 8  # Increased to maintain effective batch size of 8\n",
    "    learning_rate = 1e-5\n",
    "    lr_warmup_steps = 500\n",
    "    \n",
    "    # Image settings\n",
    "    resolution = 512\n",
    "    \n",
    "    # Checkpointing\n",
    "    output_dir = \"./controlnet_keypoint_output\"\n",
    "    save_steps = 500\n",
    "    checkpointing_steps = 1000\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir = \"./logs\"\n",
    "    report_to = \"tensorboard\"\n",
    "    \n",
    "    # Hardware\n",
    "    mixed_precision = \"fp16\"  # Use \"bf16\" if available, \"no\" for CPU\n",
    "    gradient_checkpointing = True\n",
    "    \n",
    "    # Validation\n",
    "    validation_steps = 500\n",
    "    num_validation_images = 4\n",
    "    validation_prompt = \"a person standing\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Training {config.num_training_samples} samples for {config.num_epochs} epochs\")\n",
    "print(f\"Batch size: {config.train_batch_size}, Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training (limit to 1000 samples)\n",
    "print(\"Recreating training dataset with 1000 samples...\")\n",
    "\n",
    "train_dataset_full = COCOKeypointDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=config.resolution,\n",
    "    max_samples=config.num_training_samples,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create dataloader with collate function\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function to handle captions\"\"\"\n",
    "    images = torch.stack([example['image'] for example in examples])\n",
    "    keypoints = torch.stack([example['keypoints'] for example in examples])\n",
    "    \n",
    "    # Get first caption for each image (COCO has multiple captions per image)\n",
    "    captions = []\n",
    "    for example in examples:\n",
    "        if example['captions'] and len(example['captions']) > 0:\n",
    "            captions.append(example['captions'][0])\n",
    "        else:\n",
    "            captions.append(\"a person\")  # Fallback caption\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'keypoints': keypoints,\n",
    "        'captions': captions\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_full,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training dataset ready: {len(train_dataset_full)} samples\")\n",
    "print(f\"‚úì Total batches per epoch: {len(train_dataloader)}\")\n",
    "\n",
    "# Clear memory after dataset creation (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# Initialize ControlNet from UNet\n",
    "print(\"Initializing ControlNet...\")\n",
    "controlnet = ControlNetModel.from_unet(\n",
    "    unet,\n",
    "    conditioning_channels=config.controlnet_conditioning_channels\n",
    ")\n",
    "\n",
    "# Load noise scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# Freeze VAE and text encoder - we only train ControlNet\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"‚úì Models loaded successfully!\")\n",
    "print(f\"  - ControlNet parameters: {sum(p.numel() for p in controlnet.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  - Text encoder (frozen): {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  - UNet (frozen): {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "\n",
    "# Clear memory after model loading (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
    "    num_training_steps=len(train_dataloader) * config.num_epochs * config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Optimizer configured\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Warmup steps: {config.lr_warmup_steps}\")\n",
    "print(f\"  - Total training steps: {len(train_dataloader) * config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Accelerator for distributed training and mixed precision\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    log_with=config.report_to,\n",
    "    project_dir=config.logging_dir,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if config.gradient_checkpointing:\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "\n",
    "# Prepare models with accelerator\n",
    "controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Move models to device\n",
    "unet.to(accelerator.device)\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "# Set models to eval mode (only ControlNet is in training mode)\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "print(f\"‚úì Accelerator initialized\")\n",
    "print(f\"  - Device: {accelerator.device}\")\n",
    "print(f\"  - Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"  - Distributed: {accelerator.num_processes} process(es)\")\n",
    "print(f\"  - Gradient accumulation: {config.gradient_accumulation_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a18159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_controlnet():\n",
    "    \"\"\"Main training loop for ControlNet\"\"\"\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(\n",
    "        range(len(train_dataloader) * config.num_epochs),\n",
    "        desc=\"Training\",\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting ControlNet Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total epochs: {config.num_epochs}\")\n",
    "    print(f\"Samples per epoch: {len(train_dataset_full)}\")\n",
    "    print(f\"Batches per epoch: {len(train_dataloader)}\")\n",
    "    print(f\"Total training steps: {len(train_dataloader) * config.num_epochs}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        controlnet.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                # Get images and conditioning\n",
    "                images = batch['images'].to(accelerator.device, dtype=torch.float32)\n",
    "                keypoint_conditioning = batch['keypoints'].to(accelerator.device, dtype=torch.float32)\n",
    "                captions = batch['captions']\n",
    "                \n",
    "                # Encode images to latent space with VAE\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(images).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Sample noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                \n",
    "                # Sample random timesteps for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps, (bsz,),\n",
    "                    device=latents.device\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "                \n",
    "                # Add noise to latents (forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Encode text prompts\n",
    "                with torch.no_grad():\n",
    "                    text_inputs = tokenizer(\n",
    "                        captions,\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "                    text_embeddings = text_encoder(text_inputs.input_ids.to(accelerator.device))[0]\n",
    "                \n",
    "                # Get ControlNet output\n",
    "                down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    controlnet_cond=keypoint_conditioning,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "                \n",
    "                # Predict noise with UNet + ControlNet conditioning\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    down_block_additional_residuals=down_block_res_samples,\n",
    "                    mid_block_additional_residual=mid_block_res_sample,\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss (MSE between predicted and actual noise)\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                \n",
    "                # Backpropagation\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                epoch_loss += loss.detach().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % 10 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    logs = {\n",
    "                        \"loss\": loss.detach().item(),\n",
    "                        \"avg_loss\": avg_loss,\n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch,\n",
    "                    }\n",
    "                    progress_bar.set_postfix(**logs)\n",
    "                    accelerator.log(logs, step=global_step)\n",
    "                \n",
    "                # Memory monitoring (Kaggle optimization)\n",
    "                if global_step % 100 == 0:\n",
    "                    print_memory_usage()\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % config.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        print(f\"\\n‚úì Checkpoint saved: {save_path}\")\n",
    "        \n",
    "        # End of epoch summary\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs} completed\")\n",
    "        print(f\"Average loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    # Save final model\n",
    "    if accelerator.is_main_process:\n",
    "        controlnet_save_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "        unwrapped_controlnet = accelerator.unwrap_model(controlnet)\n",
    "        unwrapped_controlnet.save_pretrained(controlnet_save_path)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì Training completed!\")\n",
    "        print(f\"‚úì Final ControlNet saved to: {controlnet_save_path}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "    \n",
    "    print(\"‚úì Training function ready\")\n",
    "\n",
    "    return controlnet\n",
    "    return controlnetprint(\"‚úì Training function ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "trained_controlnet = train_controlnet()\n",
    "\n",
    "print(f\"\\n‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e2964",
   "metadata": {},
   "source": [
    "## Test the Trained ControlNet\n",
    "\n",
    "Generate images using the trained ControlNet with keypoint conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained ControlNet and create pipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Clear memory before inference (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Loading trained ControlNet pipeline...\")\n",
    "\n",
    "# Load the saved ControlNet\n",
    "controlnet_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "controlnet_trained = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "# Create inference pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    controlnet=controlnet_trained,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ")\n",
    "pipe = pipe.to(accelerator.device)\n",
    "\n",
    "# Enable memory efficient attention (optional, if xformers is available)\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"‚úì XFormers memory efficient attention enabled\")\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"‚ö†Ô∏è  XFormers not available, using default attention: {e}\")print(\"‚úì Pipeline ready for inference!\")\n",
    "\n",
    "    print(\"   (This is fine, just uses a bit more memory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images using trained ControlNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a test sample from validation dataset\n",
    "test_sample = val_dataset[0]\n",
    "test_keypoint = test_sample['keypoints']\n",
    "test_caption = test_sample['captions'][0] if test_sample['captions'] else \"a person standing\"\n",
    "\n",
    "print(f\"Test prompt: \\\"{test_caption}\\\"\")\n",
    "print(f\"Keypoint shape: {test_keypoint.shape}\")\n",
    "\n",
    "# Prepare keypoint conditioning as tensor (keep single channel!)\n",
    "# Add batch dimension and move to device\n",
    "test_keypoint_input = test_keypoint.unsqueeze(0).to(accelerator.device, dtype=torch.float16)\n",
    "print(f\"Keypoint input shape: {test_keypoint_input.shape}\")\n",
    "\n",
    "# Generate image\n",
    "print(\"\\nGenerating image with ControlNet...\")\n",
    "generator = torch.Generator(device=accelerator.device).manual_seed(42)\n",
    "\n",
    "# Use the tensor directly instead of PIL image to maintain single channel\n",
    "output = pipe(\n",
    "    prompt=test_caption,\n",
    "    image=test_keypoint_input,  # Pass tensor directly\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    guidance_scale=7.5,\n",
    ").images[0]\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Original image\n",
    "orig_img = test_sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "orig_img = (orig_img * 0.5 + 0.5)\n",
    "orig_img = np.clip(orig_img, 0, 1)\n",
    "axes[0].imshow(orig_img)\n",
    "axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Keypoint conditioning\n",
    "axes[1].imshow(test_keypoint.squeeze().cpu().numpy(), cmap='hot')\n",
    "axes[1].set_title('Keypoint Conditioning', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[2].imshow(output)\n",
    "axes[2].set_title('Generated Image', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Side by side comparison\n",
    "axes[3].imshow(orig_img)\n",
    "axes[3].imshow(test_keypoint.squeeze().cpu().numpy(), cmap='hot', alpha=0.3)\n",
    "axes[3].set_title('Original + Keypoints', fontsize=14, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{test_caption}\"', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Image generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
