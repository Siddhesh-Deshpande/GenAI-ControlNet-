{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77372c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:23.436576Z",
     "iopub.status.busy": "2025-12-10T19:12:23.435957Z",
     "iopub.status.idle": "2025-12-10T19:12:23.441817Z",
     "shell.execute_reply": "2025-12-10T19:12:23.441213Z",
     "shell.execute_reply.started": "2025-12-10T19:12:23.436552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úì Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"‚úì Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e753de9",
   "metadata": {},
   "source": [
    "# ControlNet Training for Pose-Based Spatial Conditioning\n",
    "\n",
    "## üìã **Pre-Flight Checklist - MUST DO BEFORE RUNNING!**\n",
    "\n",
    "### ‚úÖ **1. Update Caption File Path**\n",
    "In Cell 8 (Training Configuration), update:\n",
    "```python\n",
    "train_captions_file = './train_captions.json'  # ‚Üê CHANGE THIS!\n",
    "```\n",
    "\n",
    "Options:\n",
    "- **Local**: `'./train_captions.json'` or `'path/to/your/train_captions.json'`\n",
    "- **Kaggle**: `'/kaggle/input/captions/train_captions.json'`\n",
    "\n",
    "### ‚úÖ **2. Verify Caption File Format**\n",
    "Your JSON file should look like:\n",
    "```json\n",
    "{\n",
    "  \"000000391895.jpg\": \"A person wearing a red shirt...\",\n",
    "  \"000000522418.jpg\": \"A person standing outdoors...\",\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "### ‚úÖ **3. Check GPU/Hardware**\n",
    "- Requires CUDA GPU (tested on T4, P100, V100)\n",
    "- ~16GB GPU memory minimum\n",
    "- ~50GB disk space for COCO data + checkpoints\n",
    "\n",
    "### ‚úÖ **4. Install Dependencies** (if not already installed)\n",
    "```bash\n",
    "pip install diffusers transformers accelerate xformers safetensors tensorboard pycocotools\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Once Ready:**\n",
    "Run cells sequentially from top to bottom. Training will:\n",
    "- Download COCO annotations automatically\n",
    "- Download images on-the-fly as needed\n",
    "- Save checkpoints to `./controlnet_pose_output/`\n",
    "- Generate validation samples every epoch\n",
    "- Auto-cleanup old checkpoints to save disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fbd1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:23.648737Z",
     "iopub.status.busy": "2025-12-10T19:12:23.648543Z",
     "iopub.status.idle": "2025-12-10T19:12:34.620924Z",
     "shell.execute_reply": "2025-12-10T19:12:34.620230Z",
     "shell.execute_reply.started": "2025-12-10T19:12:23.648723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import zipfile\n",
    "\n",
    "class COCOPoseDataset(Dataset):\n",
    "    def __init__(self, root_dir='./coco_data', split='train', transform=None, image_size=512, \n",
    "                 custom_captions_file=None, max_samples=None, download=True):\n",
    "        \"\"\"\n",
    "        Custom Dataset with COCO Pose Skeletons + Your Custom Captions\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory to store COCO data\n",
    "            split (str): 'train' or 'val'\n",
    "            transform: Optional transform to be applied on images\n",
    "            image_size (int): Size to resize images to\n",
    "            custom_captions_file (str): Path to JSON file with custom captions {image_name: caption}\n",
    "            max_samples (int): Optional limit on number of samples to load\n",
    "            download (bool): Whether to download COCO annotations if not found\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        self.custom_captions_file = custom_captions_file\n",
    "        \n",
    "        # COCO 2017 URLs\n",
    "        self.annotation_urls = {\n",
    "            'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "            'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
    "        }\n",
    "        \n",
    "        # Setup paths\n",
    "        self.ann_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.img_dir = os.path.join(root_dir, f'{split}2017')\n",
    "        \n",
    "        split_name = 'train' if split == 'train' else 'val'\n",
    "        self.ann_file = os.path.join(self.ann_dir, f'person_keypoints_{split_name}2017.json')\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.ann_dir, exist_ok=True)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        \n",
    "        # Load custom captions - REQUIRED\n",
    "        self.custom_captions = None\n",
    "        self.custom_caption_map = {}\n",
    "        self.img_ids = []\n",
    "        \n",
    "        if not custom_captions_file:\n",
    "            raise ValueError(f\"‚ùå custom_captions_file is REQUIRED! Please provide a JSON file with captions.\")\n",
    "        \n",
    "        # Check if caption file exists (try absolute and relative paths)\n",
    "        caption_path = custom_captions_file\n",
    "        if not os.path.exists(caption_path):\n",
    "            # Try absolute path if relative doesn't work\n",
    "            caption_path = os.path.abspath(custom_captions_file)\n",
    "        \n",
    "        if not os.path.exists(caption_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Caption file not found: {custom_captions_file}\\n\"\n",
    "                f\"Tried paths:\\n\"\n",
    "                f\"  - {custom_captions_file}\\n\"\n",
    "                f\"  - {os.path.abspath(custom_captions_file)}\\n\"\n",
    "                f\"Current working directory: {os.getcwd()}\\n\"\n",
    "                f\"Please ensure the caption file exists with format: {{'image_filename.jpg': 'caption text', ...}}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading custom captions from {caption_path}...\")\n",
    "        with open(caption_path, 'r') as f:\n",
    "            self.custom_captions = json.load(f)\n",
    "        print(f\"‚úì Loaded {len(self.custom_captions)} custom captions\")\n",
    "        \n",
    "        # Download COCO annotations if needed\n",
    "        if download and not os.path.exists(self.ann_file):\n",
    "            print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n",
    "            self._download_annotations()\n",
    "        \n",
    "        # Check if annotation file exists\n",
    "        if not os.path.exists(self.ann_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Annotation file not found: {self.ann_file}\\n\"\n",
    "                f\"Please download COCO 2017 annotations from:\\n\"\n",
    "                f\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\\n\"\n",
    "                f\"Extract to: {self.ann_dir}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading COCO {split} pose annotations...\")\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        \n",
    "        # Map images from caption file\n",
    "        print(\"Setting up dataset with YOUR images, poses from COCO, and YOUR captions...\")\n",
    "        items_to_process = list(self.custom_captions.items())\n",
    "        if max_samples:\n",
    "            items_to_process = items_to_process[:max_samples]\n",
    "        \n",
    "        for img_filename, caption in items_to_process:\n",
    "            # Extract image ID from filename (e.g., '000000391895.jpg' -> 391895)\n",
    "            img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n",
    "            self.img_ids.append(img_id)\n",
    "            self.custom_caption_map[img_id] = caption\n",
    "        \n",
    "        limit_msg = f\" (limited to first {max_samples})\" if max_samples else \"\"\n",
    "        print(f\"‚úì Dataset ready with {len(self.img_ids)} images\")\n",
    "        print(f\"  - Images from: {self.img_dir}\")\n",
    "        print(f\"  - Captions from: {custom_captions_file}{limit_msg}\")\n",
    "        print(f\"  - Poses from: COCO person_keypoints annotations\")\n",
    "        print(f\"‚úÖ Using ACTUAL POSE SKELETONS (stick figures) as conditioning!\\n\")\n",
    "        \n",
    "        if len(self.img_ids) == 0:\n",
    "            print(\"\\n‚ùå ERROR: No images found in caption file!\")\n",
    "    \n",
    "    def _download_annotations(self):\n",
    "        \"\"\"Download COCO annotations\"\"\"\n",
    "        url = self.annotation_urls[self.split]\n",
    "        zip_path = os.path.join(self.root_dir, 'annotations.zip')\n",
    "        \n",
    "        print(f\"Downloading from {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(zip_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        \n",
    "        print(\"Extracting annotations...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(\"‚úì Annotations downloaded successfully!\")\n",
    "    \n",
    "    def _download_image(self, img_id, img_filename):\n",
    "        \"\"\"Download a single image from COCO dataset on-the-fly\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        # If image already exists, skip download\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "        \n",
    "        # Get image info from COCO API\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info['coco_url']\n",
    "        \n",
    "        # Download image\n",
    "        try:\n",
    "            response = requests.get(img_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save image\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return img_path\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to download image {img_filename} from {img_url}: {str(e)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_filename = list(self.custom_captions.keys())[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        # Download image if it doesn't exist (ON-THE-FLY DOWNLOAD)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"üì• Downloading image: {img_filename}...\")\n",
    "            img_path = self._download_image(img_id, img_filename)\n",
    "        \n",
    "        # Try to load image, skip if corrupted\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        image = None\n",
    "        \n",
    "        while retry_count < max_retries and image is None:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                width, height = image.size\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"‚ö†Ô∏è  Failed to load image {img_filename} (attempt {retry_count}/{max_retries}): {str(e)[:50]}\")\n",
    "                \n",
    "                # Delete corrupted file and try re-downloading\n",
    "                if os.path.exists(img_path):\n",
    "                    os.remove(img_path)\n",
    "                    print(f\"   Deleted corrupted file: {img_filename}\")\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    try:\n",
    "                        print(f\"   Re-downloading image...\")\n",
    "                        img_path = self._download_image(img_id, img_filename)\n",
    "                    except Exception as download_err:\n",
    "                        print(f\"   Download failed: {str(download_err)[:50]}\")\n",
    "        \n",
    "        # If still can't load, return a placeholder sample\n",
    "        if image is None:\n",
    "            print(f\"‚ùå Giving up on {img_filename}, returning next valid image instead...\")\n",
    "            # Try next image in dataset\n",
    "            next_idx = (idx + 1) % len(self.img_ids)\n",
    "            if next_idx != idx:  # Avoid infinite loop\n",
    "                return self.__getitem__(next_idx)\n",
    "            else:\n",
    "                # Return black image as fallback\n",
    "                image = Image.new('RGB', (self.image_size, self.image_size), color='black')\n",
    "                width, height = self.image_size, self.image_size\n",
    "        \n",
    "        # Get caption\n",
    "        caption = self.custom_caption_map[img_id]\n",
    "        \n",
    "        # ‚úÖ EXTRACT ACTUAL POSE KEYPOINTS FROM COCO\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Get pose keypoints from first person with visible keypoints\n",
    "        keypoints = None\n",
    "        for ann in anns:\n",
    "            if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                break\n",
    "        \n",
    "        if keypoints is None:\n",
    "            keypoints = np.zeros((17, 3))\n",
    "        \n",
    "        # ‚úÖ CREATE ACTUAL POSE SKELETON (stick figure from keypoints)\n",
    "        pose_skeleton = self.create_pose_skeleton(keypoints, width, height)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5])\n",
    "            ])(image)\n",
    "        \n",
    "        pose_map = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "        ])(pose_skeleton)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'pose': pose_map,\n",
    "            'raw_keypoints': keypoints,\n",
    "            'image_id': img_id,\n",
    "            'captions': [caption]  # Your custom caption\n",
    "        }\n",
    "    \n",
    "    def create_pose_skeleton(self, keypoints, width, height):\n",
    "        \"\"\"\n",
    "        Create ACTUAL human pose skeleton from COCO keypoint annotations\n",
    "        Draws a stick figure with bones connecting the 17 joints\n",
    "        \n",
    "        COCO Keypoints (17 total):\n",
    "        0: nose, 1: L eye, 2: R eye, 3: L ear, 4: R ear,\n",
    "        5: L shoulder, 6: R shoulder, 7: L elbow, 8: R elbow, 9: L wrist, 10: R wrist,\n",
    "        11: L hip, 12: R hip, 13: L knee, 14: R knee, 15: L ankle, 16: R ankle\n",
    "        \"\"\"\n",
    "        # Create black canvas\n",
    "        pose_img = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # COCO keypoint skeleton connections (bones linking joints)\n",
    "        skeleton = [\n",
    "            (0, 1), (0, 2),           # nose to eyes\n",
    "            (1, 3), (2, 4),           # eyes to ears\n",
    "            (0, 5), (0, 6),           # nose to shoulders\n",
    "            (5, 7), (7, 9),           # left arm (shoulder‚Üíelbow‚Üíwrist)\n",
    "            (6, 8), (8, 10),          # right arm (shoulder‚Üíelbow‚Üíwrist)\n",
    "            (5, 11), (6, 12),         # shoulders to hips\n",
    "            (11, 12),                 # hip to hip\n",
    "            (11, 13), (13, 15),       # left leg (hip‚Üíknee‚Üíankle)\n",
    "            (12, 14), (14, 16)        # right leg (hip‚Üíknee‚Üíankle)\n",
    "        ]\n",
    "        \n",
    "        # Line thickness and circle radius scale with image size\n",
    "        line_thickness = max(2, int(min(width, height) / 100))\n",
    "        circle_radius = max(3, int(min(width, height) / 80))\n",
    "        \n",
    "        # Draw bones (connections) FIRST\n",
    "        for start_idx, end_idx in skeleton:\n",
    "            if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
    "                x1, y1, v1 = keypoints[start_idx]\n",
    "                x2, y2, v2 = keypoints[end_idx]\n",
    "                \n",
    "                # Draw line ONLY if both keypoints are visible (v > 0)\n",
    "                if v1 > 0 and v2 > 0:\n",
    "                    cv2.line(pose_img, (int(x1), int(y1)), (int(x2), int(y2)), \n",
    "                            255, line_thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw keypoint circles ON TOP of bones\n",
    "        for i, (x, y, v) in enumerate(keypoints):\n",
    "            if v > 0:  # Only draw visible keypoints\n",
    "                cv2.circle(pose_img, (int(x), int(y)), circle_radius, 255, -1)\n",
    "        \n",
    "        return pose_img\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUSTOM DATASET LOADER WITH ACTUAL POSE SKELETONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Pipeline:\")\n",
    "print(\"1. ‚úÖ Downloads COCO images on-the-fly (from COCO CDN)\")\n",
    "print(\"2. ‚úÖ Loads YOUR custom captions\")\n",
    "print(\"3. ‚úÖ Extracts ACTUAL pose keypoints from COCO annotations\")\n",
    "print(\"4. ‚úÖ Draws stick figures (nose‚Üíeyes‚Üíears, shoulders‚Üíelbows‚Üíwrists, etc.)\")\n",
    "print(\"5. ‚úÖ Passes: image + caption + pose skeleton to training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets with COCO POSES + YOUR CUSTOM CAPTIONS\n",
    "train_dataset = COCOPoseDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=512,\n",
    "    custom_captions_file='/kaggle/input/captions/train_captions.json',\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = COCOPoseDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='val',\n",
    "    image_size=512,\n",
    "    custom_captions_file='/kaggle/input/captions/val_captions.json',\n",
    "    max_samples=None,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)} (images + your captions + COCO poses)\")\n",
    "print(f\"Validation samples: {len(val_dataset)} (images + your captions + COCO poses)\")\n",
    "\n",
    "# Show a sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample = train_dataset[0]\n",
    "    if sample['captions']:\n",
    "        print(f\"\\nüìù Sample caption from training:\")\n",
    "        caption = sample['captions'][0]\n",
    "        print(f\"   \\\"{caption[:200]}...\\\"\" if len(caption) > 200 else f\"   \\\"{caption}\\\"\")\n",
    "    print(f\"ü¶¥ Sample has {(sample['raw_keypoints'][:, 2] > 0).sum()} visible pose keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c4e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:34.622584Z",
     "iopub.status.busy": "2025-12-10T19:12:34.622339Z",
     "iopub.status.idle": "2025-12-10T19:12:35.852398Z",
     "shell.execute_reply": "2025-12-10T19:12:35.851543Z",
     "shell.execute_reply.started": "2025-12-10T19:12:34.622568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one sample from the training dataset\n",
    "sample = train_dataset[0]\n",
    "img_id = sample['image_id']\n",
    "\n",
    "# Get annotations for this image from COCO\n",
    "ann_ids = train_dataset.coco.getAnnIds(imgIds=img_id)\n",
    "anns = train_dataset.coco.loadAnns(ann_ids)\n",
    "img_info = train_dataset.coco.loadImgs(img_id)[0]\n",
    "\n",
    "# Get captions (text prompts)\n",
    "captions = sample['captions']\n",
    "\n",
    "# Build metadata text\n",
    "metadata = f\"Image ID: {img_id}\\nFilename: {img_info['file_name']}\\n\"\n",
    "metadata += f\"Size: {img_info['width']}x{img_info['height']}\\n\"\n",
    "metadata += f\"Person annotations: {len([a for a in anns if a.get('category_id') == 1])}\\n\"\n",
    "\n",
    "for i, ann in enumerate(anns):\n",
    "    if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "        metadata += f\"\\nPerson {i+1}: {ann['num_keypoints']} keypoints\"\n",
    "        if 'area' in ann:\n",
    "            metadata += f\", area: {int(ann['area'])}\"\n",
    "        break\n",
    "\n",
    "# Convert tensors back to displayable format\n",
    "image = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "image = (image * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "# Convert PIL Image to numpy array\n",
    "pose_map = np.array(sample['pose'])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Original image (larger)\n",
    "ax1 = fig.add_subplot(gs[0:2, 0])\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Pose skeleton\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(pose_map, cmap='gray')\n",
    "ax2.set_title('Pose Skeleton', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(image)\n",
    "ax3.imshow(pose_map, cmap='hot', alpha=0.5)\n",
    "ax3.set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Text Captions/Prompts - TOP SECTION\n",
    "ax_captions = fig.add_subplot(gs[1, 1:3])\n",
    "ax_captions.axis('off')\n",
    "ax_captions.text(0.05, 0.95, 'üìù TEXT PROMPTS (Custom Captions):', \n",
    "                fontsize=13, fontweight='bold', va='top')\n",
    "\n",
    "if captions:\n",
    "    caption_text = \"\\n\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
    "else:\n",
    "    caption_text = \"No captions available for this image.\"\n",
    "\n",
    "ax_captions.text(0.05, 0.80, caption_text, \n",
    "                fontsize=10, va='top', wrap=True,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Metadata\n",
    "ax_meta = fig.add_subplot(gs[2, 0])\n",
    "ax_meta.axis('off')\n",
    "ax_meta.text(0.05, 0.95, 'Image Metadata:', \n",
    "            fontsize=12, fontweight='bold', va='top')\n",
    "ax_meta.text(0.05, 0.75, metadata, \n",
    "            fontsize=10, va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Pose keypoint details\n",
    "ax_kp = fig.add_subplot(gs[2, 1:3])\n",
    "ax_kp.axis('off')\n",
    "\n",
    "keypoint_names = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "keypoint_text = \"Keypoint Details:\\n\" + \"‚îÄ\" * 40 + \"\\n\"\n",
    "for i, (x, y, v) in enumerate(sample['raw_keypoints']):\n",
    "    if v > 0:  # visible keypoint\n",
    "        visibility = \"visible\" if v == 2 else \"occluded\"\n",
    "        keypoint_text += f\"{keypoint_names[i]:15s}: ({int(x):3d}, {int(y):3d}) - {visibility}\\n\"\n",
    "\n",
    "ax_kp.text(0.05, 0.95, keypoint_text, \n",
    "          fontsize=9, va='top', family='monospace')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Image shape: {sample['image'].shape}\")\n",
    "print(f\"Pose skeleton shape: {pose_map.shape}\")\n",
    "print(f\"Number of visible keypoints: {(sample['raw_keypoints'][:, 2] > 0).sum()}\")\n",
    "print(f\"Number of captions: {len(captions)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0ae6",
   "metadata": {},
   "source": [
    "# ControlNet Training Setup\n",
    "\n",
    "This section sets up and trains a ControlNet model for pose-guided image generation using:\n",
    "- **Spatial Conditioning**: Pose skeleton (stick figure from COCO keypoints)\n",
    "- **Text Conditioning**: Your Custom Captions\n",
    "- **Base Model**: Stable Diffusion v1.5\n",
    "\n",
    "The training uses your custom-captioned images with COCO pose annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28baec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.853428Z",
     "iopub.status.busy": "2025-12-10T19:12:35.853217Z",
     "iopub.status.idle": "2025-12-10T19:12:35.858717Z",
     "shell.execute_reply": "2025-12-10T19:12:35.857838Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.853410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install diffusers transformers accelerate xformers safetensors tensorboard\n",
    "# ‚úÖ NEW: Required for 8-bit optimizer\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REQUIRED PACKAGES FOR 8-BIT OPTIMIZER TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Core packages: diffusers, transformers, accelerate, xformers\")\n",
    "print(\"‚úÖ Bitsandbytes: For 8-bit optimizer (memory efficient)\")\n",
    "print(\"‚úÖ Tensorboard: For training visualization\")\n",
    "print(\"\\nIf you get 'bitsandbytes' import error, run:\")\n",
    "print(\"  pip install bitsandbytes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ef460",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "### ‚úÖ **NEW: Memory-Efficient 8-bit Optimizer with Accelerate + FP16**\n",
    "\n",
    "This training setup now uses **state-of-the-art memory optimization**:\n",
    "\n",
    "#### 1. **8-bit Optimizer (bitsandbytes)**\n",
    "- Uses `AdamW8bit` from bitsandbytes library\n",
    "- **Memory reduction**: ~75% less memory for optimizer states\n",
    "- **Previous**: Standard AdamW (stores full precision optimizer states)\n",
    "- **Now**: 8-bit quantized optimizer states\n",
    "- **Benefits**:\n",
    "  - **Larger batch sizes** (16 instead of 1)\n",
    "  - **Better gradient updates** from larger batches\n",
    "  - **Faster convergence** with more diverse samples per step\n",
    "\n",
    "#### 2. **Accelerate + FP16 Mixed Precision**\n",
    "- `accelerate` library handles distributed training and memory optimization\n",
    "- FP16 (float16) computation reduces memory by 50%\n",
    "- Maintains numerical stability with loss scaling\n",
    "- **Combined with 8-bit optimizer**: Near optimal memory efficiency\n",
    "\n",
    "#### 3. **Effective Batch Size = 16** ‚úÖ\n",
    "- **Before**: batch_size=1, gradient_accumulation=8 ‚Üí effective batch=8\n",
    "- **After**: batch_size=16, gradient_accumulation=1 ‚Üí effective batch=16\n",
    "- **Why better**:\n",
    "  - 8x larger batch = better gradient estimates\n",
    "  - No gradient accumulation overhead\n",
    "  - Faster training iterations\n",
    "  - More stable training dynamics\n",
    "\n",
    "#### 4. **Gradient Checkpointing**\n",
    "- Trades computation for memory during forward/backward pass\n",
    "- Reduces activation memory by ~40-50%\n",
    "- Minimal speed impact with modern GPUs\n",
    "\n",
    "### üìä **Memory Impact Summary**\n",
    "```\n",
    "Configuration              | GPU Memory | Batch Size | Training Speed\n",
    "Standard FP32 (Baseline)   | 100%       | 1x         | 100%\n",
    "FP16 only                  | 50%        | 2x         | 110%\n",
    "FP16 + Gradient Ckpt       | 25-30%     | 4x         | 105%\n",
    "‚úÖ 8-bit + FP16 + Ckpt    | 15-20%     | 16x        | 95%\n",
    "```\n",
    "\n",
    "### üéØ **Why This Matters**\n",
    "- **Effective batch size of 16** provides much better gradient estimates\n",
    "- **8-bit optimizer** doesn't hurt convergence (bitsandbytes handles this carefully)\n",
    "- **Combined approach** achieves 5-6x memory savings vs standard FP32\n",
    "- **Result**: Better training quality in same memory budget\n",
    "\n",
    "### üìù **Configuration Options**\n",
    "In the Training Configuration cell, you can control:\n",
    "- `use_8bit_optimizer`: Toggle 8-bit optimizer (True/False)\n",
    "- `train_batch_size`: Main batch size (default: 16)\n",
    "- `gradient_accumulation_steps`: Accumulation steps (default: 1, increase if OOM)\n",
    "- `num_training_samples`: Set to `None` to use ALL data, or limit to a number\n",
    "\n",
    "### ‚ö†Ô∏è **Important Notes**\n",
    "- **bitsandbytes required**: Install with `pip install bitsandbytes`\n",
    "- **CUDA recommended**: 8-bit operations optimized for NVIDIA GPUs\n",
    "- **All training data used**: `num_training_samples = None` uses complete dataset\n",
    "- **Expected training time**: ~30-50% faster with larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b700681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases login (hardcode your key below)\n",
    "import os, wandb\n",
    "\n",
    "# üîë Put your key in the string below\n",
    "WANDB_KEY = \"PASTE_YOUR_WANDB_API_KEY_HERE\"\n",
    "\n",
    "if WANDB_KEY and WANDB_KEY != \"PASTE_YOUR_WANDB_API_KEY_HERE\":\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_KEY\n",
    "    wandb.login(key=WANDB_KEY, relogin=True)\n",
    "    print(\"‚úì W&B logged in with provided key\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Set WANDB_KEY to your actual API key to enable W&B logging.\")\n",
    "\n",
    "# Default project if not provided via env/secret\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"controlnet-pose\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3819a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.859714Z",
     "iopub.status.busy": "2025-12-10T19:12:35.859455Z",
     "iopub.status.idle": "2025-12-10T19:12:35.874798Z",
     "shell.execute_reply": "2025-12-10T19:12:35.874019Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.859691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    # Model settings\n",
    "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "    controlnet_conditioning_channels = 1  # Grayscale pose skeleton\n",
    "    \n",
    "    # Training settings\n",
    "    num_training_samples = None  # Set to None to use ALL images, or set to a number (e.g., 1000) to limit\n",
    "    num_epochs = 10\n",
    "    train_batch_size = 16\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    lr_scheduler_type = \"constant_with_warmup\"\n",
    "    caption_dropout_prob = 0.5\n",
    "    \n",
    "    # Data paths - UPDATE THESE TO YOUR PATHS!\n",
    "    train_captions_file = './train_captions.json'  # Path to training captions JSON\n",
    "    # For Kaggle, use: '/kaggle/input/train_captions.json'\n",
    "    \n",
    "    # Image settings\n",
    "    resolution = 512\n",
    "    \n",
    "    # Checkpointing & Validation\n",
    "    output_dir = \"./controlnet_pose_output\"\n",
    "    validate_every_n_epochs = 1  # Generate validation samples every N epochs\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir = \"./logs\"\n",
    "    report_to = \"wandb\"\n",
    "    wandb_project = os.environ.get(\"WANDB_PROJECT\", \"controlnet-pose\")\n",
    "    wandb_run_name = None  # Optionally set a custom run name\n",
    "    \n",
    "    # Hardware & Optimization\n",
    "    mixed_precision = \"fp16\"  # Use \"bf16\" if available, \"no\" for CPU\n",
    "    gradient_checkpointing = True\n",
    "    use_8bit_optimizer = False  # ‚úÖ DISABLED: Use standard AdamW instead\n",
    "    \n",
    "    # Validation\n",
    "    validation_steps = 500\n",
    "    num_validation_images = 4\n",
    "    validation_prompt = \"a person standing\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Training {config.num_training_samples or 'ALL (COMPLETE DATASET)'} samples for {config.num_epochs} epochs\")\n",
    "print(f\"‚úÖ Using ALL available training data points\")\n",
    "print(f\"\\nBatch Configuration:\")\n",
    "print(f\"  - Batch size: {config.train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"\\nOptimizer & Memory:\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"  - 8-bit optimizer: {'‚úÖ ENABLED' if config.use_8bit_optimizer else '‚ùå DISABLED'}\")\n",
    "print(f\"  - Gradient checkpointing: {'‚úÖ ENABLED' if config.gradient_checkpointing else '‚ùå DISABLED'}\")\n",
    "print(f\"\\nTraining Strategy:\")\n",
    "print(f\"  - Caption dropout: {config.caption_dropout_prob*100:.0f}% (enables unconditional generation)\")\n",
    "print(f\"  - Validation: Every {config.validate_every_n_epochs} epoch(s)\")\n",
    "print(f\"\\nLogging:\")\n",
    "print(f\"  - Reporting to: {config.report_to}\")\n",
    "print(f\"  - W&B Project: {config.wandb_project}\")\n",
    "print(f\"  - Run name: {config.wandb_run_name or 'auto'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389b625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.876988Z",
     "iopub.status.busy": "2025-12-10T19:12:35.876791Z",
     "iopub.status.idle": "2025-12-10T19:12:45.774266Z",
     "shell.execute_reply": "2025-12-10T19:12:45.773631Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.876973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define collate function with caption dropout\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that:\n",
    "    1. Handles variable-sized inputs\n",
    "    2. Applies caption dropout for classifier-free guidance\n",
    "    3. Normalizes pose maps to [-1, 1]\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    poses = []\n",
    "    captions = []\n",
    "    \n",
    "    for sample in batch:\n",
    "        images.append(sample['image'])\n",
    "        \n",
    "        # Convert pose PIL Image to tensor and normalize to [-1, 1]\n",
    "        pose_tensor = transforms.ToTensor()(sample['pose'])\n",
    "        poses.append(pose_tensor * 2 - 1)  # Normalize from [0, 1] to [-1, 1]\n",
    "        \n",
    "        # Caption dropout: 50% chance to use empty caption\n",
    "        caption = sample['captions'][0] if sample['captions'] else \"\"\n",
    "        if random.random() < config.caption_dropout_prob:\n",
    "            caption = \"\"\n",
    "        captions.append(caption)\n",
    "    \n",
    "    return {\n",
    "        \"images\": torch.stack(images),\n",
    "        \"poses\": torch.stack(poses),\n",
    "        \"captions\": captions\n",
    "    }\n",
    "\n",
    "# Create training dataloader with caption dropout\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"DATASET CONFIGURATION\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"‚úÖ Total training samples: {len(train_dataset)} (USING ALL AVAILABLE DATA)\")\n",
    "print(f\"‚úÖ Batches per epoch: {len(train_dataloader)}\")\n",
    "print(f\"‚úÖ Total training iterations: {len(train_dataloader) * config.num_epochs}\")\n",
    "print(f\"‚úÖ Caption dropout: {config.caption_dropout_prob*100:.0f}% of samples use empty captions\")\n",
    "print(f\"‚úÖ Pose conditioning normalized to [-1, 1] range\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6fa31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:45.775173Z",
     "iopub.status.busy": "2025-12-10T19:12:45.775000Z",
     "iopub.status.idle": "2025-12-10T19:13:10.171629Z",
     "shell.execute_reply": "2025-12-10T19:13:10.170933Z",
     "shell.execute_reply.started": "2025-12-10T19:12:45.775159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# Initialize ControlNet from UNet\n",
    "print(\"Initializing ControlNet...\")\n",
    "controlnet = ControlNetModel.from_unet(\n",
    "    unet,\n",
    "    conditioning_channels=config.controlnet_conditioning_channels\n",
    ")\n",
    "\n",
    "# Load noise scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# Freeze VAE and text encoder - we only train ControlNet\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"‚úì Models loaded successfully!\")\n",
    "print(f\"  - ControlNet parameters: {sum(p.numel() for p in controlnet.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  - Text encoder (frozen): {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  - UNet (frozen): {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "\n",
    "# Clear memory after model loading (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:10.172755Z",
     "iopub.status.busy": "2025-12-10T19:13:10.172452Z",
     "iopub.status.idle": "2025-12-10T19:13:10.179244Z",
     "shell.execute_reply": "2025-12-10T19:13:10.178560Z",
     "shell.execute_reply.started": "2025-12-10T19:13:10.172731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "if config.use_8bit_optimizer:\n",
    "    # ‚úÖ 8-bit optimizer from bitsandbytes (memory efficient!)\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"\\n‚úÖ Initializing 8-bit optimizer from bitsandbytes...\")\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        controlnet.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "    print(f\"‚úÖ 8-bit optimizer loaded successfully!\")\n",
    "else:\n",
    "    # Standard AdamW optimizer (no extra dependencies needed)\n",
    "    print(f\"\\n‚úÖ Initializing standard AdamW optimizer...\")\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        controlnet.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "    print(f\"‚úÖ Standard AdamW optimizer loaded successfully!\")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
    "    num_training_steps=len(train_dataloader) * config.num_epochs * config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"OPTIMIZER CONFIGURATION\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"‚úÖ Optimizer type: {'8-bit AdamW (bitsandbytes)' if config.use_8bit_optimizer else 'Standard AdamW'}\")\n",
    "print(f\"‚úÖ Learning rate: {config.learning_rate}\")\n",
    "print(f\"‚úÖ Warmup steps: {config.lr_warmup_steps}\")\n",
    "print(f\"‚úÖ Total training steps: {len(train_dataloader) * config.num_epochs}\")\n",
    "print(f\"‚úÖ Learning rate scheduler: cosine with warmup\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c22c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:10.180425Z",
     "iopub.status.busy": "2025-12-10T19:13:10.180079Z",
     "iopub.status.idle": "2025-12-10T19:13:14.406603Z",
     "shell.execute_reply": "2025-12-10T19:13:14.405910Z",
     "shell.execute_reply.started": "2025-12-10T19:13:10.180402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Accelerator for distributed training and mixed precision\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    log_with=config.report_to,\n",
    "    project_dir=config.logging_dir,\n",
    ")\n",
    "\n",
    "if config.report_to:\n",
    "    tracker_config = {\n",
    "        \"learning_rate\": config.learning_rate,\n",
    "        \"num_epochs\": config.num_epochs,\n",
    "        \"train_batch_size\": config.train_batch_size,\n",
    "        \"grad_accum\": config.gradient_accumulation_steps,\n",
    "        \"num_training_samples\": config.num_training_samples,\n",
    "        \"resolution\": config.resolution,\n",
    "    }\n",
    "    init_kwargs = {}\n",
    "    if \"wandb\" in str(config.report_to):\n",
    "        init_kwargs[\"wandb\"] = {\"name\": config.wandb_run_name} if config.wandb_run_name else {}\n",
    "    accelerator.init_trackers(\n",
    "        project_name=config.wandb_project,\n",
    "        config=tracker_config,\n",
    "        init_kwargs=init_kwargs or None,\n",
    "    )\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if config.gradient_checkpointing:\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "\n",
    "# Prepare models with accelerator\n",
    "controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Move models to device\n",
    "unet.to(accelerator.device)\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "# Set models to eval mode (only ControlNet is in training mode)\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "# ‚úÖ MULTI-GPU SUPPORT: Use DataParallel for multiple GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"\\n‚úÖ MULTI-GPU DETECTED: {num_gpus} GPUs available!\")\n",
    "    print(f\"   GPU Details: {torch.cuda.get_device_name(0)}\")\n",
    "    controlnet = torch.nn.DataParallel(controlnet)\n",
    "    unet = torch.nn.DataParallel(unet)\n",
    "    vae = torch.nn.DataParallel(vae)\n",
    "    text_encoder = torch.nn.DataParallel(text_encoder)\n",
    "    print(f\"   ‚úÖ Models wrapped with DataParallel for distributed training\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ SINGLE GPU MODE: Using {accelerator.device}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"ACCELERATE CONFIGURATION\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"‚úÖ Device: {accelerator.device}\")\n",
    "print(f\"‚úÖ GPUs Available: {num_gpus}\")\n",
    "print(f\"‚úÖ Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"‚úÖ Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n",
    "print(f\"‚úÖ Gradient checkpointing: {'ENABLED' if config.gradient_checkpointing else 'DISABLED'}\")\n",
    "print(f\"\\nüöÄ TRAINING READINESS SUMMARY:\")\n",
    "print(f\"  - Effective Batch Size: {config.train_batch_size * config.gradient_accumulation_steps * num_gpus}\")\n",
    "print(f\"  - Batch per GPU: {config.train_batch_size}\")\n",
    "print(f\"  - GPUs in use: {num_gpus}\")\n",
    "print(f\"  - Total Training Data: {len(train_dataloader) * config.train_batch_size} samples/epoch\")\n",
    "print(f\"  - Memory Optimization: fp16 + gradient checkpointing + multi-GPU\")\n",
    "print(f\"  - Expected Memory per GPU: ~4-6 GB\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de51e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:14.407552Z",
     "iopub.status.busy": "2025-12-10T19:13:14.407342Z",
     "iopub.status.idle": "2025-12-10T19:13:14.422136Z",
     "shell.execute_reply": "2025-12-10T19:13:14.421291Z",
     "shell.execute_reply.started": "2025-12-10T19:13:14.407532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_controlnet():\n",
    "    \"\"\"\n",
    "    Training function with multi-GPU support via DataParallel\n",
    "    Prints epoch-level training loss summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Setup Device & Precision\n",
    "    device = accelerator.device\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "        \n",
    "    # Cast frozen models to weight_dtype for memory efficiency\n",
    "    vae_model = vae.module if isinstance(vae, torch.nn.DataParallel) else vae\n",
    "    text_encoder_model = text_encoder.module if isinstance(text_encoder, torch.nn.DataParallel) else text_encoder\n",
    "    unet_model = unet.module if isinstance(unet, torch.nn.DataParallel) else unet\n",
    "    \n",
    "    vae_model.to(device, dtype=weight_dtype)\n",
    "    text_encoder_model.to(device, dtype=weight_dtype)\n",
    "    unet_model.to(device, dtype=weight_dtype)\n",
    "\n",
    "    # 2. Calculate Steps\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / config.gradient_accumulation_steps)\n",
    "    max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    # 3. Setup Progress Bar\n",
    "    progress_bar = tqdm(\n",
    "        range(max_train_steps), \n",
    "        desc=\"Steps\", \n",
    "        disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    \n",
    "    accelerator.print(f\"\\n{'='*50}\")\n",
    "    accelerator.print(f\"üöÄ TRAINING STARTING (Multi-GPU Enabled)\")\n",
    "    accelerator.print(f\"{'='*50}\")\n",
    "    accelerator.print(f\"Total Epochs: {config.num_epochs}\")\n",
    "    accelerator.print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "    accelerator.print(f\"Precision: {weight_dtype}\")\n",
    "    accelerator.print(\"Validation: disabled during training\")\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        controlnet.train()\n",
    "        train_loss = 0.0\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_loss_count = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                pixel_values = batch[\"images\"].to(device, dtype=torch.float32)\n",
    "                controlnet_image = batch[\"poses\"].to(device, dtype=torch.float32)\n",
    "                captions = batch[\"captions\"]\n",
    "                \n",
    "                with accelerator.autocast():\n",
    "                    # VAE encode\n",
    "                    with torch.no_grad():\n",
    "                        latents = vae_model.encode(pixel_values.to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                        latents = latents * vae_model.config.scaling_factor\n",
    "\n",
    "                    # Add noise\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    bsz = latents.shape[0]\n",
    "                    timesteps = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device\n",
    "                    ).long()\n",
    "\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                    # Text embeddings\n",
    "                    with torch.no_grad():\n",
    "                        inputs = tokenizer(\n",
    "                            captions, \n",
    "                            max_length=tokenizer.model_max_length, \n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\"\n",
    "                        )\n",
    "                        encoder_hidden_states = text_encoder_model(inputs.input_ids.to(device))[0]\n",
    "\n",
    "                    # ControlNet forward\n",
    "                    down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        controlnet_cond=controlnet_image,\n",
    "                        return_dict=False,\n",
    "                    )\n",
    "\n",
    "                    # UNet forward\n",
    "                    model_pred = unet_model(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        down_block_additional_residuals=down_block_res_samples,\n",
    "                        mid_block_additional_residual=mid_block_res_sample,\n",
    "                    ).sample\n",
    "\n",
    "                    # Loss\n",
    "                    loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "                # Backprop\n",
    "                avg_loss = accelerator.gather(loss.repeat(config.train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item() / config.gradient_accumulation_steps\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # Accumulate epoch loss summary\n",
    "                    epoch_loss_sum += avg_loss.item()\n",
    "                    epoch_loss_count += 1\n",
    "                    \n",
    "                    logs = {\"loss\": train_loss, \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                    progress_bar.set_postfix(**logs)\n",
    "                    accelerator.log(logs, step=global_step)\n",
    "                    train_loss = 0.0\n",
    "\n",
    "                    if hasattr(config, 'checkpointing_steps') and global_step % config.checkpointing_steps == 0:\n",
    "                         save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                         accelerator.save_state(save_path)\n",
    "\n",
    "        # End of Epoch\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            # Print epoch-level training loss summary\n",
    "            if epoch_loss_count > 0:\n",
    "                epoch_loss_avg = epoch_loss_sum / epoch_loss_count\n",
    "                accelerator.print(f\"üìâ Epoch {epoch+1}/{config.num_epochs} - Training Loss: {epoch_loss_avg:.6f}\")\n",
    "                # Log epoch loss for dashboards (e.g., TensorBoard/W&B)\n",
    "                accelerator.log({\"train_loss_epoch\": epoch_loss_avg}, step=global_step)\n",
    "\n",
    "            # Get the unwrapped model for saving\n",
    "            controlnet_unwrapped = controlnet.module if isinstance(controlnet, torch.nn.DataParallel) else controlnet\n",
    "            save_path = os.path.join(config.output_dir, f\"epoch-{epoch+1}\")\n",
    "            controlnet_unwrapped.save_pretrained(save_path)\n",
    "            accelerator.print(f\"‚úÖ Epoch {epoch+1} Saved: {save_path}\")\n",
    "            \n",
    "            # Auto-cleanup: Delete previous epoch checkpoint\n",
    "            if epoch > 0:\n",
    "                prev_epoch_path = os.path.join(config.output_dir, f\"epoch-{epoch}\")\n",
    "                if os.path.exists(prev_epoch_path):\n",
    "                    import shutil\n",
    "                    shutil.rmtree(prev_epoch_path)\n",
    "                    accelerator.print(f\"üßπ Cleaned up previous checkpoint: epoch-{epoch}\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Final Save\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        controlnet_unwrapped = controlnet.module if isinstance(controlnet, torch.nn.DataParallel) else controlnet\n",
    "        final_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "        controlnet_unwrapped.save_pretrained(final_path)\n",
    "        accelerator.print(f\"üéâ Training Complete! Final model saved to {final_path}\")\n",
    "\n",
    "    accelerator.end_training()\n",
    "    \n",
    "    return os.path.join(config.output_dir, \"controlnet_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfcf90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:14.424206Z",
     "iopub.status.busy": "2025-12-10T19:13:14.423753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "trained_controlnet = train_controlnet()\n",
    "\n",
    "print(f\"\\n‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e2964",
   "metadata": {},
   "source": [
    "## Test the Trained ControlNet\n",
    "\n",
    "Generate images using the trained ControlNet with pose skeleton conditioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346c3da",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the trained ControlNet and create pipeline\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Clear memory before inference (important for Kaggle)\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading trained ControlNet pipeline...\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved ControlNet\n",
    "\n",
    "controlnet_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "\n",
    "controlnet_trained = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "# Create inference pipeline\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "\n",
    "    controlnet=controlnet_trained,\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "\n",
    "    safety_checker=None,\n",
    "\n",
    ")\n",
    "\n",
    "pipe = pipe.to(accelerator.device)\n",
    "\n",
    "\n",
    "\n",
    "# Enable memory efficient attention (optional, if xformers is available)\n",
    "\n",
    "try:\n",
    "\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    print(\"‚úì XFormers memory efficient attention enabled\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"‚ö†Ô∏è  XFormers not available, using default attention: {e}\")\n",
    "\n",
    "    print(\"   (This is fine, just uses a bit more memory)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úì Pipeline ready for inference!\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Quick validation preview on a single held-out sample ---\n",
    "\n",
    "# Use validation dataset, not training, to avoid optimistic bias\n",
    "\n",
    "val_sample = val_dataset[0]\n",
    "\n",
    "val_pose = val_sample['pose']\n",
    "\n",
    "val_caption = val_sample['captions'][0] if val_sample['captions'] else \"a person\"\n",
    "\n",
    "\n",
    "\n",
    "# Convert pose to tensor for pipeline input\n",
    "\n",
    "val_pose_tensor = transforms.ToTensor()(val_pose)\n",
    "\n",
    "val_pose_input = val_pose_tensor.unsqueeze(0).to(accelerator.device, dtype=torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "# Generate one preview image using the trained controlnet + validation pose\n",
    "\n",
    "try:\n",
    "\n",
    "    preview_image = pipe(\n",
    "        prompt=val_caption,\n",
    "\n",
    "        image=val_pose_input,\n",
    "\n",
    "        num_inference_steps=20,\n",
    "\n",
    "        guidance_scale=7.5,\n",
    "\n",
    "    ).images[0]\n",
    "\n",
    "    print(\"‚úì Generated a validation preview image\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    preview_image = None\n",
    "\n",
    "    print(f\"‚ö†Ô∏è  Preview generation failed: {str(e)[:80]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Visualize original image, pose, and generated preview\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].imshow(val_sample['image'].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\n",
    "axes[0].set_title('Validation: Original Image')\n",
    "\n",
    "axes[0].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "axes[1].imshow(val_pose_tensor.squeeze().cpu().numpy(), cmap='gray')\n",
    "\n",
    "axes[1].set_title('Validation: Pose Skeleton')\n",
    "\n",
    "axes[1].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "axes[2].imshow(preview_image if preview_image is not None else np.zeros((256, 256, 3)))\n",
    "\n",
    "axes[2].set_title('Validation: Generated Image' if preview_image is not None else 'Generation Failed')\n",
    "\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6f7f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Trained ControlNet on 25 Validation Images\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import cv2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING TRAINED CONTROLNET ON VALIDATION IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Number of images to test\n",
    "num_test_images = min(25, len(val_dataset))\n",
    "print(f\"\\nGenerating results for {num_test_images} validation images...\")\n",
    "print(f\"Each will show: [Original Image] [Pose Skeleton] [Generated Image]\\n\")\n",
    "\n",
    "# Create figure with subplots for all results\n",
    "fig = plt.figure(figsize=(20, 5 * num_test_images))\n",
    "gs = GridSpec(num_test_images, 3, figure=fig, hspace=0.4, wspace=0.2)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "generator = torch.Generator(device=accelerator.device).manual_seed(42)\n",
    "\n",
    "generated_images = []\n",
    "test_results = []\n",
    "\n",
    "# Process each validation image\n",
    "for idx in range(num_test_images):\n",
    "    print(f\"Processing image {idx+1}/{num_test_images}...\")\n",
    "    \n",
    "    # Get validation sample\n",
    "    test_sample = val_dataset[idx]\n",
    "    test_image = test_sample['image']\n",
    "    test_pose = test_sample['pose']\n",
    "    test_caption = test_sample['captions'][0] if test_sample['captions'] else \"a person standing\"\n",
    "    \n",
    "    # Store for results\n",
    "    test_results.append({\n",
    "        'caption': test_caption,\n",
    "        'image_id': test_sample['image_id'],\n",
    "        'keypoints': test_sample['raw_keypoints']\n",
    "    })\n",
    "    \n",
    "    # Convert pose to tensor\n",
    "    test_pose_tensor = transforms.ToTensor()(test_pose)\n",
    "    \n",
    "    # Prepare pose input (single channel, normalized to [-1, 1])\n",
    "    test_pose_input = test_pose_tensor.unsqueeze(0).to(accelerator.device, dtype=torch.float16)\n",
    "    \n",
    "    # Generate image\n",
    "    try:\n",
    "        output = pipe(\n",
    "            prompt=test_caption,\n",
    "            image=test_pose_input,\n",
    "            num_inference_steps=20,\n",
    "            generator=generator,\n",
    "            guidance_scale=7.5,\n",
    "        ).images[0]\n",
    "        generated_images.append(output)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error generating image: {str(e)[:50]}\")\n",
    "        generated_images.append(None)\n",
    "        continue\n",
    "    \n",
    "    # --- Plot Results for this image ---\n",
    "    \n",
    "    # Column 1: Original Image\n",
    "    ax1 = fig.add_subplot(gs[idx, 0])\n",
    "    orig_img = test_image.permute(1, 2, 0).cpu().numpy()\n",
    "    orig_img = (orig_img * 0.5 + 0.5)\n",
    "    orig_img = np.clip(orig_img, 0, 1)\n",
    "    ax1.imshow(orig_img)\n",
    "    ax1.set_title(f'Image {idx+1}: Original', fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Column 2: Pose Skeleton\n",
    "    ax2 = fig.add_subplot(gs[idx, 1])\n",
    "    ax2.imshow(test_pose_tensor.squeeze().cpu().numpy(), cmap='gray')\n",
    "    num_keypoints = (test_sample['raw_keypoints'][:, 2] > 0).sum()\n",
    "    ax2.set_title(f'Pose Skeleton\\n({int(num_keypoints)} keypoints visible)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Column 3: Generated Image\n",
    "    ax3 = fig.add_subplot(gs[idx, 2])\n",
    "    if generated_images[idx] is not None:\n",
    "        ax3.imshow(generated_images[idx])\n",
    "        ax3.set_title(f'Generated Image', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Generation Failed', ha='center', va='center', \n",
    "                fontsize=14, color='red', fontweight='bold')\n",
    "        ax3.set_title('Generated Image', fontsize=12, fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Add caption as suptitle for this row\n",
    "    caption_text = test_caption if len(test_caption) <= 60 else test_caption[:57] + \"...\"\n",
    "    fig.text(0.5, 0.98 - (idx * (1/num_test_images)) - 0.015, \n",
    "            f'Caption: \"{caption_text}\"', \n",
    "            ha='center', fontsize=10, style='italic', alpha=0.7)\n",
    "\n",
    "plt.suptitle(f'ControlNet Validation Results - {num_test_images} Images\\n' + \n",
    "             'Original Image | Pose Skeleton (from COCO) | Generated Image', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_generations = sum(1 for img in generated_images if img is not None)\n",
    "print(f\"‚úì Successfully generated: {successful_generations}/{num_test_images} images\")\n",
    "print(f\"‚úì Success rate: {100*successful_generations/num_test_images:.1f}%\")\n",
    "\n",
    "print(f\"\\nValidation Captions & Keypoints:\")\n",
    "for i, result in enumerate(test_results[:5]):  # Show first 5 as examples\n",
    "    print(f\"  Image {i+1}: '{result['caption'][:50]}...' ({int(result['keypoints'][:, 2].sum())} keypoints)\")\n",
    "\n",
    "print(f\"\\n‚úì Validation testing complete!\")\n",
    "print(f\"‚úì All {num_test_images} validation images tested with their COCO poses and custom captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967545f5",
   "metadata": {},
   "source": [
    "Saving  the Model Weights"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8887620,
     "sourceId": 14095779,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
