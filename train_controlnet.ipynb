{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77372c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:23.436576Z",
     "iopub.status.busy": "2025-12-10T19:12:23.435957Z",
     "iopub.status.idle": "2025-12-10T19:12:23.441817Z",
     "shell.execute_reply": "2025-12-10T19:12:23.441213Z",
     "shell.execute_reply.started": "2025-12-10T19:12:23.436552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úì Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"‚úì Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e753de9",
   "metadata": {},
   "source": [
    "# Memory Optimization for Kaggle\n",
    "\n",
    "**Important Notes for Kaggle Execution:**\n",
    "- Kaggle has stricter memory limits than Colab\n",
    "- Clear GPU cache between major operations\n",
    "- Use smaller batch sizes if OOM occurs\n",
    "- Monitor memory usage with `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fbd1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:23.648737Z",
     "iopub.status.busy": "2025-12-10T19:12:23.648543Z",
     "iopub.status.idle": "2025-12-10T19:12:34.620924Z",
     "shell.execute_reply": "2025-12-10T19:12:34.620230Z",
     "shell.execute_reply.started": "2025-12-10T19:12:23.648723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import zipfile\n",
    "\n",
    "class COCOPoseDataset(Dataset):\n",
    "    def __init__(self, root_dir='./coco_data', split='train', transform=None, image_size=512, \n",
    "                 custom_captions_file=None, max_samples=None, download=True):\n",
    "        \"\"\"\n",
    "        Custom Dataset with COCO Pose Skeletons + Your Custom Captions\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory to store COCO data\n",
    "            split (str): 'train' or 'val'\n",
    "            transform: Optional transform to be applied on images\n",
    "            image_size (int): Size to resize images to\n",
    "            custom_captions_file (str): Path to JSON file with custom captions {image_name: caption}\n",
    "            max_samples (int): Optional limit on number of samples to load\n",
    "            download (bool): Whether to download COCO annotations if not found\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        self.custom_captions_file = custom_captions_file\n",
    "        \n",
    "        # COCO 2017 URLs\n",
    "        self.annotation_urls = {\n",
    "            'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "            'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
    "        }\n",
    "        \n",
    "        # Setup paths\n",
    "        self.ann_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.img_dir = os.path.join(root_dir, f'{split}2017')\n",
    "        \n",
    "        split_name = 'train' if split == 'train' else 'val'\n",
    "        self.ann_file = os.path.join(self.ann_dir, f'person_keypoints_{split_name}2017.json')\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.ann_dir, exist_ok=True)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        \n",
    "        # Load custom captions - REQUIRED\n",
    "        self.custom_captions = None\n",
    "        self.custom_caption_map = {}\n",
    "        self.img_ids = []\n",
    "        \n",
    "        if not custom_captions_file:\n",
    "            raise ValueError(f\"‚ùå custom_captions_file is REQUIRED! Please provide a JSON file with captions.\")\n",
    "        \n",
    "        # Check if caption file exists (try absolute and relative paths)\n",
    "        caption_path = custom_captions_file\n",
    "        if not os.path.exists(caption_path):\n",
    "            # Try absolute path if relative doesn't work\n",
    "            caption_path = os.path.abspath(custom_captions_file)\n",
    "        \n",
    "        if not os.path.exists(caption_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Caption file not found: {custom_captions_file}\\n\"\n",
    "                f\"Tried paths:\\n\"\n",
    "                f\"  - {custom_captions_file}\\n\"\n",
    "                f\"  - {os.path.abspath(custom_captions_file)}\\n\"\n",
    "                f\"Current working directory: {os.getcwd()}\\n\"\n",
    "                f\"Please ensure the caption file exists with format: {{'image_filename.jpg': 'caption text', ...}}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading custom captions from {caption_path}...\")\n",
    "        with open(caption_path, 'r') as f:\n",
    "            self.custom_captions = json.load(f)\n",
    "        print(f\"‚úì Loaded {len(self.custom_captions)} custom captions\")\n",
    "        \n",
    "        # Download COCO annotations if needed\n",
    "        if download and not os.path.exists(self.ann_file):\n",
    "            print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n",
    "            self._download_annotations()\n",
    "        \n",
    "        # Check if annotation file exists\n",
    "        if not os.path.exists(self.ann_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Annotation file not found: {self.ann_file}\\n\"\n",
    "                f\"Please download COCO 2017 annotations from:\\n\"\n",
    "                f\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\\n\"\n",
    "                f\"Extract to: {self.ann_dir}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading COCO {split} pose annotations...\")\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        \n",
    "        # Map images from caption file\n",
    "        print(\"Setting up dataset with YOUR images, poses from COCO, and YOUR captions...\")\n",
    "        items_to_process = list(self.custom_captions.items())\n",
    "        if max_samples:\n",
    "            items_to_process = items_to_process[:max_samples]\n",
    "        \n",
    "        for img_filename, caption in items_to_process:\n",
    "            # Extract image ID from filename (e.g., '000000391895.jpg' -> 391895)\n",
    "            img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n",
    "            self.img_ids.append(img_id)\n",
    "            self.custom_caption_map[img_id] = caption\n",
    "        \n",
    "        limit_msg = f\" (limited to first {max_samples})\" if max_samples else \"\"\n",
    "        print(f\"‚úì Dataset ready with {len(self.img_ids)} images\")\n",
    "        print(f\"  - Images from: {self.img_dir}\")\n",
    "        print(f\"  - Captions from: {custom_captions_file}{limit_msg}\")\n",
    "        print(f\"  - Poses from: COCO person_keypoints annotations\")\n",
    "        print(f\"‚úÖ Using ACTUAL POSE SKELETONS (stick figures) as conditioning!\\n\")\n",
    "        \n",
    "        if len(self.img_ids) == 0:\n",
    "            print(\"\\n‚ùå ERROR: No images found in caption file!\")\n",
    "    \n",
    "    def _download_annotations(self):\n",
    "        \"\"\"Download COCO annotations\"\"\"\n",
    "        url = self.annotation_urls[self.split]\n",
    "        zip_path = os.path.join(self.root_dir, 'annotations.zip')\n",
    "        \n",
    "        print(f\"Downloading from {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(zip_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        \n",
    "        print(\"Extracting annotations...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(\"‚úì Annotations downloaded successfully!\")\n",
    "    \n",
    "    def _download_image(self, img_id, img_filename):\n",
    "        \"\"\"Download a single image from COCO dataset on-the-fly\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        # If image already exists, skip download\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "        \n",
    "        # Get image info from COCO API\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info['coco_url']\n",
    "        \n",
    "        # Download image\n",
    "        try:\n",
    "            response = requests.get(img_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save image\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return img_path\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to download image {img_filename} from {img_url}: {str(e)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_filename = list(self.custom_captions.keys())[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        # Download image if it doesn't exist (ON-THE-FLY DOWNLOAD)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"üì• Downloading image: {img_filename}...\")\n",
    "            img_path = self._download_image(img_id, img_filename)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Get caption\n",
    "        caption = self.custom_caption_map[img_id]\n",
    "        \n",
    "        # ‚úÖ EXTRACT ACTUAL POSE KEYPOINTS FROM COCO\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.coco.getCatIds(catNms=['person']), iscrowd=False)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Get pose keypoints from first person with visible keypoints\n",
    "        keypoints = None\n",
    "        for ann in anns:\n",
    "            if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "                keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "                break\n",
    "        \n",
    "        if keypoints is None:\n",
    "            keypoints = np.zeros((17, 3))\n",
    "        \n",
    "        # ‚úÖ CREATE ACTUAL POSE SKELETON (stick figure from keypoints)\n",
    "        pose_skeleton = self.create_pose_skeleton(keypoints, width, height)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5])\n",
    "            ])(image)\n",
    "        \n",
    "        pose_map = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "        ])(pose_skeleton)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'pose': pose_map,\n",
    "            'raw_keypoints': keypoints,\n",
    "            'image_id': img_id,\n",
    "            'captions': [caption]  # Your custom caption\n",
    "        }\n",
    "    \n",
    "    def create_pose_skeleton(self, keypoints, width, height):\n",
    "        \"\"\"\n",
    "        Create ACTUAL human pose skeleton from COCO keypoint annotations\n",
    "        Draws a stick figure with bones connecting the 17 joints\n",
    "        \n",
    "        COCO Keypoints (17 total):\n",
    "        0: nose, 1: L eye, 2: R eye, 3: L ear, 4: R ear,\n",
    "        5: L shoulder, 6: R shoulder, 7: L elbow, 8: R elbow, 9: L wrist, 10: R wrist,\n",
    "        11: L hip, 12: R hip, 13: L knee, 14: R knee, 15: L ankle, 16: R ankle\n",
    "        \"\"\"\n",
    "        # Create black canvas\n",
    "        pose_img = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # COCO keypoint skeleton connections (bones linking joints)\n",
    "        skeleton = [\n",
    "            (0, 1), (0, 2),           # nose to eyes\n",
    "            (1, 3), (2, 4),           # eyes to ears\n",
    "            (0, 5), (0, 6),           # nose to shoulders\n",
    "            (5, 7), (7, 9),           # left arm (shoulder‚Üíelbow‚Üíwrist)\n",
    "            (6, 8), (8, 10),          # right arm (shoulder‚Üíelbow‚Üíwrist)\n",
    "            (5, 11), (6, 12),         # shoulders to hips\n",
    "            (11, 12),                 # hip to hip\n",
    "            (11, 13), (13, 15),       # left leg (hip‚Üíknee‚Üíankle)\n",
    "            (12, 14), (14, 16)        # right leg (hip‚Üíknee‚Üíankle)\n",
    "        ]\n",
    "        \n",
    "        # Line thickness and circle radius scale with image size\n",
    "        line_thickness = max(2, int(min(width, height) / 100))\n",
    "        circle_radius = max(3, int(min(width, height) / 80))\n",
    "        \n",
    "        # Draw bones (connections) FIRST\n",
    "        for start_idx, end_idx in skeleton:\n",
    "            if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
    "                x1, y1, v1 = keypoints[start_idx]\n",
    "                x2, y2, v2 = keypoints[end_idx]\n",
    "                \n",
    "                # Draw line ONLY if both keypoints are visible (v > 0)\n",
    "                if v1 > 0 and v2 > 0:\n",
    "                    cv2.line(pose_img, (int(x1), int(y1)), (int(x2), int(y2)), \n",
    "                            255, line_thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw keypoint circles ON TOP of bones\n",
    "        for i, (x, y, v) in enumerate(keypoints):\n",
    "            if v > 0:  # Only draw visible keypoints\n",
    "                cv2.circle(pose_img, (int(x), int(y)), circle_radius, 255, -1)\n",
    "        \n",
    "        return pose_img\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUSTOM DATASET LOADER WITH ACTUAL POSE SKELETONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Pipeline:\")\n",
    "print(\"1. ‚úÖ Downloads COCO images on-the-fly (from COCO CDN)\")\n",
    "print(\"2. ‚úÖ Loads YOUR custom captions\")\n",
    "print(\"3. ‚úÖ Extracts ACTUAL pose keypoints from COCO annotations\")\n",
    "print(\"4. ‚úÖ Draws stick figures (nose‚Üíeyes‚Üíears, shoulders‚Üíelbows‚Üíwrists, etc.)\")\n",
    "print(\"5. ‚úÖ Passes: image + caption + pose skeleton to training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets with COCO POSES + YOUR CUSTOM CAPTIONS\n",
    "train_dataset = COCOPoseDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=512,\n",
    "    custom_captions_file='/kaggle/input/captions/train_captions.json',\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = COCOPoseDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='val',\n",
    "    image_size=512,\n",
    "    custom_captions_file='/kaggle/input/captions/val_captions.json',\n",
    "    max_samples=None,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)} (images + your captions + COCO poses)\")\n",
    "print(f\"Validation samples: {len(val_dataset)} (images + your captions + COCO poses)\")\n",
    "\n",
    "# Show a sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample = train_dataset[0]\n",
    "    if sample['captions']:\n",
    "        print(f\"\\nüìù Sample caption from training:\")\n",
    "        caption = sample['captions'][0]\n",
    "        print(f\"   \\\"{caption[:200]}...\\\"\" if len(caption) > 200 else f\"   \\\"{caption}\\\"\")\n",
    "    print(f\"ü¶¥ Sample has {(sample['raw_keypoints'][:, 2] > 0).sum()} visible pose keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c4e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:34.622584Z",
     "iopub.status.busy": "2025-12-10T19:12:34.622339Z",
     "iopub.status.idle": "2025-12-10T19:12:35.852398Z",
     "shell.execute_reply": "2025-12-10T19:12:35.851543Z",
     "shell.execute_reply.started": "2025-12-10T19:12:34.622568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one sample from the training dataset\n",
    "sample = train_dataset[0]\n",
    "img_id = sample['image_id']\n",
    "\n",
    "# Get annotations for this image from COCO\n",
    "ann_ids = train_dataset.coco.getAnnIds(imgIds=img_id)\n",
    "anns = train_dataset.coco.loadAnns(ann_ids)\n",
    "img_info = train_dataset.coco.loadImgs(img_id)[0]\n",
    "\n",
    "# Get captions (text prompts)\n",
    "captions = sample['captions']\n",
    "\n",
    "# Build metadata text\n",
    "metadata = f\"Image ID: {img_id}\\nFilename: {img_info['file_name']}\\n\"\n",
    "metadata += f\"Size: {img_info['width']}x{img_info['height']}\\n\"\n",
    "metadata += f\"Person annotations: {len([a for a in anns if a.get('category_id') == 1])}\\n\"\n",
    "\n",
    "for i, ann in enumerate(anns):\n",
    "    if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n",
    "        metadata += f\"\\nPerson {i+1}: {ann['num_keypoints']} keypoints\"\n",
    "        if 'area' in ann:\n",
    "            metadata += f\", area: {int(ann['area'])}\"\n",
    "        break\n",
    "\n",
    "# Convert tensors back to displayable format\n",
    "image = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "image = (image * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "# Convert PIL Image to numpy array\n",
    "pose_map = np.array(sample['pose'])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Original image (larger)\n",
    "ax1 = fig.add_subplot(gs[0:2, 0])\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Pose skeleton\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(pose_map, cmap='gray')\n",
    "ax2.set_title('Pose Skeleton', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(image)\n",
    "ax3.imshow(pose_map, cmap='hot', alpha=0.5)\n",
    "ax3.set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Text Captions/Prompts - TOP SECTION\n",
    "ax_captions = fig.add_subplot(gs[1, 1:3])\n",
    "ax_captions.axis('off')\n",
    "ax_captions.text(0.05, 0.95, 'üìù TEXT PROMPTS (Custom Captions):', \n",
    "                fontsize=13, fontweight='bold', va='top')\n",
    "\n",
    "if captions:\n",
    "    caption_text = \"\\n\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
    "else:\n",
    "    caption_text = \"No captions available for this image.\"\n",
    "\n",
    "ax_captions.text(0.05, 0.80, caption_text, \n",
    "                fontsize=10, va='top', wrap=True,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Metadata\n",
    "ax_meta = fig.add_subplot(gs[2, 0])\n",
    "ax_meta.axis('off')\n",
    "ax_meta.text(0.05, 0.95, 'Image Metadata:', \n",
    "            fontsize=12, fontweight='bold', va='top')\n",
    "ax_meta.text(0.05, 0.75, metadata, \n",
    "            fontsize=10, va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Pose keypoint details\n",
    "ax_kp = fig.add_subplot(gs[2, 1:3])\n",
    "ax_kp.axis('off')\n",
    "\n",
    "keypoint_names = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "keypoint_text = \"Keypoint Details:\\n\" + \"‚îÄ\" * 40 + \"\\n\"\n",
    "for i, (x, y, v) in enumerate(sample['raw_keypoints']):\n",
    "    if v > 0:  # visible keypoint\n",
    "        visibility = \"visible\" if v == 2 else \"occluded\"\n",
    "        keypoint_text += f\"{keypoint_names[i]:15s}: ({int(x):3d}, {int(y):3d}) - {visibility}\\n\"\n",
    "\n",
    "ax_kp.text(0.05, 0.95, keypoint_text, \n",
    "          fontsize=9, va='top', family='monospace')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Image shape: {sample['image'].shape}\")\n",
    "print(f\"Pose skeleton shape: {pose_map.shape}\")\n",
    "print(f\"Number of visible keypoints: {(sample['raw_keypoints'][:, 2] > 0).sum()}\")\n",
    "print(f\"Number of captions: {len(captions)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0ae6",
   "metadata": {},
   "source": [
    "# ControlNet Training Setup\n",
    "\n",
    "This section sets up and trains a ControlNet model for pose-guided image generation using:\n",
    "- **Spatial Conditioning**: Pose skeleton (stick figure from COCO keypoints)\n",
    "- **Text Conditioning**: Your Custom Captions\n",
    "- **Base Model**: Stable Diffusion v1.5\n",
    "\n",
    "The training uses your custom-captioned images with COCO pose annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28baec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.853428Z",
     "iopub.status.busy": "2025-12-10T19:12:35.853217Z",
     "iopub.status.idle": "2025-12-10T19:12:35.858717Z",
     "shell.execute_reply": "2025-12-10T19:12:35.857838Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.853410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install diffusers transformers accelerate xformers safetensors tensorboard\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3819a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.859714Z",
     "iopub.status.busy": "2025-12-10T19:12:35.859455Z",
     "iopub.status.idle": "2025-12-10T19:12:35.874798Z",
     "shell.execute_reply": "2025-12-10T19:12:35.874019Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.859691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    # Model settings\n",
    "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "    controlnet_conditioning_channels = 1  # Grayscale pose skeleton\n",
    "    \n",
    "    # Training settings\n",
    "    num_training_samples = None  # Set to None to use all images, or set to a number (e.g., 1000) to limit\n",
    "    num_epochs = 10\n",
    "    train_batch_size = 1  # Reduced for Kaggle memory constraints\n",
    "    gradient_accumulation_steps = 8  # Increased to maintain effective batch size of 8\n",
    "    learning_rate = 1e-5\n",
    "    lr_warmup_steps = 500\n",
    "    \n",
    "    # Image settings\n",
    "    resolution = 512\n",
    "    \n",
    "    # Checkpointing\n",
    "    output_dir = \"./controlnet_pose_output\"\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir = \"./logs\"\n",
    "    report_to = \"tensorboard\"\n",
    "    \n",
    "    # Hardware\n",
    "    mixed_precision = \"fp16\"  # Use \"bf16\" if available, \"no\" for CPU\n",
    "    gradient_checkpointing = True\n",
    "    \n",
    "    # Validation\n",
    "    validation_steps = 500\n",
    "    num_validation_images = 4\n",
    "    validation_prompt = \"a person standing\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Training {config.num_training_samples} samples for {config.num_epochs} epochs\")\n",
    "print(f\"Batch size: {config.train_batch_size}, Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {config.train_batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389b625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:35.876988Z",
     "iopub.status.busy": "2025-12-10T19:12:35.876791Z",
     "iopub.status.idle": "2025-12-10T19:12:45.774266Z",
     "shell.execute_reply": "2025-12-10T19:12:45.773631Z",
     "shell.execute_reply.started": "2025-12-10T19:12:35.876973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "# ‚úÖ Control everything through config.num_training_samples and config.num_epochs\n",
    "print(f\"Creating training dataset (max samples: {config.num_training_samples})...\")\n",
    "\n",
    "train_dataset_full = COCOPoseDataset(\n",
    "    root_dir='./coco_data',\n",
    "    split='train',\n",
    "    image_size=config.resolution,\n",
    "    custom_captions_file='/kaggle/input/captions/train_captions.json',\n",
    "    max_samples=config.num_training_samples,  # ‚Üê Control training size here\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create dataloader with collate function\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function to handle captions\"\"\"\n",
    "    images = torch.stack([example['image'] for example in examples])\n",
    "    \n",
    "    # Convert PIL Images to tensors for pose maps\n",
    "    pose_tensors = []\n",
    "    for example in examples:\n",
    "        pose_pil = example['pose']\n",
    "        # Convert PIL to tensor and add channel dimension\n",
    "        pose_tensor = transforms.ToTensor()(pose_pil)\n",
    "        pose_tensors.append(pose_tensor)\n",
    "    poses = torch.stack(pose_tensors)\n",
    "    \n",
    "    # Get first caption for each image (COCO has multiple captions per image)\n",
    "    captions = []\n",
    "    for example in examples:\n",
    "        if example['captions'] and len(example['captions']) > 0:\n",
    "            captions.append(example['captions'][0])\n",
    "        else:\n",
    "            captions.append(\"a person\")  # Fallback caption\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'poses': poses,\n",
    "        'captions': captions\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_full,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training dataset ready: {len(train_dataset_full)} samples\")\n",
    "print(f\"‚úì Total batches per epoch: {len(train_dataloader)}\")\n",
    "\n",
    "# Clear memory after dataset creation (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6fa31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:12:45.775173Z",
     "iopub.status.busy": "2025-12-10T19:12:45.775000Z",
     "iopub.status.idle": "2025-12-10T19:13:10.171629Z",
     "shell.execute_reply": "2025-12-10T19:13:10.170933Z",
     "shell.execute_reply.started": "2025-12-10T19:12:45.775159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_model_name, \n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# Initialize ControlNet from UNet\n",
    "print(\"Initializing ControlNet...\")\n",
    "controlnet = ControlNetModel.from_unet(\n",
    "    unet,\n",
    "    conditioning_channels=config.controlnet_conditioning_channels\n",
    ")\n",
    "\n",
    "# Load noise scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# Freeze VAE and text encoder - we only train ControlNet\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"‚úì Models loaded successfully!\")\n",
    "print(f\"  - ControlNet parameters: {sum(p.numel() for p in controlnet.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  - Text encoder (frozen): {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  - UNet (frozen): {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "\n",
    "# Clear memory after model loading (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:10.172755Z",
     "iopub.status.busy": "2025-12-10T19:13:10.172452Z",
     "iopub.status.idle": "2025-12-10T19:13:10.179244Z",
     "shell.execute_reply": "2025-12-10T19:13:10.178560Z",
     "shell.execute_reply.started": "2025-12-10T19:13:10.172731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
    "    num_training_steps=len(train_dataloader) * config.num_epochs * config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Optimizer configured\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Warmup steps: {config.lr_warmup_steps}\")\n",
    "print(f\"  - Total training steps: {len(train_dataloader) * config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c22c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:10.180425Z",
     "iopub.status.busy": "2025-12-10T19:13:10.180079Z",
     "iopub.status.idle": "2025-12-10T19:13:14.406603Z",
     "shell.execute_reply": "2025-12-10T19:13:14.405910Z",
     "shell.execute_reply.started": "2025-12-10T19:13:10.180402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Accelerator for distributed training and mixed precision\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    log_with=config.report_to,\n",
    "    project_dir=config.logging_dir,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if config.gradient_checkpointing:\n",
    "    controlnet.enable_gradient_checkpointing()\n",
    "\n",
    "# Prepare models with accelerator\n",
    "controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Move models to device\n",
    "unet.to(accelerator.device)\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "# Set models to eval mode (only ControlNet is in training mode)\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "print(f\"‚úì Accelerator initialized\")\n",
    "print(f\"  - Device: {accelerator.device}\")\n",
    "print(f\"  - Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"  - Distributed: {accelerator.num_processes} process(es)\")\n",
    "print(f\"  - Gradient accumulation: {config.gradient_accumulation_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de51e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:14.407552Z",
     "iopub.status.busy": "2025-12-10T19:13:14.407342Z",
     "iopub.status.idle": "2025-12-10T19:13:14.422136Z",
     "shell.execute_reply": "2025-12-10T19:13:14.421291Z",
     "shell.execute_reply.started": "2025-12-10T19:13:14.407532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_controlnet():\n",
    "    \"\"\"\n",
    "    Fixed Training Function\n",
    "    Changes:\n",
    "    1. Keeps inputs in float32 (fixes the c10::Half error).\n",
    "    2. Casts frozen models (VAE, Text Encoder, UNet) to fp16 to save memory.\n",
    "    3. Uses accelerator.autocast() context explicitly to be safe.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Setup Device & Precision\n",
    "    device = accelerator.device\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "        \n",
    "    # IMPORTANT: Cast frozen models to fp16 to save memory and avoid type mismatches\n",
    "    # The ControlNet itself stays fp32 (managed by accelerator)\n",
    "    vae.to(device, dtype=weight_dtype)\n",
    "    text_encoder.to(device, dtype=weight_dtype)\n",
    "    unet.to(device, dtype=weight_dtype)\n",
    "\n",
    "    # 2. Calculate Steps\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / config.gradient_accumulation_steps)\n",
    "    max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    # 3. Setup Progress Bar\n",
    "    progress_bar = tqdm(\n",
    "        range(max_train_steps), \n",
    "        desc=\"Steps\", \n",
    "        disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    \n",
    "    accelerator.print(f\"\\n{'='*40}\")\n",
    "    accelerator.print(f\"üöÄ TRAINING STARTING (Fixed Precision)\")\n",
    "    accelerator.print(f\"{'='*40}\")\n",
    "    accelerator.print(f\"Total Epochs: {config.num_epochs}\")\n",
    "    accelerator.print(f\"Precision: {weight_dtype} (Frozen models cast to this)\")\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        controlnet.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                # -----------------------------------------------------------\n",
    "                # FIX: Load inputs as float32. \n",
    "                # Autocast will handle the conversion to fp16 for operations.\n",
    "                # -----------------------------------------------------------\n",
    "                pixel_values = batch[\"images\"].to(device, dtype=torch.float32)\n",
    "                controlnet_image = batch[\"poses\"].to(device, dtype=torch.float32)\n",
    "                captions = batch[\"captions\"]\n",
    "                \n",
    "                # Use autocast for mixed precision\n",
    "                with accelerator.autocast():\n",
    "                    # A. Encode Images (VAE is frozen and fp16, so we cast input just for VAE)\n",
    "                    # We cast pixel_values to weight_dtype ONLY for VAE encoding\n",
    "                    with torch.no_grad():\n",
    "                        latents = vae.encode(pixel_values.to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                    # B. Add Noise\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    bsz = latents.shape[0]\n",
    "                    timesteps = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device\n",
    "                    ).long()\n",
    "\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                    # C. Text Embeddings\n",
    "                    with torch.no_grad():\n",
    "                        inputs = tokenizer(\n",
    "                            captions, \n",
    "                            max_length=tokenizer.model_max_length, \n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\"\n",
    "                        )\n",
    "                        # Text encoder is fp16\n",
    "                        encoder_hidden_states = text_encoder(inputs.input_ids.to(device))[0]\n",
    "\n",
    "                    # D. ControlNet Forward\n",
    "                    # Note: We pass float32 controlnet_image; autocast handles the rest\n",
    "                    down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        controlnet_cond=controlnet_image,\n",
    "                        return_dict=False,\n",
    "                    )\n",
    "\n",
    "                    # E. UNet Forward\n",
    "                    # UNet is frozen and fp16, so inputs must match what autocast provides\n",
    "                    model_pred = unet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        down_block_additional_residuals=down_block_res_samples,\n",
    "                        mid_block_additional_residual=mid_block_res_sample,\n",
    "                    ).sample\n",
    "\n",
    "                    # F. Loss\n",
    "                    loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "                # Backprop\n",
    "                avg_loss = accelerator.gather(loss.repeat(config.train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item() / config.gradient_accumulation_steps\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    logs = {\"loss\": train_loss, \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                    progress_bar.set_postfix(**logs)\n",
    "                    accelerator.log(logs, step=global_step)\n",
    "                    train_loss = 0.0\n",
    "\n",
    "                    if global_step % config.checkpointing_steps == 0 if hasattr(config, 'checkpointing_steps') else False:\n",
    "                         save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                         accelerator.save_state(save_path)\n",
    "\n",
    "        # End of Epoch\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            save_path = os.path.join(config.output_dir, f\"epoch-{epoch+1}\")\n",
    "            unwrap_net = accelerator.unwrap_model(controlnet)\n",
    "            unwrap_net.save_pretrained(save_path)\n",
    "            accelerator.print(f\"‚úÖ Epoch {epoch+1} Saved: {save_path}\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Final Save\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        final_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "        accelerator.unwrap_model(controlnet).save_pretrained(final_path)\n",
    "        accelerator.print(f\"üéâ Training Complete! Final model saved to {final_path}\")\n",
    "    \n",
    "    return os.path.join(config.output_dir, \"controlnet_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfcf90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T19:13:14.424206Z",
     "iopub.status.busy": "2025-12-10T19:13:14.423753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "trained_controlnet = train_controlnet()\n",
    "\n",
    "print(f\"\\n‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e2964",
   "metadata": {},
   "source": [
    "## Test the Trained ControlNet\n",
    "\n",
    "Generate images using the trained ControlNet with pose skeleton conditioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346c3da",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the trained ControlNet and create pipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Clear memory before inference (important for Kaggle)\n",
    "clear_memory()\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Loading trained ControlNet pipeline...\")\n",
    "\n",
    "# Load the saved ControlNet\n",
    "controlnet_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "controlnet_trained = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "# Create inference pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    config.pretrained_model_name,\n",
    "    controlnet=controlnet_trained,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ")\n",
    "pipe = pipe.to(accelerator.device)\n",
    "\n",
    "# Enable memory efficient attention (optional, if xformers is available)\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"‚úì XFormers memory efficient attention enabled\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  XFormers not available, using default attention: {e}\")\n",
    "    print(\"   (This is fine, just uses a bit more memory)\")\n",
    "\n",
    "print(\"‚úì Pipeline ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6f7f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Trained ControlNet on 25 Validation Images\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import cv2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING TRAINED CONTROLNET ON VALIDATION IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Number of images to test\n",
    "num_test_images = min(25, len(val_dataset))\n",
    "print(f\"\\nGenerating results for {num_test_images} validation images...\")\n",
    "print(f\"Each will show: [Original Image] [Pose Skeleton] [Generated Image]\\n\")\n",
    "\n",
    "# Create figure with subplots for all results\n",
    "fig = plt.figure(figsize=(20, 5 * num_test_images))\n",
    "gs = GridSpec(num_test_images, 3, figure=fig, hspace=0.4, wspace=0.2)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "generator = torch.Generator(device=accelerator.device).manual_seed(42)\n",
    "\n",
    "generated_images = []\n",
    "test_results = []\n",
    "\n",
    "# Process each validation image\n",
    "for idx in range(num_test_images):\n",
    "    print(f\"Processing image {idx+1}/{num_test_images}...\")\n",
    "    \n",
    "    # Get validation sample\n",
    "    test_sample = val_dataset[idx]\n",
    "    test_image = test_sample['image']\n",
    "    test_pose = test_sample['pose']\n",
    "    test_caption = test_sample['captions'][0] if test_sample['captions'] else \"a person standing\"\n",
    "    \n",
    "    # Store for results\n",
    "    test_results.append({\n",
    "        'caption': test_caption,\n",
    "        'image_id': test_sample['image_id'],\n",
    "        'keypoints': test_sample['raw_keypoints']\n",
    "    })\n",
    "    \n",
    "    # Convert pose to tensor\n",
    "    test_pose_tensor = transforms.ToTensor()(test_pose)\n",
    "    \n",
    "    # Prepare pose input (single channel, normalized to [-1, 1])\n",
    "    test_pose_input = test_pose_tensor.unsqueeze(0).to(accelerator.device, dtype=torch.float16)\n",
    "    \n",
    "    # Generate image\n",
    "    try:\n",
    "        output = pipe(\n",
    "            prompt=test_caption,\n",
    "            image=test_pose_input,\n",
    "            num_inference_steps=20,\n",
    "            generator=generator,\n",
    "            guidance_scale=7.5,\n",
    "        ).images[0]\n",
    "        generated_images.append(output)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error generating image: {str(e)[:50]}\")\n",
    "        generated_images.append(None)\n",
    "        continue\n",
    "    \n",
    "    # --- Plot Results for this image ---\n",
    "    \n",
    "    # Column 1: Original Image\n",
    "    ax1 = fig.add_subplot(gs[idx, 0])\n",
    "    orig_img = test_image.permute(1, 2, 0).cpu().numpy()\n",
    "    orig_img = (orig_img * 0.5 + 0.5)\n",
    "    orig_img = np.clip(orig_img, 0, 1)\n",
    "    ax1.imshow(orig_img)\n",
    "    ax1.set_title(f'Image {idx+1}: Original', fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Column 2: Pose Skeleton\n",
    "    ax2 = fig.add_subplot(gs[idx, 1])\n",
    "    ax2.imshow(test_pose_tensor.squeeze().cpu().numpy(), cmap='gray')\n",
    "    num_keypoints = (test_sample['raw_keypoints'][:, 2] > 0).sum()\n",
    "    ax2.set_title(f'Pose Skeleton\\n({int(num_keypoints)} keypoints visible)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Column 3: Generated Image\n",
    "    ax3 = fig.add_subplot(gs[idx, 2])\n",
    "    if generated_images[idx] is not None:\n",
    "        ax3.imshow(generated_images[idx])\n",
    "        ax3.set_title(f'Generated Image', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Generation Failed', ha='center', va='center', \n",
    "                fontsize=14, color='red', fontweight='bold')\n",
    "        ax3.set_title('Generated Image', fontsize=12, fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Add caption as suptitle for this row\n",
    "    caption_text = test_caption if len(test_caption) <= 60 else test_caption[:57] + \"...\"\n",
    "    fig.text(0.5, 0.98 - (idx * (1/num_test_images)) - 0.015, \n",
    "            f'Caption: \"{caption_text}\"', \n",
    "            ha='center', fontsize=10, style='italic', alpha=0.7)\n",
    "\n",
    "plt.suptitle(f'ControlNet Validation Results - {num_test_images} Images\\n' + \n",
    "             'Original Image | Pose Skeleton (from COCO) | Generated Image', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_generations = sum(1 for img in generated_images if img is not None)\n",
    "print(f\"‚úì Successfully generated: {successful_generations}/{num_test_images} images\")\n",
    "print(f\"‚úì Success rate: {100*successful_generations/num_test_images:.1f}%\")\n",
    "\n",
    "print(f\"\\nValidation Captions & Keypoints:\")\n",
    "for i, result in enumerate(test_results[:5]):  # Show first 5 as examples\n",
    "    print(f\"  Image {i+1}: '{result['caption'][:50]}...' ({int(result['keypoints'][:, 2].sum())} keypoints)\")\n",
    "\n",
    "print(f\"\\n‚úì Validation testing complete!\")\n",
    "print(f\"‚úì All {num_test_images} validation images tested with their COCO poses and custom captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967545f5",
   "metadata": {},
   "source": [
    "Saving  the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8696c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a downloadable zip of the final model weights\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING DOWNLOADABLE ZIP FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Path to the final model\n",
    "final_model_path = os.path.join(config.output_dir, \"controlnet_final\")\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    # Create zip file\n",
    "    zip_filename = \"controlnet_pose_final_weights.zip\"\n",
    "    print(f\"\\nüì¶ Zipping final model weights...\")\n",
    "    print(f\"Source: {final_model_path}\")\n",
    "    \n",
    "    # Create the zip archive\n",
    "    shutil.make_archive(\n",
    "        \"controlnet_pose_final_weights\",  # Name without .zip extension\n",
    "        'zip',                             # Archive format\n",
    "        final_model_path                   # Directory to zip\n",
    "    )\n",
    "    \n",
    "    # Get file size\n",
    "    zip_size = os.path.getsize(zip_filename) / (1024**2)  # Convert to MB\n",
    "    \n",
    "    print(f\"‚úì Zip file created: {zip_filename}\")\n",
    "    print(f\"‚úì Size: {zip_size:.2f} MB\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ READY TO DOWNLOAD!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"You can now download: {zip_filename}\")\n",
    "    print(f\"This contains only the final ControlNet weights (~1.5GB)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Optional: Clean up intermediate checkpoints to save space\n",
    "    cleanup = input(\"Clean up intermediate epoch checkpoints to save space? (y/n): \").lower().strip() == 'y'\n",
    "    \n",
    "    if cleanup:\n",
    "        print(\"\\nüßπ Cleaning up intermediate checkpoints...\")\n",
    "        for i in range(1, config.num_epochs + 1):\n",
    "            epoch_dir = os.path.join(config.output_dir, f\"epoch-{i}\")\n",
    "            if os.path.exists(epoch_dir):\n",
    "                shutil.rmtree(epoch_dir)\n",
    "                print(f\"  ‚úì Removed: epoch-{i}\")\n",
    "        print(\"‚úì Cleanup complete!\")\n",
    "else:\n",
    "    print(f\"‚ùå Final model not found at: {final_model_path}\")\n",
    "    print(\"Make sure training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8887620,
     "sourceId": 14095779,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
