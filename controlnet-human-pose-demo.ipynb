{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14124750,"sourceType":"datasetVersion","datasetId":8999286},{"sourceId":685834,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520209,"modelId":534517}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nimport torchvision.transforms as transforms\n\nfrom diffusers import (\n    StableDiffusionControlNetPipeline,\n    ControlNetModel\n)\n\n# CONFIG\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nCONTROLNET_PATH = \"/kaggle/input/controlnet-humna-pose/pytorch/default/1\"\nBASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n\nVAL_CAPTIONS_FILE = \"/kaggle/input/captions/val_captions.json\"\nCOCO_ROOT = \"/kaggle/working/coco_data\"\n\nNUM_VALIDATION_IMAGES = 75\nIMAGE_SIZE = 512\n\nprint(\"Validation config loaded\")\n\nOUTPUT_DIR = \"/kaggle/working/validation_outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Saving generated images to: {OUTPUT_DIR}\")\n\n# LOAD CONTROLNET PIPELINE\nprint(\"Loading ControlNet pipeline...\")\n\ncontrolnet = ControlNetModel.from_pretrained(\n    CONTROLNET_PATH,\n    torch_dtype=torch.float16\n)\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    BASE_MODEL,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n)\n\npipe = pipe.to(DEVICE)\n# pipe.enable_xformers_memory_efficient_attention()\n\nprint(\"Pipeline ready\")\n\nfrom pycocotools.coco import COCO\nimport cv2\nimport os\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport torchvision.transforms as transforms\nimport os\nfrom pycocotools.coco import COCO\nimport requests\nfrom tqdm import tqdm\nimport json\nimport cv2\nimport zipfile\n\nclass COCOPoseDataset(Dataset):\n    def __init__(self, root_dir='/kaggle/working/coco_data', split='train', transform=None, image_size=512, \n                 custom_captions_file=None, max_samples=None, download=True):\n        \"\"\"\n        Custom Dataset with COCO Pose Skeletons + Custom Captions\n        \"\"\"\n        self.root_dir = root_dir\n        self.split = split\n        self.transform = transform\n        self.image_size = image_size\n        self.custom_captions_file = custom_captions_file\n        \n        # COCO 2017 URLs\n        self.annotation_urls = {\n            'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n            'val': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n        }\n        \n        # Setup paths\n        self.ann_dir = os.path.join(root_dir, 'annotations')\n        self.img_dir = os.path.join(root_dir, f'{split}2017')\n        \n        split_name = 'train' if split == 'train' else 'val'\n        self.ann_file = os.path.join(self.ann_dir, f'person_keypoints_{split_name}2017.json')\n        \n        # Create directories\n        os.makedirs(self.ann_dir, exist_ok=True)\n        os.makedirs(self.img_dir, exist_ok=True)\n        \n        # Load custom captions - REQUIRED\n        self.custom_captions = None\n        self.custom_caption_map = {}\n        self.img_ids = []\n        \n        if not custom_captions_file:\n            raise ValueError(f\"custom_captions_file is REQUIRED! Please provide a JSON file with captions.\")\n        \n        # Check if caption file exists (try absolute and relative paths)\n        caption_path = custom_captions_file\n        if not os.path.exists(caption_path):\n            # Try absolute path if relative doesn't work\n            caption_path = os.path.abspath(custom_captions_file)\n        \n        if not os.path.exists(caption_path):\n            raise FileNotFoundError(\n                f\"Caption file not found: {custom_captions_file}\\n\"\n                f\"Tried paths:\\n\"\n                f\"  - {custom_captions_file}\\n\"\n                f\"  - {os.path.abspath(custom_captions_file)}\\n\"\n                f\"Current working directory: {os.getcwd()}\\n\"\n                f\"Please ensure the caption file exists with format: {{'image_filename.jpg': 'caption text', ...}}\"\n            )\n        \n        print(f\"Loading custom captions from {caption_path}...\")\n        with open(caption_path, 'r') as f:\n            self.custom_captions = json.load(f)\n        print(f\"Loaded {len(self.custom_captions)} custom captions\")\n        \n        # Download COCO annotations if needed\n        if download and not os.path.exists(self.ann_file):\n            print(f\"Annotation file not found. Downloading COCO 2017 annotations...\")\n            self._download_annotations()\n        \n        # Check if annotation file exists\n        if not os.path.exists(self.ann_file):\n            raise FileNotFoundError(\n                f\"Annotation file not found: {self.ann_file}\\n\"\n                f\"Please download COCO 2017 annotations from:\\n\"\n                f\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\\n\"\n                f\"Extract to: {self.ann_dir}\"\n            )\n        \n        print(f\"Loading COCO {split} pose annotations...\")\n        self.coco = COCO(self.ann_file)\n        \n        # Map images from caption file\n        print(\"Setting up dataset with images, poses from COCO, and custom captions...\")\n        items_to_process = list(self.custom_captions.items())\n        if max_samples:\n            items_to_process = items_to_process[:max_samples]\n        \n        for img_filename, caption in items_to_process:\n            # Extract image ID from filename\n            img_id = int(img_filename.split('.')[0].lstrip('0') or '0')\n            self.img_ids.append(img_id)\n            self.custom_caption_map[img_id] = caption\n        \n        limit_msg = f\" (limited to first {max_samples})\" if max_samples else \"\"\n        print(f\"Dataset ready with {len(self.img_ids)} images\")\n        print(f\"  - Images from: {self.img_dir}\")\n        print(f\"  - Captions from: {custom_captions_file}{limit_msg}\")\n        print(f\"  - Poses from: COCO person_keypoints annotations\")\n        print(f\"Using ACTUAL POSE SKELETONS (stick figures) as conditioning!\\n\")\n        \n        if len(self.img_ids) == 0:\n            print(\"\\nERROR: No images found in caption file!\")\n    \n    def _download_annotations(self):\n        \"\"\"Download COCO annotations\"\"\"\n        url = self.annotation_urls[self.split]\n        zip_path = os.path.join(self.root_dir, 'annotations.zip')\n        \n        print(f\"Downloading from {url}...\")\n        response = requests.get(url, stream=True)\n        total_size = int(response.headers.get('content-length', 0))\n        \n        with open(zip_path, 'wb') as f:\n            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n        \n        print(\"Extracting annotations...\")\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(self.root_dir)\n\n        if os.path.exists(zip_path):\n            os.remove(zip_path)\n        \n        print(\"Annotations downloaded successfully!\")\n    \n    def _download_image(self, img_id, img_filename):\n        \"\"\"Download a single image from COCO dataset on-the-fly\"\"\"\n        img_path = os.path.join(self.img_dir, img_filename)\n        \n        # If image already exists, skip download\n        if os.path.exists(img_path):\n            return img_path\n        \n        # Get image info from COCO API\n        img_info = self.coco.loadImgs(img_id)[0]\n        img_url = img_info['coco_url']\n        \n        # Download image\n        try:\n            response = requests.get(img_url, timeout=10)\n            response.raise_for_status()\n            \n            # Save image\n            with open(img_path, 'wb') as f:\n                f.write(response.content)\n            \n            return img_path\n        except Exception as e:\n            raise RuntimeError(f\"Failed to download image {img_filename} from {img_url}: {str(e)}\")\n    \n    def __len__(self):\n        return len(self.img_ids)\n    \n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_filename = list(self.custom_captions.keys())[idx]\n        img_path = os.path.join(self.img_dir, img_filename)\n        \n        # Download image if it doesn't exist (ON-THE-FLY DOWNLOAD)\n        if not os.path.exists(img_path):\n            # print(f\"ðŸ“¥ Downloading image: {img_filename}...\")\n            img_path = self._download_image(img_id, img_filename)\n        \n        # Try to load image, skip if corrupted\n        max_retries = 3\n        retry_count = 0\n        image = None\n        \n        while retry_count < max_retries and image is None:\n            try:\n                image = Image.open(img_path).convert('RGB')\n                width, height = image.size\n                break\n            except Exception as e:\n                retry_count += 1\n                print(f\"Failed to load image {img_filename} (attempt {retry_count}/{max_retries}): {str(e)[:50]}\")\n                \n                # Delete corrupted file and try re-downloading\n                if os.path.exists(img_path):\n                    os.remove(img_path)\n                    print(f\"   Deleted corrupted file: {img_filename}\")\n                \n                if retry_count < max_retries:\n                    try:\n                        print(f\"   Re-downloading image...\")\n                        img_path = self._download_image(img_id, img_filename)\n                    except Exception as download_err:\n                        print(f\"   Download failed: {str(download_err)[:50]}\")\n        \n        if image is None:\n            print(f\"Giving up on {img_filename}, returning next valid image instead...\")\n            # Try next image in dataset\n            next_idx = (idx + 1) % len(self.img_ids)\n            if next_idx != idx:  # Avoid infinite loop\n                return self.__getitem__(next_idx)\n            else:\n                # Return black image as fallback\n                image = Image.new('RGB', (self.image_size, self.image_size), color='black')\n                width, height = self.image_size, self.image_size\n        \n        # Get caption\n        caption = self.custom_caption_map[img_id]\n        \n        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.coco.getCatIds(catNms=['person']), iscrowd=False)\n        anns = self.coco.loadAnns(ann_ids)\n        \n        # Get pose keypoints from first person with visible keypoints\n        keypoints = None\n        for ann in anns:\n            if 'keypoints' in ann and ann.get('num_keypoints', 0) > 0:\n                keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n                break\n        \n        if keypoints is None:\n            keypoints = np.zeros((17, 3))\n        \n        pose_skeleton = self.create_pose_skeleton(keypoints, width, height)\n        \n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = transforms.Compose([\n                transforms.Resize((self.image_size, self.image_size)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5])\n            ])(image)\n        \n        pose_map = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((self.image_size, self.image_size)),\n        ])(pose_skeleton)\n        \n        return {\n            'image': image,\n            'pose': pose_map,\n            'raw_keypoints': keypoints,\n            'image_id': img_id,\n            'captions': [caption]  \n        }\n    \n    def create_pose_skeleton(self, keypoints, width, height):\n        \"\"\"\n        Create ACTUAL human pose skeleton from COCO keypoint annotations\n        Draws a stick figure with bones connecting the 17 joints\n        \n        COCO Keypoints (17 total):\n        0: nose, 1: L eye, 2: R eye, 3: L ear, 4: R ear,\n        5: L shoulder, 6: R shoulder, 7: L elbow, 8: R elbow, 9: L wrist, 10: R wrist,\n        11: L hip, 12: R hip, 13: L knee, 14: R knee, 15: L ankle, 16: R ankle\n        \"\"\"\n        # Create black canvas\n        pose_img = np.zeros((height, width), dtype=np.uint8)\n        \n        # COCO keypoint skeleton connections (bones linking joints)\n        skeleton = [\n            (0, 1), (0, 2),           # nose to eyes\n            (1, 3), (2, 4),           # eyes to ears\n            (0, 5), (0, 6),           # nose to shoulders\n            (5, 7), (7, 9),           # left arm (shoulderâ†’elbowâ†’wrist)\n            (6, 8), (8, 10),          # right arm (shoulderâ†’elbowâ†’wrist)\n            (5, 11), (6, 12),         # shoulders to hips\n            (11, 12),                 # hip to hip\n            (11, 13), (13, 15),       # left leg (hipâ†’kneeâ†’ankle)\n            (12, 14), (14, 16)        # right leg (hipâ†’kneeâ†’ankle)\n        ]\n        \n        # Line thickness and circle radius scale with image size\n        line_thickness = max(2, int(min(width, height) / 100))\n        circle_radius = max(3, int(min(width, height) / 80))\n        \n        # Draw bones (connections) FIRST\n        for start_idx, end_idx in skeleton:\n            if start_idx < len(keypoints) and end_idx < len(keypoints):\n                x1, y1, v1 = keypoints[start_idx]\n                x2, y2, v2 = keypoints[end_idx]\n                \n                # Draw line ONLY if both keypoints are visible (v > 0)\n                if v1 > 0 and v2 > 0:\n                    cv2.line(pose_img, (int(x1), int(y1)), (int(x2), int(y2)), \n                            255, line_thickness, cv2.LINE_AA)\n        \n        # Draw keypoint circles ON TOP of bones\n        for i, (x, y, v) in enumerate(keypoints):\n            if v > 0:  # Only draw visible keypoints\n                cv2.circle(pose_img, (int(x), int(y)), circle_radius, 255, -1)\n        \n        return pose_img\n    \n# Validation dataset\nval_dataset = COCOPoseDataset(\n    root_dir='/kaggle/working/coco_data',\n    split='val',\n    image_size=512,\n    custom_captions_file='/kaggle/input/captions/val_captions.json',\n    max_samples=None,\n    download=True\n)\n\nprint(f\"Validation samples available: {len(val_dataset)}\")\n\nnum_samples = len(val_dataset)\n\npipe.set_progress_bar_config(disable=True)\n\nfor i in tqdm(range(num_samples), desc=\"Validating\"):\n    sample = val_dataset[i]\n\n    # Original image (for display)\n    original = sample[\"image\"]\n    caption = sample[\"captions\"][0]\n    img_id = sample[\"image_id\"]\n\n    # Convert original image for display\n    orig_np = original.permute(1, 2, 0).cpu().numpy()\n    orig_np = (orig_np * 0.5 + 0.5)\n    orig_np = np.clip(orig_np, 0, 1)\n\n    pose_pil = sample[\"pose\"]  # grayscale PIL\n    pose_tensor = transforms.ToTensor()(pose_pil)          # [1, H, W]\n    pose_tensor = pose_tensor.unsqueeze(0).to(DEVICE)     # [1, 1, H, W]\n\n    # Generate\n    with torch.autocast(\"cuda\"):\n        result = pipe(\n            prompt=caption,\n            image=pose_tensor,   \n            num_inference_steps=30,\n            guidance_scale=7.5\n        )\n\n    generated = result.images[0]\n\n    # SAVE GENERATED IMAGE\n    save_name = f\"{img_id:012d}_gen.png\"\n    save_path = os.path.join(OUTPUT_DIR, save_name)\n    generated.save(save_path)\n\n    # DISPLAY ALL 3\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n    axs[0].imshow(orig_np)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(pose_pil, cmap=\"gray\")\n    axs[1].set_title(\"Pose Skeleton\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(generated)\n    axs[2].set_title(\"Generated Image\")\n    axs[2].axis(\"off\")\n\n    plt.suptitle(caption, fontsize=10)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:03:29.523480Z","iopub.execute_input":"2025-12-16T10:03:29.524275Z"}},"outputs":[],"execution_count":null}]}