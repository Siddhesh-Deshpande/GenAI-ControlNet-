{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cefb3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:33.757141Z",
     "iopub.status.busy": "2025-12-10T09:46:33.756941Z",
     "iopub.status.idle": "2025-12-10T09:46:47.538489Z",
     "shell.execute_reply": "2025-12-10T09:46:47.537635Z"
    },
    "papermill": {
     "duration": 13.788067,
     "end_time": "2025-12-10T09:46:47.539725",
     "exception": false,
     "start_time": "2025-12-10T09:46:33.751658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ PyTorch version: 2.6.0+cu124\n",
      "✓ CUDA available: True\n",
      "✓ GPU: Tesla T4\n",
      "  GPU 0: Tesla T4\n",
      "  GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0608466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.548072Z",
     "iopub.status.busy": "2025-12-10T09:46:47.547568Z",
     "iopub.status.idle": "2025-12-10T09:46:47.553035Z",
     "shell.execute_reply": "2025-12-10T09:46:47.552300Z"
    },
    "papermill": {
     "duration": 0.010547,
     "end_time": "2025-12-10T09:46:47.554060",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.543513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# Memory optimization utilities for Kaggle\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"✓ Memory cleared\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "print(\"✓ Memory utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb4037",
   "metadata": {
    "papermill": {
     "duration": 0.003334,
     "end_time": "2025-12-10T09:46:47.560982",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.557648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set your dataset generation parameters here:\n",
    "\n",
    "### ⏱️ **Time Estimation for Kaggle (12hr limit):**\n",
    "- **Qwen-VL-Chat inference**: ~3-5 seconds per image (with short captions)\n",
    "- **5,000 train images**: ~4-7 hours\n",
    "- **500 val images**: ~25-42 minutes\n",
    "- **Total estimated time**: ~4.5-8 hours (fits within 12hr limit!)\n",
    "\n",
    "**Optimizations to speed up:**\n",
    "- Shorter captions (70 tokens max) = faster generation\n",
    "- Batch processing where possible\n",
    "- Skip failed downloads quickly\n",
    "- GPU acceleration (T4/P100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc40859b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.568511Z",
     "iopub.status.busy": "2025-12-10T09:46:47.568286Z",
     "iopub.status.idle": "2025-12-10T09:46:47.577558Z",
     "shell.execute_reply": "2025-12-10T09:46:47.576924Z"
    },
    "papermill": {
     "duration": 0.014289,
     "end_time": "2025-12-10T09:46:47.578542",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.564253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "Working Directory: /kaggle/working\n",
      "COCO Root: /kaggle/working/coco_data\n",
      "Output Directory: /kaggle/working/dataset_output\n",
      "Target Train Images: 5000\n",
      "Target Val Images: 500\n",
      "Quality Thresholds:\n",
      "  - Min Keypoints: 10/17\n",
      "  - Min Person Area: 5000px²\n",
      "Caption Settings:\n",
      "  - Max Tokens: 70\n",
      "  - Device: cuda\n",
      "  - Model: Qwen/Qwen2-VL-2B-Instruct\n",
      "  - Model Type: qwen2-vl\n",
      "\n",
      "✓ Using Qwen2-VL-2B (proven working, no OOM issues)\n",
      "  - Smaller than Qwen-VL-Chat\n",
      "  - Disk cache system for captions\n",
      "  - Can unload model after caption generation\n",
      "================================================================================\n",
      "\n",
      "✓ Directories created/verified:\n",
      "  - /kaggle/working/coco_data\n",
      "  - /kaggle/working/dataset_output\n"
     ]
    }
   ],
   "source": [
    "# Dataset Generation Configuration\n",
    "class Config:\n",
    "    # Get notebook directory for absolute paths\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "    \n",
    "    # Paths (using absolute paths to avoid directory issues)\n",
    "    COCO_ROOT = os.path.join(NOTEBOOK_DIR, 'coco_data')\n",
    "    OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, 'dataset_output')\n",
    "    \n",
    "    # Dataset sizes\n",
    "    TARGET_TRAIN = 5000  # Training images\n",
    "    TARGET_VAL = 500     # Validation images\n",
    "    \n",
    "    # Quality filtering thresholds\n",
    "    MIN_KEYPOINTS = 10   # Minimum visible keypoints (out of 17)\n",
    "    MIN_PERSON_AREA = 5000  # Minimum person area in pixels²\n",
    "    \n",
    "    # Caption generation\n",
    "    MAX_CAPTION_TOKENS = 70  # CLIP limit is 77, we use 70 for safety\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Model - Using Qwen2-VL-2B (proven to work, no OOM)\n",
    "    MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    MODEL_TYPE = \"qwen2-vl\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Working Directory: {config.NOTEBOOK_DIR}\")\n",
    "print(f\"COCO Root: {config.COCO_ROOT}\")\n",
    "print(f\"Output Directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Target Train Images: {config.TARGET_TRAIN}\")\n",
    "print(f\"Target Val Images: {config.TARGET_VAL}\")\n",
    "print(f\"Quality Thresholds:\")\n",
    "print(f\"  - Min Keypoints: {config.MIN_KEYPOINTS}/17\")\n",
    "print(f\"  - Min Person Area: {config.MIN_PERSON_AREA}px²\")\n",
    "print(f\"Caption Settings:\")\n",
    "print(f\"  - Max Tokens: {config.MAX_CAPTION_TOKENS}\")\n",
    "print(f\"  - Device: {config.DEVICE}\")\n",
    "print(f\"  - Model: {config.MODEL_NAME}\")\n",
    "print(f\"  - Model Type: {config.MODEL_TYPE}\")\n",
    "print(f\"\\n✓ Using Qwen2-VL-2B (proven working, no OOM issues)\")\n",
    "print(f\"  - Smaller than Qwen-VL-Chat\")\n",
    "print(f\"  - Disk cache system for captions\")\n",
    "print(f\"  - Can unload model after caption generation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.COCO_ROOT, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'train2017'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.COCO_ROOT, 'val2017'), exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Directories created/verified:\")\n",
    "print(f\"  - {config.COCO_ROOT}\")\n",
    "print(f\"  - {config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ecae6",
   "metadata": {
    "papermill": {
     "duration": 0.003272,
     "end_time": "2025-12-10T09:46:47.585180",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.581908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Load COCO Annotations\n",
    "\n",
    "Download and load COCO 2017 annotations for person keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2d1885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:46:47.592608Z",
     "iopub.status.busy": "2025-12-10T09:46:47.592426Z",
     "iopub.status.idle": "2025-12-10T09:47:03.710511Z",
     "shell.execute_reply": "2025-12-10T09:47:03.709677Z"
    },
    "papermill": {
     "duration": 16.123209,
     "end_time": "2025-12-10T09:47:03.711732",
     "exception": false,
     "start_time": "2025-12-10T09:46:47.588523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  COCO annotations not found! Downloading...\n",
      "Downloading from: http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "This may take a few minutes (~252 MB)...\n",
      "✓ Download complete! Extracting...\n",
      "✓ Extraction complete!\n",
      "\n",
      "Loading COCO annotations...\n",
      "loading annotations into memory...\n",
      "Done (t=7.75s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.92s)\n",
      "creating index...\n",
      "index created!\n",
      "✓ COCO annotations loaded successfully!\n",
      "  - Train images: 118287\n",
      "  - Val images: 5000\n"
     ]
    }
   ],
   "source": [
    "# Load COCO annotations\n",
    "ann_dir = os.path.join(config.COCO_ROOT, 'annotations')\n",
    "train_ann_file = os.path.join(ann_dir, 'person_keypoints_train2017.json')\n",
    "val_ann_file = os.path.join(ann_dir, 'person_keypoints_val2017.json')\n",
    "\n",
    "# Download annotations if they don't exist\n",
    "if not os.path.exists(train_ann_file) or not os.path.exists(val_ann_file):\n",
    "    print(\"⚠️  COCO annotations not found! Downloading...\")\n",
    "    \n",
    "    import zipfile\n",
    "    import urllib.request\n",
    "    \n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    zip_path = os.path.join(config.COCO_ROOT, 'annotations_trainval2017.zip')\n",
    "    \n",
    "    print(f\"Downloading from: {ann_url}\")\n",
    "    print(\"This may take a few minutes (~252 MB)...\")\n",
    "    \n",
    "    # Download with progress\n",
    "    urllib.request.urlretrieve(ann_url, zip_path)\n",
    "    print(\"✓ Download complete! Extracting...\")\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(config.COCO_ROOT)\n",
    "    \n",
    "    # Cleanup zip file\n",
    "    os.remove(zip_path)\n",
    "    print(\"✓ Extraction complete!\")\n",
    "\n",
    "# Now load COCO annotations\n",
    "print(\"\\nLoading COCO annotations...\")\n",
    "coco_train = COCO(train_ann_file)\n",
    "coco_val = COCO(val_ann_file)\n",
    "print(\"✓ COCO annotations loaded successfully!\")\n",
    "print(f\"  - Train images: {len(coco_train.getImgIds())}\")\n",
    "print(f\"  - Val images: {len(coco_val.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217e515",
   "metadata": {
    "papermill": {
     "duration": 0.003653,
     "end_time": "2025-12-10T09:47:03.719410",
     "exception": false,
     "start_time": "2025-12-10T09:47:03.715757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Filter High-Quality Pose Images\n",
    "\n",
    "Filter COCO dataset for images with high-quality pose annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f83c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:47:03.727674Z",
     "iopub.status.busy": "2025-12-10T09:47:03.727289Z",
     "iopub.status.idle": "2025-12-10T09:47:04.330887Z",
     "shell.execute_reply": "2025-12-10T09:47:04.329852Z"
    },
    "papermill": {
     "duration": 0.609069,
     "end_time": "2025-12-10T09:47:04.332024",
     "exception": false,
     "start_time": "2025-12-10T09:47:03.722955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FILTERING TRAIN IMAGES FOR HIGH-QUALITY POSES\n",
      "================================================================================\n",
      "Criteria:\n",
      "  - Minimum keypoints: 10/17\n",
      "  - Minimum person area: 5000px²\n",
      "  - No crowd annotations\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering train images: 100%|██████████| 64115/64115 [00:00<00:00, 127652.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Filtered 5000 high-quality images from 64115 total\n",
      "  - Average keypoints: 15.7/17\n",
      "  - Quality range: 17.95 to 25.67\n",
      "\n",
      "================================================================================\n",
      "FILTERING VAL IMAGES FOR HIGH-QUALITY POSES\n",
      "================================================================================\n",
      "Criteria:\n",
      "  - Minimum keypoints: 10/17\n",
      "  - Minimum person area: 5000px²\n",
      "  - No crowd annotations\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering val images: 100%|██████████| 2693/2693 [00:00<00:00, 143309.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Filtered 500 high-quality images from 2693 total\n",
      "  - Average keypoints: 15.8/17\n",
      "  - Quality range: 16.68 to 22.91\n",
      "\n",
      "================================================================================\n",
      "FILTERING COMPLETE\n",
      "================================================================================\n",
      "✓ Train: 5000 images selected\n",
      "✓ Val: 500 images selected\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_high_quality_images(coco, split='train', min_keypoints=10, min_area=5000, max_images=None):\n",
    "    \"\"\"\n",
    "    Filter COCO dataset for high-quality pose images\n",
    "    \n",
    "    Args:\n",
    "        coco: COCO API instance\n",
    "        split: 'train' or 'val'\n",
    "        min_keypoints: Minimum visible keypoints (out of 17)\n",
    "        min_area: Minimum person area in pixels\n",
    "        max_images: Maximum number of images to return\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered image IDs with quality scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILTERING {split.upper()} IMAGES FOR HIGH-QUALITY POSES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Criteria:\")\n",
    "    print(f\"  - Minimum keypoints: {min_keypoints}/17\")\n",
    "    print(f\"  - Minimum person area: {min_area}px²\")\n",
    "    print(f\"  - No crowd annotations\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    cat_ids = coco.getCatIds(catNms=['person'])\n",
    "    all_img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "    \n",
    "    quality_images = []\n",
    "    \n",
    "    for img_id in tqdm(all_img_ids, desc=f\"Filtering {split} images\"):\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids, iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        best_quality_score = 0\n",
    "        best_keypoint_count = 0\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'keypoints' not in ann:\n",
    "                continue\n",
    "            \n",
    "            num_keypoints = ann.get('num_keypoints', 0)\n",
    "            person_area = ann.get('area', 0)\n",
    "            \n",
    "            # Quality filtering\n",
    "            if (num_keypoints >= min_keypoints and \n",
    "                person_area >= min_area and \n",
    "                ann.get('iscrowd', 0) == 0):\n",
    "                \n",
    "                # Quality score: combine keypoints and area\n",
    "                quality_score = num_keypoints * 1.0 + (person_area / 10000) * 0.5\n",
    "                \n",
    "                if quality_score > best_quality_score:\n",
    "                    best_quality_score = quality_score\n",
    "                    best_keypoint_count = num_keypoints\n",
    "        \n",
    "        if best_quality_score > 0:\n",
    "            quality_images.append({\n",
    "                'image_id': img_id,\n",
    "                'quality_score': best_quality_score,\n",
    "                'keypoints': best_keypoint_count\n",
    "            })\n",
    "    \n",
    "    # Sort by quality score (best first)\n",
    "    quality_images.sort(key=lambda x: x['quality_score'], reverse=True)\n",
    "    \n",
    "    # Limit to max_images if specified\n",
    "    if max_images and len(quality_images) > max_images:\n",
    "        quality_images = quality_images[:max_images]\n",
    "    \n",
    "    print(f\"\\n✓ Filtered {len(quality_images)} high-quality images from {len(all_img_ids)} total\")\n",
    "    print(f\"  - Average keypoints: {np.mean([img['keypoints'] for img in quality_images]):.1f}/17\")\n",
    "    print(f\"  - Quality range: {quality_images[-1]['quality_score']:.2f} to {quality_images[0]['quality_score']:.2f}\")\n",
    "    \n",
    "    return quality_images\n",
    "\n",
    "# Check if COCO annotations are loaded\n",
    "if 'coco_train' not in globals() or 'coco_val' not in globals():\n",
    "    print(\"⚠️  ERROR: COCO annotations not loaded!\")\n",
    "    print(\"Please run Cell 5 (Step 1: Load COCO Annotations) first.\")\n",
    "    raise NameError(\"coco_train and coco_val are not defined. Run the previous cells in order.\")\n",
    "\n",
    "# Filter train and val images\n",
    "train_images = filter_high_quality_images(\n",
    "    coco_train, \n",
    "    split='train',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_TRAIN\n",
    ")\n",
    "\n",
    "val_images = filter_high_quality_images(\n",
    "    coco_val, \n",
    "    split='val',\n",
    "    min_keypoints=config.MIN_KEYPOINTS,\n",
    "    min_area=config.MIN_PERSON_AREA,\n",
    "    max_images=config.TARGET_VAL\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FILTERING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Train: {len(train_images)} images selected\")\n",
    "print(f\"✓ Val: {len(val_images)} images selected\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e86f9a",
   "metadata": {
    "papermill": {
     "duration": 0.004112,
     "end_time": "2025-12-10T09:47:04.340637",
     "exception": false,
     "start_time": "2025-12-10T09:47:04.336525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Initialize Caption Generator\n",
    "\n",
    "Load Qwen-VL-Chat model for generating CLIP-compatible captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7d9146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:47:04.349730Z",
     "iopub.status.busy": "2025-12-10T09:47:04.349516Z",
     "iopub.status.idle": "2025-12-10T09:49:01.736907Z",
     "shell.execute_reply": "2025-12-10T09:49:01.736090Z"
    },
    "papermill": {
     "duration": 117.393323,
     "end_time": "2025-12-10T09:49:01.738014",
     "exception": false,
     "start_time": "2025-12-10T09:47:04.344691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies for Qwen2-VL-2B...\n",
      "✓ GPU memory cleared\n",
      "Installing transformers>=4.45.0...\n",
      "Installing accelerate...\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 5.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 109.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 89.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 36.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 1.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 8.7 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 31.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 15.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 9.5 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 90.1 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing qwen-vl-utils...\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.2/40.2 MB 48.7 MB/s eta 0:00:00\n",
      "Installing pillow...\n",
      "Installing torchvision...\n",
      "\n",
      "✓ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies for Qwen2-VL-2B\n",
    "import subprocess\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "print(\"Installing dependencies for Qwen2-VL-2B...\")\n",
    "\n",
    "# First, clear any existing GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU memory cleared\")\n",
    "\n",
    "# Install compatible versions\n",
    "packages = [\n",
    "    'transformers>=4.45.0',\n",
    "    'accelerate',\n",
    "    'qwen-vl-utils',\n",
    "    'pillow',\n",
    "    'torchvision'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"\\n✓ All dependencies installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b588e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:01.749496Z",
     "iopub.status.busy": "2025-12-10T09:49:01.748875Z",
     "iopub.status.idle": "2025-12-10T09:49:02.447915Z",
     "shell.execute_reply": "2025-12-10T09:49:02.447188Z"
    },
    "papermill": {
     "duration": 0.70583,
     "end_time": "2025-12-10T09:49:02.449052",
     "exception": false,
     "start_time": "2025-12-10T09:49:01.743222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPU MEMORY STATUS\n",
      "================================================================================\n",
      "GPU: Tesla T4\n",
      "Total GPU Memory: 15.83 GB\n",
      "\n",
      "Current Memory Usage:\n",
      "  - Allocated: 0.00 GB\n",
      "  - Reserved: 0.00 GB\n",
      "  - Free: 15.83 GB\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU Memory Status Check\n",
    "import gc\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPU MEMORY STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Force garbage collection and clear cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"Total GPU Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Get current memory usage\n",
    "    current_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = props.total_memory / 1e9\n",
    "    free = total - (reserved)\n",
    "    \n",
    "    print(f\"\\nCurrent Memory Usage:\")\n",
    "    print(f\"  - Allocated: {current_allocated:.2f} GB\")\n",
    "    print(f\"  - Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  - Free: {free:.2f} GB\")\n",
    "    \n",
    "    if current_allocated > 10:\n",
    "        print(f\"\\n⚠️  WARNING: High GPU memory usage!\")\n",
    "        print(\"Clearing unneeded variables...\")\n",
    "        # Try to identify and clear large objects\n",
    "        if 'train_images' in globals() or 'val_images' in globals():\n",
    "            print(\"  - Keeping image lists (needed for caption generation)\")\n",
    "        if 'coco_train' in globals() or 'coco_val' in globals():\n",
    "            print(\"  - Keeping COCO objects (needed for image info)\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available!\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82393a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:02.460419Z",
     "iopub.status.busy": "2025-12-10T09:49:02.460156Z",
     "iopub.status.idle": "2025-12-10T09:49:02.471549Z",
     "shell.execute_reply": "2025-12-10T09:49:02.470948Z"
    },
    "papermill": {
     "duration": 0.018472,
     "end_time": "2025-12-10T09:49:02.472638",
     "exception": false,
     "start_time": "2025-12-10T09:49:02.454166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionGenerator:\n",
    "    \"\"\"Ultra-optimized caption generator for Kaggle - minimal memory footprint\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2-VL-2B-Instruct\"):\n",
    "        \"\"\"Initialize with aggressive memory management\"\"\"\n",
    "        import gc\n",
    "        from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        # Pre-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Load processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Load model in fp16 for memory efficiency\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        ).eval()\n",
    "        \n",
    "        print(f\"✓ Model loaded successfully\")\n",
    "        \n",
    "        # Post-load cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        \"\"\"Generate caption with minimal memory usage\"\"\"\n",
    "        import gc\n",
    "        from PIL import Image as PILImage\n",
    "        \n",
    "        try:\n",
    "            # Pre-process cleanup\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Load and resize image (critical for memory)\n",
    "            image = PILImage.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Aggressive downsizing - max 768 pixels\n",
    "            if max(image.size) > 768:\n",
    "                ratio = 768 / max(image.size)\n",
    "                new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "                image = image.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            \n",
    "            # Create prompt\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image for image generation. Include: main subjects, their positions, colors, lighting, mood, background, and style. Be specific, vivid, and under 70 words.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process with minimal overhead\n",
    "            text_prompt = self.processor.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=text_prompt,\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=70,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            # isolate generated tokens (remove prompt)\n",
    "            gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            \n",
    "            # decode only the assistant's answer\n",
    "            caption = self.processor.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Remove common prefixes\n",
    "            for prefix in ['assistant:', '<|assistant|>', 'sure ', 'here ', 'the image']:\n",
    "                if caption.lower().startswith(prefix):\n",
    "                    caption = caption[len(prefix):].strip()\n",
    "            \n",
    "            # Enforce 70-token limit (hard cutoff)\n",
    "            words = caption.split()\n",
    "            if len(words) > 70:\n",
    "                caption = ' '.join(words[:70])\n",
    "            \n",
    "            # Ensure proper ending\n",
    "            if caption and not caption.endswith(('.', '!', '?')):\n",
    "                caption += '.'\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del image, inputs, output_ids, text_prompt, conversation\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            return caption if len(caption) > 5 else \"A person in a scene.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error: {str(e)[:40]}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return \"A person in a scene.\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Free all GPU memory\"\"\"\n",
    "        import gc\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"✓ Model cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a3601ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:02.483321Z",
     "iopub.status.busy": "2025-12-10T09:49:02.483089Z",
     "iopub.status.idle": "2025-12-10T09:49:56.004732Z",
     "shell.execute_reply": "2025-12-10T09:49:56.003888Z"
    },
    "papermill": {
     "duration": 53.52831,
     "end_time": "2025-12-10T09:49:56.005940",
     "exception": false,
     "start_time": "2025-12-10T09:49:02.477630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING CAPTION GENERATOR\n",
      "================================================================================\n",
      "Loading Qwen2-VL-2B model (this will take 2-3 minutes)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 09:49:09.113621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765360149.563568      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765360149.671071      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2-VL-2B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de6e05e1b084171b180ee2cbc3b78aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c20dd1c5e3496e87adde0049e12e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36d54ef83c14d72a58694ae0c45683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83079f24886f4d9e8eb370be125a49a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f8711a701f4fbd83571f1d8371b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01bb9aa9eb4456abfd50e6b1412f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097e6646f53e4038ac0045ddd852abe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d754e664e51a4d1fb66fbd89d3f7f9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8df09f574104514892ebfc66eefec04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00edc26f9cb3427aabae2482291262f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb16d99aa6b04d4e82ea28f7bb2a0935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6ca0961cf049c5bce709795e982c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580e44c23931407a9e7aafb0c597e700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "\n",
      "================================================================================\n",
      "✓ CAPTION GENERATOR READY!\n",
      "================================================================================\n",
      "\n",
      "GPU Memory: 2.02GB allocated, 2.46GB reserved\n"
     ]
    }
   ],
   "source": [
    "# Initialize Caption Generator\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING CAPTION GENERATOR\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loading Qwen2-VL-2B model (this will take 2-3 minutes)...\")\n",
    "print()\n",
    "\n",
    "caption_gen = CaptionGenerator(model_name=config.MODEL_NAME)\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"✓ CAPTION GENERATOR READY!\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9204a",
   "metadata": {
    "papermill": {
     "duration": 0.006507,
     "end_time": "2025-12-10T09:49:56.019902",
     "exception": false,
     "start_time": "2025-12-10T09:49:56.013395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Generate Training Captions\n",
    "\n",
    "Download training images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6acd11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:49:56.034316Z",
     "iopub.status.busy": "2025-12-10T09:49:56.033580Z"
    },
    "papermill": {
     "duration": 3.011359,
     "end_time": "2025-12-10T09:49:59.037863",
     "exception": false,
     "start_time": "2025-12-10T09:49:56.026504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING CAPTION GENERATOR ON 5 IMAGES\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# TEST CAPTION GENERATION ON 5 RANDOM TRAIN IMAGES\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Must have caption_gen from your previous initialization cell\n",
    "if 'caption_gen' not in globals():\n",
    "    raise NameError(\"Run Cell 13 (Initialize Caption Generator) first!\")\n",
    "\n",
    "# Pick 5 random images from COCO train set\n",
    "sample_images = random.sample(train_images, 5)\n",
    "\n",
    "test_captions = {}\n",
    "img_dir_train = os.path.join(config.COCO_ROOT, 'train2017')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TESTING CAPTION GENERATOR ON 5 IMAGES\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(sample_images, desc=\"Test\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_train.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_train, filename)\n",
    "\n",
    "    # Download if missing (COCO images sometimes are not pre-downloaded)\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Download error ({filename}): {e}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            test_captions[filename] = caption\n",
    "            print(f\"\\n[{filename}]\")\n",
    "            print(\"Caption:\", caption)\n",
    "            print(\"-\" * 80)\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "\n",
    "    # cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n✓ Success: {success}\")\n",
    "print(f\"✗ Failed : {failed}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b52eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T09:38:40.915232Z",
     "iopub.status.busy": "2025-12-10T09:38:40.914638Z",
     "iopub.status.idle": "2025-12-10T09:38:40.993454Z",
     "shell.execute_reply": "2025-12-10T09:38:40.992551Z",
     "shell.execute_reply.started": "2025-12-10T09:38:40.915207Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "CHECKPOINT_PATH = \"train_captions_checkpoint.json\"\n",
    "SAVE_INTERVAL = 200   # save every N images\n",
    "\n",
    "# Load previous progress if exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, \"r\") as f:\n",
    "        train_captions = json.load(f)\n",
    "    print(f\"Loaded {len(train_captions)} previously saved captions.\")\n",
    "else:\n",
    "    train_captions = {}\n",
    "\n",
    "img_dir_train = os.path.join(config.COCO_ROOT, 'train2017')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GENERATING {len(train_images)} TRAINING CAPTIONS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "success = len(train_captions)\n",
    "failed = 0\n",
    "\n",
    "for idx, img_data in enumerate(tqdm(train_images, desc=\"Train\")):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_train.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "\n",
    "    # Skip if already processed\n",
    "    if filename in train_captions:\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(img_dir_train, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            train_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "\n",
    "    # Periodic cleanup + save\n",
    "    if (success + failed) % SAVE_INTERVAL == 0:\n",
    "        with open(CHECKPOINT_PATH, \"w\") as f:\n",
    "            json.dump(train_captions, f)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\n💾 Saved checkpoint at {success} captions.\\n\")\n",
    "\n",
    "# Final save\n",
    "with open(CHECKPOINT_PATH, \"w\") as f:\n",
    "    json.dump(train_captions, f)\n",
    "\n",
    "print(f\"\\n✓ Generated: {success}\")\n",
    "print(f\"✗ Failed: {failed}\")\n",
    "print(f\"💾 Final save completed.\")\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6687d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Generate Validation Captions\n",
    "\n",
    "Download validation images and generate detailed captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43706edf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Validation Captions - Ultra-optimized\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "if 'caption_gen' not in globals():\n",
    "    raise NameError(\"Run Cell 13 (Initialize Caption Generator) first!\")\n",
    "\n",
    "val_captions = {}\n",
    "img_dir_val = os.path.join(config.COCO_ROOT, 'val2017')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GENERATING {len(val_images)} VALIDATION CAPTIONS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_data in tqdm(val_images, desc=\"Val\"):\n",
    "    img_id = img_data['image_id']\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    filename = img_info['file_name']\n",
    "    img_path = os.path.join(img_dir_val, filename)\n",
    "    \n",
    "    # Download if missing\n",
    "    if not os.path.exists(img_path):\n",
    "        try:\n",
    "            r = requests.get(img_info['coco_url'], timeout=10)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "    \n",
    "    # Generate caption\n",
    "    try:\n",
    "        caption = caption_gen.generate_caption(img_path)\n",
    "        if caption and len(caption) > 3:\n",
    "            val_captions[filename] = caption\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "    \n",
    "    # Periodic cleanup\n",
    "    if (success + failed) % 50 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n✓ Generated: {success}\")\n",
    "print(f\"✗ Failed: {failed}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7be8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Save Caption Files\n",
    "\n",
    "Save the generated captions to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff7fde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Dataset & Cleanup\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Cleanup model\n",
    "if 'caption_gen' in globals():\n",
    "    caption_gen.cleanup()\n",
    "\n",
    "# Save captions\n",
    "train_out = os.path.join(config.OUTPUT_DIR, 'train_captions.json')\n",
    "val_out = os.path.join(config.OUTPUT_DIR, 'val_captions.json')\n",
    "\n",
    "with open(train_out, 'w') as f:\n",
    "    json.dump(train_captions, f, indent=2)\n",
    "\n",
    "with open(val_out, 'w') as f:\n",
    "    json.dump(val_captions, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train: {len(train_captions)} captions → {train_out}\")\n",
    "print(f\"Val: {len(val_captions)} captions → {val_out}\")\n",
    "print(f\"\\nStats:\")\n",
    "print(f\"  Total: {len(train_captions) + len(val_captions)} images\")\n",
    "if train_captions:\n",
    "    avg_train = sum(len(c.split()) for c in train_captions.values()) / len(train_captions)\n",
    "    print(f\"  Avg caption (train): {avg_train:.1f} tokens\")\n",
    "if val_captions:\n",
    "    avg_val = sum(len(c.split()) for c in val_captions.values()) / len(val_captions)\n",
    "    print(f\"  Avg caption (val): {avg_val:.1f} tokens\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf47dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Verify Dataset Quality\n",
    "\n",
    "Check a few samples to verify caption quality and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee1715",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify Dataset (Quick Check Only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE CAPTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if train_captions:\n",
    "    for i, (fname, cap) in enumerate(list(train_captions.items())[:2]):\n",
    "        print(f\"\\nTrain[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")\n",
    "\n",
    "if val_captions:\n",
    "    for i, (fname, cap) in enumerate(list(val_captions.items())[:2]):\n",
    "        print(f\"\\nVal[{i}]: {fname}\")\n",
    "        print(f\"  Caption: {cap[:100]}...\")\n",
    "        print(f\"  Tokens: {len(cap.split())}\")\n",
    "\n",
    "print(\"\\n✅ Ready for ControlNet training!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 210.335121,
   "end_time": "2025-12-10T09:49:59.046802",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T09:46:28.711681",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
